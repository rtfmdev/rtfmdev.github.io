{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Devops manuals, tips and notes \u00b6 Hello and welcome! Here I'm collecting some interesting and relevant manuals and notes about technologies and processes I use in my work or studying now for future projects. All these materials I've collected and created for my personal use only, but you are free to use it and post it anywhere. Thanks for links to original... :) If you have any advices, recommendations, please write comments and I'll answer as soon as possible. My current technologies stack: \u00b6 Automate CI/CD processes with GitlabCI , Github Actions and Jenkins . Containerize all with Docker and orchestrate them with ECS or Kubernetes . Infrastructure as a Code with Terraform and Ansible . Service discovery, mesh and secret management with Consul , Vault and Nomad . Message distributing with Apache Kafka and RabbitMQ Setup and support monitoring environment with ELK , Prometheus and Grafana . Infrastructure monitoring and analytics with New Relic . AWS cloud services management: EC2, ECS, Fargate, RDS, CloudWatch, etc. Serverless AWS Lambda with Javascript and Python . Hardware and network maintenances, system scripts and backend API with Python . About all these technologies, but not only, I write or soon will write here something interesting! Stay tuned! These pages are always on developing state. Some texts could be changed or extended. Sorry, but I'm working... :)","title":"Main page"},{"location":"#devops-manuals-tips-and-notes","text":"Hello and welcome! Here I'm collecting some interesting and relevant manuals and notes about technologies and processes I use in my work or studying now for future projects. All these materials I've collected and created for my personal use only, but you are free to use it and post it anywhere. Thanks for links to original... :) If you have any advices, recommendations, please write comments and I'll answer as soon as possible.","title":"Devops manuals, tips and notes"},{"location":"#my-current-technologies-stack","text":"Automate CI/CD processes with GitlabCI , Github Actions and Jenkins . Containerize all with Docker and orchestrate them with ECS or Kubernetes . Infrastructure as a Code with Terraform and Ansible . Service discovery, mesh and secret management with Consul , Vault and Nomad . Message distributing with Apache Kafka and RabbitMQ Setup and support monitoring environment with ELK , Prometheus and Grafana . Infrastructure monitoring and analytics with New Relic . AWS cloud services management: EC2, ECS, Fargate, RDS, CloudWatch, etc. Serverless AWS Lambda with Javascript and Python . Hardware and network maintenances, system scripts and backend API with Python . About all these technologies, but not only, I write or soon will write here something interesting! Stay tuned! These pages are always on developing state. Some texts could be changed or extended. Sorry, but I'm working... :)","title":"My current technologies stack:"},{"location":"AWS_services/","text":"","title":"AWS services"},{"location":"AWS_services/assumeRole/","text":"Managing AWS Users and Roles in a Multi-Account Organization \u00b6 The underlying problem: how do you manage multiple AWS deployments? The typical example is development/qa/production, but developer sandboxes \u2014 in which developers have the freedom to experiment with services without fear of impacting anyone else \u2014 are perhaps even more relevant. The standard answer to this problem is to create multiple AWS accounts, and with the release of AWS Organizations in 2017 it became much easier to implement: in addition to simplifying billing, Organizations gives the master account more control over the children via Service Control Policies. But if you use multiple accounts, how do you manage users in those accounts? One not-very-good answer is to create separate users in each account. This quickly becomes a management nightmare, both for the organization and your users. For the organization, you need to add users to the appropriate accounts, manage their permissions, and remove them if they leave the company; this can be solved with automation. But for users, it\u2019s harder to solve: I\u2019ve watched coworkers cycle through a list of accounts/passwords until they found the right one for the task they were about to do. And inevitably, if you\u2019re working with multiple accounts you end up with an \u201coops!\u201d where you did something in the wrong account. Architecture \u00b6 There is an architecture, in which all users are defined in the organization\u2019s master account, and have the ability to assume roles in the child accounts (note: each account has a made-up account ID that\u2019s used in subsequent examples): The one thing that all roles will have in common is a trust relationship that allows them to be assumed by users in the master account: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::012345678901:root\" }, \"Action\" : \"sts:AssumeRole\" } ] } This trust relationship means the role can be assumed by any user in the organizational master account who is allowed the sts:AssumeRole action. This privilege is granted via policies in the master account, and in keeping with AWS best practice, those policies should be attached to groups rather than individual users. As a concrete example, let\u2019s assume that we have an application named Alfie. The developers who work on that application should have different levels of access to the resources used by that application depending on environment: full access in development, access for deployments in QA, and read-only access in production. Each level of access will require a role in the relevant account; the specifics of these roles are beyond the scope of this post. After creating the roles, we\u2019d then create an alfie-developers group in the master account, assign the developers to that group, and give it a permissions policy that looks like the following: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789012:role/AlfieReadOnly\" , \"arn:aws:iam::234567890123:role/AlfieDeployment\" , \"arn:aws:iam::345678901234:role/AlfieFullAccess\" ] } ] } Role managing and .aws config \u00b6 All well and good, but how do your users work in this environment? As long as you stick with the AWS Console, it\u2019s easy to switch roles. From the command line it\u2019s a little more difficult. The simplest option is to update your AWS configuration files, stored in $HOME/.aws. There are two files, credentials and config, and while in practice you can specify assumable roles in either, the docs are very explicit that the former is only for actual credentials. Assuming that you ran aws configure, it will look like this: [default] aws_access_key_id = AKIAEXAMPLEXXXXXXXXX aws_secret_access_key = EXAMPLExxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx In the second file, config, you can specify multiple profiles (you also have a default profile in this file, where you can specify your default region): [default] region = us-east-1 [profile alfie-development] role_arn = arn:aws:iam::345678901234:role/AlfieFullAccess source_profile = default [profile alfie-qa] role_arn = arn:aws:iam::234567890123:role/AlfieDeployment source_profile = default [profile alfie-production] role_arn = arn:aws:iam::123456789012:role/AlfieReadOnly source_profile = default With these profiles configured, you can use the --profile command-line parameter to specify which role you want to assume for a particular action: aws s3 ls --profile alfie-development This is somewhat onerous, and it isn\u2019t available for programs that you write. As an alternative, you can use the AWS_PROFILE environment variable to configure your profile: export AWS_PROFILE = alfie-development aws s3 ls There is one final caveat: an assumed role has a limited duration, by default one hour. With the command line tools and a configured profile, you should never run into this, because each invocation assumes its own role. But in the Console, you may get an \u201caccess denied\u201d error, or a popup message telling you to reload. Doing so will refresh your role and you can keep going for another hour. To wrap up: you may consider switching roles to be a lot of work. But in practice, developers get used to doing it in short order, and prefer not having to juggle credentials. And a more important benefit is that you can then introduce special roles for destructive operations, such as shutting down an RDS instance.","title":"Assume Role"},{"location":"AWS_services/assumeRole/#managing-aws-users-and-roles-in-a-multi-account-organization","text":"The underlying problem: how do you manage multiple AWS deployments? The typical example is development/qa/production, but developer sandboxes \u2014 in which developers have the freedom to experiment with services without fear of impacting anyone else \u2014 are perhaps even more relevant. The standard answer to this problem is to create multiple AWS accounts, and with the release of AWS Organizations in 2017 it became much easier to implement: in addition to simplifying billing, Organizations gives the master account more control over the children via Service Control Policies. But if you use multiple accounts, how do you manage users in those accounts? One not-very-good answer is to create separate users in each account. This quickly becomes a management nightmare, both for the organization and your users. For the organization, you need to add users to the appropriate accounts, manage their permissions, and remove them if they leave the company; this can be solved with automation. But for users, it\u2019s harder to solve: I\u2019ve watched coworkers cycle through a list of accounts/passwords until they found the right one for the task they were about to do. And inevitably, if you\u2019re working with multiple accounts you end up with an \u201coops!\u201d where you did something in the wrong account.","title":"Managing AWS Users and Roles in a Multi-Account Organization"},{"location":"AWS_services/assumeRole/#architecture","text":"There is an architecture, in which all users are defined in the organization\u2019s master account, and have the ability to assume roles in the child accounts (note: each account has a made-up account ID that\u2019s used in subsequent examples): The one thing that all roles will have in common is a trust relationship that allows them to be assumed by users in the master account: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::012345678901:root\" }, \"Action\" : \"sts:AssumeRole\" } ] } This trust relationship means the role can be assumed by any user in the organizational master account who is allowed the sts:AssumeRole action. This privilege is granted via policies in the master account, and in keeping with AWS best practice, those policies should be attached to groups rather than individual users. As a concrete example, let\u2019s assume that we have an application named Alfie. The developers who work on that application should have different levels of access to the resources used by that application depending on environment: full access in development, access for deployments in QA, and read-only access in production. Each level of access will require a role in the relevant account; the specifics of these roles are beyond the scope of this post. After creating the roles, we\u2019d then create an alfie-developers group in the master account, assign the developers to that group, and give it a permissions policy that looks like the following: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789012:role/AlfieReadOnly\" , \"arn:aws:iam::234567890123:role/AlfieDeployment\" , \"arn:aws:iam::345678901234:role/AlfieFullAccess\" ] } ] }","title":"Architecture"},{"location":"AWS_services/assumeRole/#role-managing-and-aws-config","text":"All well and good, but how do your users work in this environment? As long as you stick with the AWS Console, it\u2019s easy to switch roles. From the command line it\u2019s a little more difficult. The simplest option is to update your AWS configuration files, stored in $HOME/.aws. There are two files, credentials and config, and while in practice you can specify assumable roles in either, the docs are very explicit that the former is only for actual credentials. Assuming that you ran aws configure, it will look like this: [default] aws_access_key_id = AKIAEXAMPLEXXXXXXXXX aws_secret_access_key = EXAMPLExxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx In the second file, config, you can specify multiple profiles (you also have a default profile in this file, where you can specify your default region): [default] region = us-east-1 [profile alfie-development] role_arn = arn:aws:iam::345678901234:role/AlfieFullAccess source_profile = default [profile alfie-qa] role_arn = arn:aws:iam::234567890123:role/AlfieDeployment source_profile = default [profile alfie-production] role_arn = arn:aws:iam::123456789012:role/AlfieReadOnly source_profile = default With these profiles configured, you can use the --profile command-line parameter to specify which role you want to assume for a particular action: aws s3 ls --profile alfie-development This is somewhat onerous, and it isn\u2019t available for programs that you write. As an alternative, you can use the AWS_PROFILE environment variable to configure your profile: export AWS_PROFILE = alfie-development aws s3 ls There is one final caveat: an assumed role has a limited duration, by default one hour. With the command line tools and a configured profile, you should never run into this, because each invocation assumes its own role. But in the Console, you may get an \u201caccess denied\u201d error, or a popup message telling you to reload. Doing so will refresh your role and you can keep going for another hour. To wrap up: you may consider switching roles to be a lot of work. But in practice, developers get used to doing it in short order, and prefer not having to juggle credentials. And a more important benefit is that you can then introduce special roles for destructive operations, such as shutting down an RDS instance.","title":"Role managing and .aws config"},{"location":"AWS_services/fargate/","text":"\u00b6 Trapping SIG inside Fargate and ECS containers \u00b6 Wrap any container, which you want to trap signals inside. In my example I use custom SIG handler for prevent Consul Agent sidecar's early shutdown. Timeout can be set through env variable EXIT_TIMEOUT. Dockerfile FROM consul:1.6.2 COPY entrypoint.sh ./ RUN chmod +x entrypoint.sh ENTRYPOINT [ \"./entrypoint.sh\" ] entrypoint.sh #!/bin/sh set -x pid = 0 timeout = 0 if [ ! -z $EXIT_TIMEOUT ] ; then timeout = $EXIT_TIMEOUT fi # Signals-handler sig_handler () { if [ $pid -ne 0 ] ; then echo \" $1 : Waiting $2 sec...\" sleep $2 s kill - $1 \" $pid \" wait \" $pid \" fi exit $(( 128 + $3 )) ; } # Setup handlers for SIGTERM and SIGINT signals. # On callback, kill the last background process, # which is `tail -f /dev/null` and execute the specified handler. # Handler will kill with trapped signal process number from $pid variable. trap 'kill ${!}; sig_handler SIGTERM ${timeout} 15' SIGTERM trap 'kill ${!}; sig_handler SIGINT ${timeout} 2' SIGINT # Run original entrypoint script with received parametrs docker-entrypoint.sh \" $@ \" & pid = \" $! \" # Wait forever while true do tail -f /dev/null & wait ${ ! } done Pay attention! Fargate and ECS containers use different way for stop_timeout definition. Fargate defines it inside json task definition parameter, but ECS container make it through env variable ECS_CONTAINER_STOP_TIMEOUT inside it's container. Also, be careful with these parameters and wrapper trap timeout value. It has to be equal or less than stop_timeout minus real time for container's graceful shutdown. Time-based autoscaling on Fargate \u00b6 Sometimes we want to turn off our staging environments at night to save some money. If your infrastructure use a lot of Fargate containers - you can set there cron scheduler for every task you have. Only have to remember about API Call limits. If you setup many timers to trigger at the same time - they could be throttled. AWS wont explain exact numbers, but after many checks and fails - we've found out that this number is about 100. And AWS don't want to increase it by our request to support team... Maybe we are not big enough for them. :) Set parameters \u00b6 $ export ECS_CLUSTER_NAME ={ YOUR_ECS_CLUSTER_NAME } $ export ECS_SERVICE_NAME ={ YOUR_ECS_SERVICE_NAME } RegisterScalableTarget \u00b6 $ aws application-autoscaling register-scalable-target --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --min-capacity 1 \\ --max-capacity 3 PutScheduledAction \u00b6 $ export SCALE_OUT_ACTION_NAME = fargate-time-based-scale-out # configure scaling out $ aws application-autoscaling put-scheduled-action --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scheduled-action-name ${ SCALE_OUT_ACTION_NAME } \\ --schedule \"cron(50 23 * * ? *)\" \\ # every day at 8:50am JST --scalable-target-action MinCapacity = 3 ,MaxCapacity = 10 $ export SCALE_IN_ACTION_NAME = fargate-time-based-scale-in # configure scaling in $ aws application-autoscaling put-scheduled-action --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scheduled-action-name ${ SCALE_IN_ACTION_NAME } \\ --schedule \"cron(10 9 * * ? *)\" \\ # every day at 6:10pm JST --scalable-target-action MinCapacity = 1 ,MaxCapacity = 1 DeleteScheduledAction \u00b6 $ aws application-autoscaling delete-scheduled-action --service-namespace ecs \\ --scheduled-action-name ${ SCALE_OUT_ACTION_NAME } \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scalable-dimension ecs:service:DesiredCount $ aws application-autoscaling delete-scheduled-action --service-namespace ecs \\ --scheduled-action-name ${ SCALE_IN_ACTION_NAME } \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scalable-dimension ecs:service:DesiredCount DescribeScheduledActions \u00b6 $ aws application-autoscaling describe-scheduled-actions --service-namespace ecs \\ --scheduled-action-names ${ SCALE_IN_ACTION_NAME } ${ SCALE_OUT_ACTION_NAME } See also \u00b6 https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html Terraform example \u00b6 resource \"aws_appautoscaling_target\" \"tgt\" { service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" role_arn = \"${var.role_arn}\" min_capacity = 1 max_capacity = 1 lifecycle = { create_before_destroy = true } } // Night OFF ( capacity 0 ) scheduler at 21 : 00 UTC resource \"aws_appautoscaling_scheduled_action\" \"night_off\" { name = \"${var.service}-night-off-timer\" service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" schedule = \"cron(0 21 * * ? *)\" scalable_target_action { min_capacity = 0 max_capacity = 0 } depends_on = [ \"aws_appautoscaling_target.tgt\" ] } // Day ON scheduler ( capacity 1 ) at 5 : 00 UTC resource \"aws_appautoscaling_scheduled_action\" \"day_on\" { name = \"${var.service}-day-on-timer\" service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" schedule = \"cron(0 5 * * ? *)\" scalable_target_action { min_capacity = 1 max_capacity = 1 } depends_on = [ \"aws_appautoscaling_target.tgt\" ] } ESR repo access from another account \u00b6 Set Repo Permissions Policy in main Account A for Account B: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetAuthorizationToken\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:GetDownloadUrlForLayer\" , \"ecr:GetRepositoryPolicy\" , \"ecr:DescribeRepositories\" , \"ecr:ListImages\" , \"ecr:DescribeImages\" , \"ecr:BatchGetImage\" , \"ecr:InitiateLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:CompleteLayerUpload\" , \"ecr:PutImage\" ], \"Condition\" : { \"StringLike\" : { \"aws:ResourceTag/Team\" : \"Payments\" } }, \"Principal\" : { \"AWS\" : [ // Cah n ge t o \"root\" f or whole accou nt B access \"arn:aws:iam::YOUR-ACCOUNT-B-ID:user/devusr2\" ] }, \"Sid\" : \"AllowCrossAccountPushAndPull\" } ] }","title":"ECS Fargate"},{"location":"AWS_services/fargate/#_1","text":"","title":""},{"location":"AWS_services/fargate/#trapping-sig-inside-fargate-and-ecs-containers","text":"Wrap any container, which you want to trap signals inside. In my example I use custom SIG handler for prevent Consul Agent sidecar's early shutdown. Timeout can be set through env variable EXIT_TIMEOUT. Dockerfile FROM consul:1.6.2 COPY entrypoint.sh ./ RUN chmod +x entrypoint.sh ENTRYPOINT [ \"./entrypoint.sh\" ] entrypoint.sh #!/bin/sh set -x pid = 0 timeout = 0 if [ ! -z $EXIT_TIMEOUT ] ; then timeout = $EXIT_TIMEOUT fi # Signals-handler sig_handler () { if [ $pid -ne 0 ] ; then echo \" $1 : Waiting $2 sec...\" sleep $2 s kill - $1 \" $pid \" wait \" $pid \" fi exit $(( 128 + $3 )) ; } # Setup handlers for SIGTERM and SIGINT signals. # On callback, kill the last background process, # which is `tail -f /dev/null` and execute the specified handler. # Handler will kill with trapped signal process number from $pid variable. trap 'kill ${!}; sig_handler SIGTERM ${timeout} 15' SIGTERM trap 'kill ${!}; sig_handler SIGINT ${timeout} 2' SIGINT # Run original entrypoint script with received parametrs docker-entrypoint.sh \" $@ \" & pid = \" $! \" # Wait forever while true do tail -f /dev/null & wait ${ ! } done Pay attention! Fargate and ECS containers use different way for stop_timeout definition. Fargate defines it inside json task definition parameter, but ECS container make it through env variable ECS_CONTAINER_STOP_TIMEOUT inside it's container. Also, be careful with these parameters and wrapper trap timeout value. It has to be equal or less than stop_timeout minus real time for container's graceful shutdown.","title":"Trapping SIG inside Fargate and ECS containers"},{"location":"AWS_services/fargate/#time-based-autoscaling-on-fargate","text":"Sometimes we want to turn off our staging environments at night to save some money. If your infrastructure use a lot of Fargate containers - you can set there cron scheduler for every task you have. Only have to remember about API Call limits. If you setup many timers to trigger at the same time - they could be throttled. AWS wont explain exact numbers, but after many checks and fails - we've found out that this number is about 100. And AWS don't want to increase it by our request to support team... Maybe we are not big enough for them. :)","title":"Time-based autoscaling on Fargate"},{"location":"AWS_services/fargate/#set-parameters","text":"$ export ECS_CLUSTER_NAME ={ YOUR_ECS_CLUSTER_NAME } $ export ECS_SERVICE_NAME ={ YOUR_ECS_SERVICE_NAME }","title":"Set parameters"},{"location":"AWS_services/fargate/#registerscalabletarget","text":"$ aws application-autoscaling register-scalable-target --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --min-capacity 1 \\ --max-capacity 3","title":"RegisterScalableTarget"},{"location":"AWS_services/fargate/#putscheduledaction","text":"$ export SCALE_OUT_ACTION_NAME = fargate-time-based-scale-out # configure scaling out $ aws application-autoscaling put-scheduled-action --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scheduled-action-name ${ SCALE_OUT_ACTION_NAME } \\ --schedule \"cron(50 23 * * ? *)\" \\ # every day at 8:50am JST --scalable-target-action MinCapacity = 3 ,MaxCapacity = 10 $ export SCALE_IN_ACTION_NAME = fargate-time-based-scale-in # configure scaling in $ aws application-autoscaling put-scheduled-action --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scheduled-action-name ${ SCALE_IN_ACTION_NAME } \\ --schedule \"cron(10 9 * * ? *)\" \\ # every day at 6:10pm JST --scalable-target-action MinCapacity = 1 ,MaxCapacity = 1","title":"PutScheduledAction"},{"location":"AWS_services/fargate/#deletescheduledaction","text":"$ aws application-autoscaling delete-scheduled-action --service-namespace ecs \\ --scheduled-action-name ${ SCALE_OUT_ACTION_NAME } \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scalable-dimension ecs:service:DesiredCount $ aws application-autoscaling delete-scheduled-action --service-namespace ecs \\ --scheduled-action-name ${ SCALE_IN_ACTION_NAME } \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scalable-dimension ecs:service:DesiredCount","title":"DeleteScheduledAction"},{"location":"AWS_services/fargate/#describescheduledactions","text":"$ aws application-autoscaling describe-scheduled-actions --service-namespace ecs \\ --scheduled-action-names ${ SCALE_IN_ACTION_NAME } ${ SCALE_OUT_ACTION_NAME }","title":"DescribeScheduledActions"},{"location":"AWS_services/fargate/#see-also","text":"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html","title":"See also"},{"location":"AWS_services/fargate/#terraform-example","text":"resource \"aws_appautoscaling_target\" \"tgt\" { service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" role_arn = \"${var.role_arn}\" min_capacity = 1 max_capacity = 1 lifecycle = { create_before_destroy = true } } // Night OFF ( capacity 0 ) scheduler at 21 : 00 UTC resource \"aws_appautoscaling_scheduled_action\" \"night_off\" { name = \"${var.service}-night-off-timer\" service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" schedule = \"cron(0 21 * * ? *)\" scalable_target_action { min_capacity = 0 max_capacity = 0 } depends_on = [ \"aws_appautoscaling_target.tgt\" ] } // Day ON scheduler ( capacity 1 ) at 5 : 00 UTC resource \"aws_appautoscaling_scheduled_action\" \"day_on\" { name = \"${var.service}-day-on-timer\" service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" schedule = \"cron(0 5 * * ? *)\" scalable_target_action { min_capacity = 1 max_capacity = 1 } depends_on = [ \"aws_appautoscaling_target.tgt\" ] }","title":"Terraform example"},{"location":"AWS_services/fargate/#esr-repo-access-from-another-account","text":"Set Repo Permissions Policy in main Account A for Account B: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetAuthorizationToken\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:GetDownloadUrlForLayer\" , \"ecr:GetRepositoryPolicy\" , \"ecr:DescribeRepositories\" , \"ecr:ListImages\" , \"ecr:DescribeImages\" , \"ecr:BatchGetImage\" , \"ecr:InitiateLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:CompleteLayerUpload\" , \"ecr:PutImage\" ], \"Condition\" : { \"StringLike\" : { \"aws:ResourceTag/Team\" : \"Payments\" } }, \"Principal\" : { \"AWS\" : [ // Cah n ge t o \"root\" f or whole accou nt B access \"arn:aws:iam::YOUR-ACCOUNT-B-ID:user/devusr2\" ] }, \"Sid\" : \"AllowCrossAccountPushAndPull\" } ] }","title":"ESR repo access from another account"},{"location":"AWS_services/useful-tips/","text":"Useful tips for AWS Services \u00b6 AWS IP Address Ranges \u00b6 Amazon Web Services (AWS) publishes its current IP address ranges in JSON format. To view the current ranges, download the .json file. To maintain history, save successive versions of the .json file on your system. To determine whether there have been changes since the last time that you saved the file, check the publication time in the current file and compare it to the publication time in the last file that you saved. #!/usr/bin/env python import requests ip_ranges = requests . get ( 'https://ip-ranges.amazonaws.com/ip-ranges.json' ) . json ()[ 'prefixes' ] amazon_ips = [ item [ 'ip_prefix' ] for item in ip_ranges if item [ \"service\" ] == \"AMAZON\" ] ec2_ips = [ item [ 'ip_prefix' ] for item in ip_ranges if item [ \"service\" ] == \"EC2\" ] amazon_ips_less_ec2 = [] for ip in amazon_ips : if ip not in ec2_ips : amazon_ips_less_ec2 . append ( ip ) for ip in amazon_ips_less_ec2 : print ( str ( ip ))","title":"Useful Tips"},{"location":"AWS_services/useful-tips/#useful-tips-for-aws-services","text":"","title":"Useful tips for AWS Services"},{"location":"AWS_services/useful-tips/#aws-ip-address-ranges","text":"Amazon Web Services (AWS) publishes its current IP address ranges in JSON format. To view the current ranges, download the .json file. To maintain history, save successive versions of the .json file on your system. To determine whether there have been changes since the last time that you saved the file, check the publication time in the current file and compare it to the publication time in the last file that you saved. #!/usr/bin/env python import requests ip_ranges = requests . get ( 'https://ip-ranges.amazonaws.com/ip-ranges.json' ) . json ()[ 'prefixes' ] amazon_ips = [ item [ 'ip_prefix' ] for item in ip_ranges if item [ \"service\" ] == \"AMAZON\" ] ec2_ips = [ item [ 'ip_prefix' ] for item in ip_ranges if item [ \"service\" ] == \"EC2\" ] amazon_ips_less_ec2 = [] for ip in amazon_ips : if ip not in ec2_ips : amazon_ips_less_ec2 . append ( ip ) for ip in amazon_ips_less_ec2 : print ( str ( ip ))","title":"AWS IP Address Ranges"},{"location":"dev-tools/git/","text":"Git tips and tricks \u00b6 Sometimes you have to make very rare manipulations with your git repo, but can't remember how exactly it works. Here I'm collecting useful git use cases. Rebase previous N to one \u00b6 If you need to unite last N commits on current brunch run this rebase command with your number of last commits git rebase -i HEAD~4 After it opens a rebase file, check here the last commit with \"p\" letter and other ones with \"s\" letter. Save it. And save next opened file with new commit message file too. These steps will do single commit with all changes from this N commits. It could be useful if you don't want to show in remote repo all the history of your commits on the branch. Or maybe want to hide some of them. Undoing last commit \u00b6 git reset --soft HEAD~1 If you don't want to keep these changes, simply use the --hard flag. Be sure to only do this when you're sure you don't need these changes anymore. git reset --hard HEAD~1 Show diff between branches \u00b6 This shows only the changes between current local branch and the remote master branch, and ignores any changes in the local branch that came from merge commits. git diff origin/master... Replace master with better branch \u00b6 git checkout master git reset --hard better_branch git push -f origin master Merge master to current branch \u00b6 Merge all changes from master to your current branch: git pull origin master and resolve conflicts if needed. Merge with squash \u00b6 Merge from branch with squash will erase all commits history in the branch # Work in feature/1 branch git branch \"feature/1\" git checkout \"feature/1\" # Move to the master branch. git checkout master # Merge the \"feature/1\" branch and squash the commits. git merge --squash feature/1 # Resolve conflicts # Commit your changes and add a single commit message for all your commits. # You can omit the \"-m\" to have a template popping up based on your previous commit messages. git commit -m \"Feature 1 : 1, 2 et 3\" # Delete the \"feature/1\" branch that is no longer needed. git branch -D feature/1 Reset \u00b6 Soft \u00b6 You've done a few tiny commits and want them all be put into one commit: A -> B -> C -> D || A with ALL (B,C,D) -> E git reset --soft A git commit -m \u201cmy new merged commit\u201d git push origin branch New commit E contains all of the files that were committed in B, C, D. Mixed \u00b6 You\u2019ve just pushed a few commits, but you want to go back and remove a couple of files in a previous commit. A -> B-> C -> D || A with ANY (B,C,D) -> E git reset --mixed A Your branch head and index is pointing at A and all of your changes in B, C and D are there, but are untracked. Now you are free to add the files that you wish to add into a new commit. git add <files> git commit -m \"updated commit\" git push origin branch The head is now at the new commit E, and any files that you\u2019ve not staged will still be in your working tree, ready to add into another commit or to do what you want with. Hard \u00b6 Do it only if you want to go back a few commits and get rid of every change you\u2019ve made since. git reset --hard A git push origin branchname --force This will delete your commits from the remote branch history. Add submodule \u00b6 git submodule add <link_to_git_repo> <link_to_submodule_folder> git submodule init git submodule update Edit last commit message \u00b6 1 git commit --amend Add missed file to last commit \u00b6 1 2 git add missed-file.txt git commit --amend Move last commit from master to branch \u00b6 1 2 3 git branch branch-name git reset HEAD~ --hard git checkout branch-name Clean up local commits before pushing \u00b6 git rebase --interactive if you didn't specify any tracking information for this branch you will have to add upstream and remote branch information: git rebase --interactive origin branch Work With Multiple Branches Simultaneously \u00b6 Creates a linked working tree (i.e., another directory on the file system associated with the repo) called path , one level above your current working directory, with the specified branch checked out. #create git worktree add <../path> <branch> # remove git worktree remove [-f] <../path>","title":"Git Tricks"},{"location":"dev-tools/git/#git-tips-and-tricks","text":"Sometimes you have to make very rare manipulations with your git repo, but can't remember how exactly it works. Here I'm collecting useful git use cases.","title":"Git tips and tricks"},{"location":"dev-tools/git/#rebase-previous-n-to-one","text":"If you need to unite last N commits on current brunch run this rebase command with your number of last commits git rebase -i HEAD~4 After it opens a rebase file, check here the last commit with \"p\" letter and other ones with \"s\" letter. Save it. And save next opened file with new commit message file too. These steps will do single commit with all changes from this N commits. It could be useful if you don't want to show in remote repo all the history of your commits on the branch. Or maybe want to hide some of them.","title":"Rebase previous N to one"},{"location":"dev-tools/git/#undoing-last-commit","text":"git reset --soft HEAD~1 If you don't want to keep these changes, simply use the --hard flag. Be sure to only do this when you're sure you don't need these changes anymore. git reset --hard HEAD~1","title":"Undoing last commit"},{"location":"dev-tools/git/#show-diff-between-branches","text":"This shows only the changes between current local branch and the remote master branch, and ignores any changes in the local branch that came from merge commits. git diff origin/master...","title":"Show diff between branches"},{"location":"dev-tools/git/#replace-master-with-better-branch","text":"git checkout master git reset --hard better_branch git push -f origin master","title":"Replace master with better branch"},{"location":"dev-tools/git/#merge-master-to-current-branch","text":"Merge all changes from master to your current branch: git pull origin master and resolve conflicts if needed.","title":"Merge master to current branch"},{"location":"dev-tools/git/#merge-with-squash","text":"Merge from branch with squash will erase all commits history in the branch # Work in feature/1 branch git branch \"feature/1\" git checkout \"feature/1\" # Move to the master branch. git checkout master # Merge the \"feature/1\" branch and squash the commits. git merge --squash feature/1 # Resolve conflicts # Commit your changes and add a single commit message for all your commits. # You can omit the \"-m\" to have a template popping up based on your previous commit messages. git commit -m \"Feature 1 : 1, 2 et 3\" # Delete the \"feature/1\" branch that is no longer needed. git branch -D feature/1","title":"Merge with squash"},{"location":"dev-tools/git/#reset","text":"","title":"Reset"},{"location":"dev-tools/git/#soft","text":"You've done a few tiny commits and want them all be put into one commit: A -> B -> C -> D || A with ALL (B,C,D) -> E git reset --soft A git commit -m \u201cmy new merged commit\u201d git push origin branch New commit E contains all of the files that were committed in B, C, D.","title":"Soft"},{"location":"dev-tools/git/#mixed","text":"You\u2019ve just pushed a few commits, but you want to go back and remove a couple of files in a previous commit. A -> B-> C -> D || A with ANY (B,C,D) -> E git reset --mixed A Your branch head and index is pointing at A and all of your changes in B, C and D are there, but are untracked. Now you are free to add the files that you wish to add into a new commit. git add <files> git commit -m \"updated commit\" git push origin branch The head is now at the new commit E, and any files that you\u2019ve not staged will still be in your working tree, ready to add into another commit or to do what you want with.","title":"Mixed"},{"location":"dev-tools/git/#hard","text":"Do it only if you want to go back a few commits and get rid of every change you\u2019ve made since. git reset --hard A git push origin branchname --force This will delete your commits from the remote branch history.","title":"Hard"},{"location":"dev-tools/git/#add-submodule","text":"git submodule add <link_to_git_repo> <link_to_submodule_folder> git submodule init git submodule update","title":"Add submodule"},{"location":"dev-tools/git/#edit-last-commit-message","text":"1 git commit --amend","title":"Edit last commit message"},{"location":"dev-tools/git/#add-missed-file-to-last-commit","text":"1 2 git add missed-file.txt git commit --amend","title":"Add missed file to last commit"},{"location":"dev-tools/git/#move-last-commit-from-master-to-branch","text":"1 2 3 git branch branch-name git reset HEAD~ --hard git checkout branch-name","title":"Move last commit from master to branch"},{"location":"dev-tools/git/#clean-up-local-commits-before-pushing","text":"git rebase --interactive if you didn't specify any tracking information for this branch you will have to add upstream and remote branch information: git rebase --interactive origin branch","title":"Clean up local commits before pushing"},{"location":"dev-tools/git/#work-with-multiple-branches-simultaneously","text":"Creates a linked working tree (i.e., another directory on the file system associated with the repo) called path , one level above your current working directory, with the specified branch checked out. #create git worktree add <../path> <branch> # remove git worktree remove [-f] <../path>","title":"Work With Multiple Branches Simultaneously"},{"location":"dev-tools/javascript/","text":"","title":"Javascript"},{"location":"dev-tools/jenkins/","text":"Jenkins tricks \u00b6 Kill stacked job \u00b6 Sometimes Jenkins job could stack without response, and can't be killed from the UI. Go to Manage Jenkins -> Script Console and paste there one of these code-blocks. Change job name and build number to relevant ones and run it. Jenkins . instance . getItemByFullName ( \"CI/rc-preprod\" ) . getBuildByNumber ( 895 ) . finish ( hudson . model . Result . ABORTED , new java . io . IOException ( \"Aborting build\" ) ); or def jobname = \"CI/rc-preprod\" def buildnum = 895 def job = Jenkins . instance . getItemByFullName ( jobname ) for ( build in job . builds ) { if ( buildnum == build . getNumber (). toInteger ()){ if ( build . isBuilding ()){ build . doStop (); build . doKill (); } } }","title":"Jenkins"},{"location":"dev-tools/jenkins/#jenkins-tricks","text":"","title":"Jenkins tricks"},{"location":"dev-tools/jenkins/#kill-stacked-job","text":"Sometimes Jenkins job could stack without response, and can't be killed from the UI. Go to Manage Jenkins -> Script Console and paste there one of these code-blocks. Change job name and build number to relevant ones and run it. Jenkins . instance . getItemByFullName ( \"CI/rc-preprod\" ) . getBuildByNumber ( 895 ) . finish ( hudson . model . Result . ABORTED , new java . io . IOException ( \"Aborting build\" ) ); or def jobname = \"CI/rc-preprod\" def buildnum = 895 def job = Jenkins . instance . getItemByFullName ( jobname ) for ( build in job . builds ) { if ( buildnum == build . getNumber (). toInteger ()){ if ( build . isBuilding ()){ build . doStop (); build . doKill (); } } }","title":"Kill stacked job"},{"location":"dev-tools/mongo/","text":"","title":"MongoDB"},{"location":"dev-tools/nginx/","text":"NGINX proxy pitfalls related with DNS resolving \u00b6 If you're using proxy_pass and your endpoint's IPs can vary in time, please read this article to avoid misunderstandings about how nginx works. TL;DR \u00b6 If you want to force nginx resolve your endpoints, you should: Use variables within proxy_pass directive, e.g. proxy_pass https://$endpoint/; , where $endpoint can be manually set or extracted from location regex. Make sure that your endpoint isn't used on another locations w/o variables, because in this case resolving won't work. To fix this move endpoint domain to the upstream or use variables in the proxy_pass in all locations to make resolving works. You can have both resolve and non-resolve locations for same domain When a variable is used in proxy_pass directive, the location header is not longer adjusted. To get around this, simply set proxy_redirect Explanatory example \u00b6 location /api/ { proxy_pass http://api.com/ ; } In this case nginx will resolve api.com only once at startup (or reload). But there are some cases when your endpoint can be resolved to any IP, e.g. if you're using load balancer which doing magic failover via DNS mapping. If api.com will point to another IP your proxying will fail. Finding the solution \u00b6 Add a resolver directive \u00b6 You can check official nginx documentation and find there resolver directive description: location /api/ { resolver 8 .8.8.8 ; proxy_pass https://api.com/ ; } No, it will not work. Even this will not work: location /api/ { resolver 8 .8.8.8 valid=1s ; proxy_pass https://api.com/ ; } It's because of nginx doesn't respect resolver directive in this case. It will resolve api.com only at startup (or reload) by system resolver (/etc/resolv.conf), even if real TTL of A/AAAA record api.com is 1s. Add variables \u00b6 You can google a bit and find that nginx try to resolve proxy endpoint with variables . Also official documentation for proxy_pass directive notices this too . Ok, I think this should be noticed in the resolver description, but let's try anyway: location = /proxy/ { set $endpoint proxy.com ; resolver 8 .8.8.8 valid=10s ; proxy_pass https:// $endpoint/ ; } Works as expected, nginx will query proxy.com every 10s on particular requests. These configurations works too: set $endpoint api.com ; location ~ ^/api/(.*)$ { resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } location ~ ^/(?<dest_proxy>[\\w-]+)(?:/(?<path_proxy>.*))? { resolver 8 .8.8.8 ipv6=off valid=60s ; proxy_pass https:// ${dest_proxy}.example.com/${path_proxy}$is_args$args ; } Notice that nginx will start even without resolver directive, but will fail with 502 at runtime, because \"no resolver defined to resolve\". Caveats \u00b6 location = /api_version/ { proxy_pass https://api.com/version/ ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } In this case nginx will resolve api.com once at startup with system resolver and then will never do re-resolve even for /api/ requests. Example with /api_version/ is just synthetic example, you can use more complex scenarios with headers set, etc. Use variables everywhere to make it work as expected: location = /api_version/ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/version/ ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } You can move set and resolver to the server or http (or use include ) directives to avoid copy-paste (also I assume that it will increase performance a bit, but I haven't tested it). If response from proxy contains Location header, as in the case of a redirect, nginx will automatically replace these values as needed. However, if variables are used in proxy_pass , this must be done explicitly via proxy_redirect : location = /api_version/ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/version/ ; proxy_redirect https:// $endpoint/ / ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; proxy_redirect https:// $endpoint/ / ; } Single line in nginx docs that mention it: The default parameter is not permitted if proxy_pass is specified using variables. Upstreams \u00b6 If you're using nginx plus, you can use resolve parameter, check out documentation . I assume that it will be efficient, because documentation says \"monitors changes of the IP addresses that correspond to a domain name of the server\", while solutions listed above will query DNS on the particular requests. But if you're using open source nginx, no honey is available for you. No money \u2014 no honey. Two in one \u00b6 You can have both resolve and non-resolve locations for same domain upstream proxy { server proxy.com : 443 ; } server { listen 80 ; server_name fillo.me ; location = /proxy-with-resolve/ { set $endpoint proxy.com ; resolver 8 .8.8.8 valid=1s ; proxy_pass https:// $endpoint/ ; } location = /proxy-without-resolve/ { proxy_pass https://proxy/ ; proxy_set_header Host proxy.com ; } } Yes, http://fillo.me/proxy-with-resolve/ will resolve proxy.com every 1s on particular requests, while http://fillo.me/proxy-without-resolve/ will not resolve proxy.com (nginx will resolve proxy.com at startup/reload once). This magic works because upstream directive is used. Another example: upstream api_version { server version.api.com : 443 ; } server { listen 80 ; server_name fillo.me ; location = /api_version/ { proxy_pass https://api_version/version/ ; proxy_set_header Host version.api.com ; } location ~ ^/api/(?<dest_proxy>[\\w-]+)(?:/(?<path_proxy>.*))? { resolver 8 .8.8.8 valid=60s ; proxy_pass https:// ${dest_proxy}.api.com/${path_proxy}$is_args$args ; } } * If you will open http://fillo.me/api_version/ then no resolve will be done, because of nginx resolved version.api.com at startup. * If you will open http://fillo.me/api/version/version/ then it will work as expected, nginx will resolve version.api.com every 60s on particular request. * If you will open http://fillo.me/api/checkout/items/ then it will work as expected, nginx will resolve checkout.api.com every 60s on particular request. Tested on \u00b6 1.9.6 1.10.1 Although I think it works for many other versions. Further research \u00b6 This issue says that changing HTTPS to the HTTP helps. Check how protocol changes affects examples above. Compare performance with and without resolving. Compare performance with different variables scopes. How to force upstream resolving.","title":"Nginx"},{"location":"dev-tools/nginx/#nginx-proxy-pitfalls-related-with-dns-resolving","text":"If you're using proxy_pass and your endpoint's IPs can vary in time, please read this article to avoid misunderstandings about how nginx works.","title":"NGINX proxy pitfalls related with DNS resolving"},{"location":"dev-tools/nginx/#tldr","text":"If you want to force nginx resolve your endpoints, you should: Use variables within proxy_pass directive, e.g. proxy_pass https://$endpoint/; , where $endpoint can be manually set or extracted from location regex. Make sure that your endpoint isn't used on another locations w/o variables, because in this case resolving won't work. To fix this move endpoint domain to the upstream or use variables in the proxy_pass in all locations to make resolving works. You can have both resolve and non-resolve locations for same domain When a variable is used in proxy_pass directive, the location header is not longer adjusted. To get around this, simply set proxy_redirect","title":"TL;DR"},{"location":"dev-tools/nginx/#explanatory-example","text":"location /api/ { proxy_pass http://api.com/ ; } In this case nginx will resolve api.com only once at startup (or reload). But there are some cases when your endpoint can be resolved to any IP, e.g. if you're using load balancer which doing magic failover via DNS mapping. If api.com will point to another IP your proxying will fail.","title":"Explanatory example"},{"location":"dev-tools/nginx/#finding-the-solution","text":"","title":"Finding the solution"},{"location":"dev-tools/nginx/#add-a-resolver-directive","text":"You can check official nginx documentation and find there resolver directive description: location /api/ { resolver 8 .8.8.8 ; proxy_pass https://api.com/ ; } No, it will not work. Even this will not work: location /api/ { resolver 8 .8.8.8 valid=1s ; proxy_pass https://api.com/ ; } It's because of nginx doesn't respect resolver directive in this case. It will resolve api.com only at startup (or reload) by system resolver (/etc/resolv.conf), even if real TTL of A/AAAA record api.com is 1s.","title":"Add a resolver directive"},{"location":"dev-tools/nginx/#add-variables","text":"You can google a bit and find that nginx try to resolve proxy endpoint with variables . Also official documentation for proxy_pass directive notices this too . Ok, I think this should be noticed in the resolver description, but let's try anyway: location = /proxy/ { set $endpoint proxy.com ; resolver 8 .8.8.8 valid=10s ; proxy_pass https:// $endpoint/ ; } Works as expected, nginx will query proxy.com every 10s on particular requests. These configurations works too: set $endpoint api.com ; location ~ ^/api/(.*)$ { resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } location ~ ^/(?<dest_proxy>[\\w-]+)(?:/(?<path_proxy>.*))? { resolver 8 .8.8.8 ipv6=off valid=60s ; proxy_pass https:// ${dest_proxy}.example.com/${path_proxy}$is_args$args ; } Notice that nginx will start even without resolver directive, but will fail with 502 at runtime, because \"no resolver defined to resolve\".","title":"Add variables"},{"location":"dev-tools/nginx/#caveats","text":"location = /api_version/ { proxy_pass https://api.com/version/ ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } In this case nginx will resolve api.com once at startup with system resolver and then will never do re-resolve even for /api/ requests. Example with /api_version/ is just synthetic example, you can use more complex scenarios with headers set, etc. Use variables everywhere to make it work as expected: location = /api_version/ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/version/ ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } You can move set and resolver to the server or http (or use include ) directives to avoid copy-paste (also I assume that it will increase performance a bit, but I haven't tested it). If response from proxy contains Location header, as in the case of a redirect, nginx will automatically replace these values as needed. However, if variables are used in proxy_pass , this must be done explicitly via proxy_redirect : location = /api_version/ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/version/ ; proxy_redirect https:// $endpoint/ / ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; proxy_redirect https:// $endpoint/ / ; } Single line in nginx docs that mention it: The default parameter is not permitted if proxy_pass is specified using variables.","title":"Caveats"},{"location":"dev-tools/nginx/#upstreams","text":"If you're using nginx plus, you can use resolve parameter, check out documentation . I assume that it will be efficient, because documentation says \"monitors changes of the IP addresses that correspond to a domain name of the server\", while solutions listed above will query DNS on the particular requests. But if you're using open source nginx, no honey is available for you. No money \u2014 no honey.","title":"Upstreams"},{"location":"dev-tools/nginx/#two-in-one","text":"You can have both resolve and non-resolve locations for same domain upstream proxy { server proxy.com : 443 ; } server { listen 80 ; server_name fillo.me ; location = /proxy-with-resolve/ { set $endpoint proxy.com ; resolver 8 .8.8.8 valid=1s ; proxy_pass https:// $endpoint/ ; } location = /proxy-without-resolve/ { proxy_pass https://proxy/ ; proxy_set_header Host proxy.com ; } } Yes, http://fillo.me/proxy-with-resolve/ will resolve proxy.com every 1s on particular requests, while http://fillo.me/proxy-without-resolve/ will not resolve proxy.com (nginx will resolve proxy.com at startup/reload once). This magic works because upstream directive is used. Another example: upstream api_version { server version.api.com : 443 ; } server { listen 80 ; server_name fillo.me ; location = /api_version/ { proxy_pass https://api_version/version/ ; proxy_set_header Host version.api.com ; } location ~ ^/api/(?<dest_proxy>[\\w-]+)(?:/(?<path_proxy>.*))? { resolver 8 .8.8.8 valid=60s ; proxy_pass https:// ${dest_proxy}.api.com/${path_proxy}$is_args$args ; } } * If you will open http://fillo.me/api_version/ then no resolve will be done, because of nginx resolved version.api.com at startup. * If you will open http://fillo.me/api/version/version/ then it will work as expected, nginx will resolve version.api.com every 60s on particular request. * If you will open http://fillo.me/api/checkout/items/ then it will work as expected, nginx will resolve checkout.api.com every 60s on particular request.","title":"Two in one"},{"location":"dev-tools/nginx/#tested-on","text":"1.9.6 1.10.1 Although I think it works for many other versions.","title":"Tested on"},{"location":"dev-tools/nginx/#further-research","text":"This issue says that changing HTTPS to the HTTP helps. Check how protocol changes affects examples above. Compare performance with and without resolving. Compare performance with different variables scopes. How to force upstream resolving.","title":"Further research"},{"location":"dev-tools/python/","text":"Python3 on MacOS in a right way \u00b6 Install pyenv brew install pyenv pyenv install 3 .8.0 pyenv global 3 .8.0 Add the following to your configuration file ( .zshrc for me, possibly .bash_profile for you): export PATH = \"/Users/denis/.pyenv/shims: ${ PATH } \" export PYENV_SHELL = zsh source '/usr/local/Cellar/pyenv/1.2.15/libexec/../completions/pyenv.zsh' command pyenv rehash 2 >/dev/null pyenv () { local command command = \" ${ 1 :- } \" if [ \" $# \" -gt 0 ] ; then shift fi case \" $command \" in rehash | shell ) eval \" $( pyenv \"sh- $command \" \" $@ \" ) \" ;; * ) command pyenv \" $command \" \" $@ \" ;; esac } Now we know for certain that we're using Python 3.8.0 and pip will update alongside it without any manual aliasing between versions. Lambda, map, reduce, filter \u00b6 # Lambda foo = lambda n : n ** 2 if n % 2 == 0 else n ** 3 foo ( 5 ) # Output: 125 # Map col = [ 6 , 10 , 12 , 15 , 19 ] mapped = map ( lambda x : x * 10 , col ) # mapped = [60, 100, 120, 150, 190] # Filter filtered = filter ( lambda x : x % 2 == 0 and x > 7 , col ) # filtered = [10, 12] # Reduce from functools import reduce reduced = reduce ( lambda x , y : x * y + 10 , col ) # reduced = 242450 globals() locals() \u00b6 1 2 3 4 type(<object>) -> <ClassName> isinstance(<object>, <ClassName>) -> Boolean globals() -> dict with global vars locals() -> dict with local vars Context managers \u00b6 with open ( \"example.txt\" , \"w\" ) as file : # logic Two special methods: enter () and after () . They take care of what happens when execution enters and exits your with block respectively.. from datetime import datetime class DataManager (): def __init__ ( self ): self . file = None def __enter__ ( self ): now = str ( datetime . now ()) . split ( \".\" )[ 0 ] . replece ( \" \" , \"-\" ) . replace . ( \":\" , \"-\" ) filename = now + \"-DATA.txt\" self . file = open ( filename , \"w\" ) return self . file def __exit__ ( self , exc_type , exc_value , exc_traceback ): self . file . close () print ( \"File closed\" ) with DataManager () as data : data . write ( \"hello!\" ) Decorators \u00b6 def is_positive ( func ): def wrapper ( * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return func ( * args , ** kwargs ) return wrapper @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2 Same decorations with class and special method call (): class is_positive (): def __init__ ( self , func ): self . function = func def __call__ ( self , * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return self . function ( * args , ** kwargs ) @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2 Generators \u00b6 def squares ( a , b ): i = a while i < b : y yield i ** 2 i += 1 s = squares ( 5 , 10 ) next ( s ) urllib module for web requests \u00b6 You can do a lot of neat stuff with it e.g. access API resources, make different types of HTTP requests like GET, POST, PUT, DELETE etc. Simple request to google.com to see what we get. import urllib.request with urllib . request . urlopen ( \"https://www.google.com/\" ) as url : print ( url . read ( 300 ) . decode ( \"utf-8\" )) GET request \u00b6 import urllib.request req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' ) with urllib . request . urlopen ( req ) as resp : print ( resp . read () . decode ( 'utf-8' )) POST request \u00b6 import json import urllib.request # This is our Todo data = { \"userId\" : 101 , \"id\" : 100 , \"title\" : \"This is a POST request\" , \"completed\" : True } # Dump the todo object as a json string data = json . dumps ( data ) req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/' , data = bytes ( data . encode ( \"utf-8\" )), method = \"POST\" ) # Add the appropriate header. req . add_header ( \"Content-type\" , \"application/json; charset=UTF-8\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data ) PUT request \u00b6 import json import urllib.request # This is our Todo data = { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"This is a PUT request\" , \"completed\" : False } # Dump the todo object as a json string data = json . dumps ( data ) req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' , data = bytes ( data . encode ( \"utf-8\" )), method = \"PUT\" ) # Add the appropriate header. req . add_header ( \"Content-type\" , \"application/json; charset=UTF-8\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data ) DELETE request \u00b6 import json import urllib.request req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' , method = \"DELETE\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data )","title":"Python"},{"location":"dev-tools/python/#python3-on-macos-in-a-right-way","text":"Install pyenv brew install pyenv pyenv install 3 .8.0 pyenv global 3 .8.0 Add the following to your configuration file ( .zshrc for me, possibly .bash_profile for you): export PATH = \"/Users/denis/.pyenv/shims: ${ PATH } \" export PYENV_SHELL = zsh source '/usr/local/Cellar/pyenv/1.2.15/libexec/../completions/pyenv.zsh' command pyenv rehash 2 >/dev/null pyenv () { local command command = \" ${ 1 :- } \" if [ \" $# \" -gt 0 ] ; then shift fi case \" $command \" in rehash | shell ) eval \" $( pyenv \"sh- $command \" \" $@ \" ) \" ;; * ) command pyenv \" $command \" \" $@ \" ;; esac } Now we know for certain that we're using Python 3.8.0 and pip will update alongside it without any manual aliasing between versions.","title":"Python3 on MacOS in a right way"},{"location":"dev-tools/python/#lambda-map-reduce-filter","text":"# Lambda foo = lambda n : n ** 2 if n % 2 == 0 else n ** 3 foo ( 5 ) # Output: 125 # Map col = [ 6 , 10 , 12 , 15 , 19 ] mapped = map ( lambda x : x * 10 , col ) # mapped = [60, 100, 120, 150, 190] # Filter filtered = filter ( lambda x : x % 2 == 0 and x > 7 , col ) # filtered = [10, 12] # Reduce from functools import reduce reduced = reduce ( lambda x , y : x * y + 10 , col ) # reduced = 242450","title":"Lambda, map, reduce, filter"},{"location":"dev-tools/python/#globals-locals","text":"1 2 3 4 type(<object>) -> <ClassName> isinstance(<object>, <ClassName>) -> Boolean globals() -> dict with global vars locals() -> dict with local vars","title":"globals() locals()"},{"location":"dev-tools/python/#context-managers","text":"with open ( \"example.txt\" , \"w\" ) as file : # logic Two special methods: enter () and after () . They take care of what happens when execution enters and exits your with block respectively.. from datetime import datetime class DataManager (): def __init__ ( self ): self . file = None def __enter__ ( self ): now = str ( datetime . now ()) . split ( \".\" )[ 0 ] . replece ( \" \" , \"-\" ) . replace . ( \":\" , \"-\" ) filename = now + \"-DATA.txt\" self . file = open ( filename , \"w\" ) return self . file def __exit__ ( self , exc_type , exc_value , exc_traceback ): self . file . close () print ( \"File closed\" ) with DataManager () as data : data . write ( \"hello!\" )","title":"Context managers"},{"location":"dev-tools/python/#decorators","text":"def is_positive ( func ): def wrapper ( * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return func ( * args , ** kwargs ) return wrapper @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2 Same decorations with class and special method call (): class is_positive (): def __init__ ( self , func ): self . function = func def __call__ ( self , * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return self . function ( * args , ** kwargs ) @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2","title":"Decorators"},{"location":"dev-tools/python/#generators","text":"def squares ( a , b ): i = a while i < b : y yield i ** 2 i += 1 s = squares ( 5 , 10 ) next ( s )","title":"Generators"},{"location":"dev-tools/python/#urllib-module-for-web-requests","text":"You can do a lot of neat stuff with it e.g. access API resources, make different types of HTTP requests like GET, POST, PUT, DELETE etc. Simple request to google.com to see what we get. import urllib.request with urllib . request . urlopen ( \"https://www.google.com/\" ) as url : print ( url . read ( 300 ) . decode ( \"utf-8\" ))","title":"urllib module for web requests"},{"location":"dev-tools/python/#get-request","text":"import urllib.request req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' ) with urllib . request . urlopen ( req ) as resp : print ( resp . read () . decode ( 'utf-8' ))","title":"GET request"},{"location":"dev-tools/python/#post-request","text":"import json import urllib.request # This is our Todo data = { \"userId\" : 101 , \"id\" : 100 , \"title\" : \"This is a POST request\" , \"completed\" : True } # Dump the todo object as a json string data = json . dumps ( data ) req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/' , data = bytes ( data . encode ( \"utf-8\" )), method = \"POST\" ) # Add the appropriate header. req . add_header ( \"Content-type\" , \"application/json; charset=UTF-8\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data )","title":"POST request"},{"location":"dev-tools/python/#put-request","text":"import json import urllib.request # This is our Todo data = { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"This is a PUT request\" , \"completed\" : False } # Dump the todo object as a json string data = json . dumps ( data ) req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' , data = bytes ( data . encode ( \"utf-8\" )), method = \"PUT\" ) # Add the appropriate header. req . add_header ( \"Content-type\" , \"application/json; charset=UTF-8\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data )","title":"PUT request"},{"location":"dev-tools/python/#delete-request","text":"import json import urllib.request req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' , method = \"DELETE\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data )","title":"DELETE request"},{"location":"dev-tools/terraform/","text":"Terraform Up&Running \u00b6 IAM User Policies \u00b6 Add the following Managed Policies to your IAM user, as shown: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonRDSFullAccess CloudWatchFullAccess IAMFullAccess export AWS_ACCESS_KEY_ID =( your access key id ) export AWS_SECRET_ACCESS_KEY =( your secret access key )","title":"Terraform"},{"location":"dev-tools/terraform/#terraform-uprunning","text":"","title":"Terraform Up&amp;Running"},{"location":"dev-tools/terraform/#iam-user-policies","text":"Add the following Managed Policies to your IAM user, as shown: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonRDSFullAccess CloudWatchFullAccess IAMFullAccess export AWS_ACCESS_KEY_ID =( your access key id ) export AWS_SECRET_ACCESS_KEY =( your secret access key )","title":"IAM User Policies"},{"location":"docker/","text":"Docker tips and tricks \u00b6 Docker container like systemctl service \u00b6 If you want to start docker container based service # Write systemd unit file cat << EOF > /etc/systemd/system/docker-container@ecs-agent.service [Unit] Description=Docker Container %I Requires=docker.service After=cloud-final.service [Service] Restart=always ExecStartPre=-/usr/bin/docker rm -f %i ExecStart=/usr/bin/docker run --name %i \\ --privileged \\ --restart=on-failure:10 \\ --volume=/var/run:/var/run \\ --volume=/var/log/ecs/:/log:Z \\ --volume=/var/lib/ecs/data:/data:Z \\ --volume=/etc/ecs:/etc/ecs \\ --net=host \\ --env-file=/etc/ecs/ecs.config \\ amazon/amazon-ecs-agent:latest ExecStop=/usr/bin/docker stop %i [Install] WantedBy=default.target EOF systemctl enable docker-container@ecs-agent.service systemctl start docker-container@ecs-agent.service Portainer \u00b6 Portainer - is very useful GUI for managing Docker's hosts and clusters. Simply open port 9000 on your host, run container and open in your browser host_ip:9000, create user and manage all your containers. docker container run -d \\ -p 9000 :9000 \\ -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer Install Docker \u00b6 The Docker installation package available in the official Ubuntu repository may not be the latest version. To ensure we get the latest version, we\u2019ll install Docker from the official Docker repository. To do that, we\u2019ll add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package. First, update your existing list of packages: sudo apt update Next, install a few prerequisite packages which let apt use packages over HTTPS: sudo apt install apt-transport-https ca-certificates curl software-properties-common Then add the GPG key for the official Docker repository to your system: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add the Docker repository to APT sources: sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" Next, update the package database with the Docker packages from the newly added repo and install Docker: sudo apt update sudo apt install docker-ce Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it\u2019s running: sudo systemctl status docker The output should be similar to the following, showing that the service is active and running: Output: \u25cf docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-07-05 15:08:39 UTC; 2min 55s ago Docs: https://docs.docker.com Main PID: 10096 (dockerd) Tasks: 16 CGroup: /system.slice/docker.service \u251c\u250010096 /usr/bin/dockerd -H fd:// \u2514\u250010113 docker-containerd --config /var/run/docker/containerd/containerd.toml Installing Docker now gives you not just the Docker service (daemon) but also the docker command line utility, or the Docker client. For more comfortable work from your user acc - create the docker group: sudo groupadd docker Add your user to the docker group: sudo usermod -aG docker $USER Understanding containers has to cover things like chroot, the container file system and layers, cgroup isolation, and talk about the pros/cons of containers. An explanation of how these work without a discussion of cgroups and how that facilitates process/memory/network isolation is going to sound weak to an interviewer. So read up on how cgroups are implemented in the kernel. Then learn about the different ways lxc containers work vs docker containers. Then learn about emerging container specification standards. Why are they happening? Understand what\u2019s going on both under the hood in the kernel and the shifts happening in the industry. chroot \u00b6 A chroot environment provides functionality similar to that of a virtual machine, but it is a lighter solution. The captive system doesn\u2019t need a hypervisor to be installed and configured, such as VirtualBox or Virtual Machine Manager. Nor does it need to have a kernel installed in the captive system. The captive system shares your existing kernel. Creating chroot environment \u00b6 We need a directory to act as the root directory of the chroot environment. So that we have a shorthand way of referring to that directory we\u2019ll create a variable and store the name of the directory in it. Here we\u2019re setting up a variable to store a path to the \u201ctestroot\u201d directory. It doesn\u2019t matter if this directory doesn\u2019t exist yet, we\u2019re going to create it soon. If the directory does exist, it should be empty. chr = /home/dave/testroot mkdir -p $chr We need to create directories to hold the portions of the operating system our chroot environment will require. We\u2019re going to set up a minimalist Linux environment that uses Bash as the interactive shell. We\u2019ll also include the touch, rm, and ls commands. That will allow us to use all Bash\u2019s built-in commands and touch, rm, and ls. We\u2019ll be able to create, list and remove files, and use Bash. And\u2014in this simple example\u2014that\u2019s all. mkdir -p $chr / { bin,lib,lib64 } cd $chr Let\u2019s copy the binaries that we need in our minimalist Linux environment from your regular \u201c/bin\u201d directory into our chroot \u201c/bin\u201d directory. The -v (verbose) option makes cp tell us what it is doing as it performs each copy action. cp -v /bin/ { bash,touch,ls,rm } $chr Dependencies \u00b6 These binaries will have dependencies. We need to discover what they are and copy those files into our environment. This way, for example we can add all dependencies for /bin/bash: list = \" $( ldd /bin/bash | egrep -o '/lib.*\\.[0-9]' ) \" for i in $list ; do cp -v --parents \" $i \" \" ${ chr } \" ; done Use that technique to capture the dependencies of each of the other commands. Or you can write one more loop through all your apps. chroot command \u00b6 The last of our dependencies are copied into our chroot environment. We\u2019re finally ready to use the chroot command. This command sets the root of the chroot environment, and specifies which application to run as the shell. sudo chroot $chr /bin/bash Our chroot environment is now active. The terminal window prompt has changed, and the interactive shell is the being handled by the bash shell in our environment. Use exit to leave the chroot environment: exit Open sockets inside docker container \u00b6 docker inspect -f '{{.State.Pid}}' container_name_or_id And once you have the PID, use that as the argument to the target (-t) option of nsenter . For example, to run netstat inside the container network namespace: $ sudo nsenter -t 15652 -n netstat Active Internet connections ( only servers ) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0 .0.0.0:80 0 .0.0.0:* LISTEN","title":"Docker tips"},{"location":"docker/#docker-tips-and-tricks","text":"","title":"Docker tips and tricks"},{"location":"docker/#docker-container-like-systemctl-service","text":"If you want to start docker container based service # Write systemd unit file cat << EOF > /etc/systemd/system/docker-container@ecs-agent.service [Unit] Description=Docker Container %I Requires=docker.service After=cloud-final.service [Service] Restart=always ExecStartPre=-/usr/bin/docker rm -f %i ExecStart=/usr/bin/docker run --name %i \\ --privileged \\ --restart=on-failure:10 \\ --volume=/var/run:/var/run \\ --volume=/var/log/ecs/:/log:Z \\ --volume=/var/lib/ecs/data:/data:Z \\ --volume=/etc/ecs:/etc/ecs \\ --net=host \\ --env-file=/etc/ecs/ecs.config \\ amazon/amazon-ecs-agent:latest ExecStop=/usr/bin/docker stop %i [Install] WantedBy=default.target EOF systemctl enable docker-container@ecs-agent.service systemctl start docker-container@ecs-agent.service","title":"Docker container like systemctl service"},{"location":"docker/#portainer","text":"Portainer - is very useful GUI for managing Docker's hosts and clusters. Simply open port 9000 on your host, run container and open in your browser host_ip:9000, create user and manage all your containers. docker container run -d \\ -p 9000 :9000 \\ -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer","title":"Portainer"},{"location":"docker/#install-docker","text":"The Docker installation package available in the official Ubuntu repository may not be the latest version. To ensure we get the latest version, we\u2019ll install Docker from the official Docker repository. To do that, we\u2019ll add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package. First, update your existing list of packages: sudo apt update Next, install a few prerequisite packages which let apt use packages over HTTPS: sudo apt install apt-transport-https ca-certificates curl software-properties-common Then add the GPG key for the official Docker repository to your system: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add the Docker repository to APT sources: sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" Next, update the package database with the Docker packages from the newly added repo and install Docker: sudo apt update sudo apt install docker-ce Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it\u2019s running: sudo systemctl status docker The output should be similar to the following, showing that the service is active and running: Output: \u25cf docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-07-05 15:08:39 UTC; 2min 55s ago Docs: https://docs.docker.com Main PID: 10096 (dockerd) Tasks: 16 CGroup: /system.slice/docker.service \u251c\u250010096 /usr/bin/dockerd -H fd:// \u2514\u250010113 docker-containerd --config /var/run/docker/containerd/containerd.toml Installing Docker now gives you not just the Docker service (daemon) but also the docker command line utility, or the Docker client. For more comfortable work from your user acc - create the docker group: sudo groupadd docker Add your user to the docker group: sudo usermod -aG docker $USER Understanding containers has to cover things like chroot, the container file system and layers, cgroup isolation, and talk about the pros/cons of containers. An explanation of how these work without a discussion of cgroups and how that facilitates process/memory/network isolation is going to sound weak to an interviewer. So read up on how cgroups are implemented in the kernel. Then learn about the different ways lxc containers work vs docker containers. Then learn about emerging container specification standards. Why are they happening? Understand what\u2019s going on both under the hood in the kernel and the shifts happening in the industry.","title":"Install Docker"},{"location":"docker/#chroot","text":"A chroot environment provides functionality similar to that of a virtual machine, but it is a lighter solution. The captive system doesn\u2019t need a hypervisor to be installed and configured, such as VirtualBox or Virtual Machine Manager. Nor does it need to have a kernel installed in the captive system. The captive system shares your existing kernel.","title":"chroot"},{"location":"docker/#creating-chroot-environment","text":"We need a directory to act as the root directory of the chroot environment. So that we have a shorthand way of referring to that directory we\u2019ll create a variable and store the name of the directory in it. Here we\u2019re setting up a variable to store a path to the \u201ctestroot\u201d directory. It doesn\u2019t matter if this directory doesn\u2019t exist yet, we\u2019re going to create it soon. If the directory does exist, it should be empty. chr = /home/dave/testroot mkdir -p $chr We need to create directories to hold the portions of the operating system our chroot environment will require. We\u2019re going to set up a minimalist Linux environment that uses Bash as the interactive shell. We\u2019ll also include the touch, rm, and ls commands. That will allow us to use all Bash\u2019s built-in commands and touch, rm, and ls. We\u2019ll be able to create, list and remove files, and use Bash. And\u2014in this simple example\u2014that\u2019s all. mkdir -p $chr / { bin,lib,lib64 } cd $chr Let\u2019s copy the binaries that we need in our minimalist Linux environment from your regular \u201c/bin\u201d directory into our chroot \u201c/bin\u201d directory. The -v (verbose) option makes cp tell us what it is doing as it performs each copy action. cp -v /bin/ { bash,touch,ls,rm } $chr","title":"Creating chroot environment"},{"location":"docker/#dependencies","text":"These binaries will have dependencies. We need to discover what they are and copy those files into our environment. This way, for example we can add all dependencies for /bin/bash: list = \" $( ldd /bin/bash | egrep -o '/lib.*\\.[0-9]' ) \" for i in $list ; do cp -v --parents \" $i \" \" ${ chr } \" ; done Use that technique to capture the dependencies of each of the other commands. Or you can write one more loop through all your apps.","title":"Dependencies"},{"location":"docker/#chroot-command","text":"The last of our dependencies are copied into our chroot environment. We\u2019re finally ready to use the chroot command. This command sets the root of the chroot environment, and specifies which application to run as the shell. sudo chroot $chr /bin/bash Our chroot environment is now active. The terminal window prompt has changed, and the interactive shell is the being handled by the bash shell in our environment. Use exit to leave the chroot environment: exit","title":"chroot command"},{"location":"docker/#open-sockets-inside-docker-container","text":"docker inspect -f '{{.State.Pid}}' container_name_or_id And once you have the PID, use that as the argument to the target (-t) option of nsenter . For example, to run netstat inside the container network namespace: $ sudo nsenter -t 15652 -n netstat Active Internet connections ( only servers ) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0 .0.0.0:80 0 .0.0.0:* LISTEN","title":"Open sockets inside docker container"},{"location":"docker/commans/","text":"Docker commands cheat-sheet \u00b6 Docker docs and manuals Run busybox inside docker-compose \u00b6 1 docker run --rm -it --network <NETWORK_NAME> busybox Containers \u00b6 docker container ls # docker ps docker container top { cont_name } # top command for cont docker container inspect { cont_name } # json with all cont parameters docker container inspect { cont_name } --format '{{ .NetworkSettings.IPAddress}}' docekr container stats # like watch command but for containers docker container start # start stopped cont docker container post { cont_name } # shows shared ports Network \u00b6 docker network ls docker network inspect { network_name } docker network create --driver docker network connect { network_name } docker network disconnect { network_name } Reset \u00b6 Delete all running and stopped containers \u00b6 docker container rm -f $( docker ps -aq ) Unused cont prune on current host \u00b6 docker system prune --all --force --volumes Full docker prune on current host \u00b6 docker stop $( docker container ls -a -q ) && docker system prune -a -f --volumes Print last container\u2019s logs \u00b6 docker container logs --tail 100 web","title":"Cheatsheet"},{"location":"docker/commans/#docker-commands-cheat-sheet","text":"Docker docs and manuals","title":"Docker commands cheat-sheet"},{"location":"docker/commans/#run-busybox-inside-docker-compose","text":"1 docker run --rm -it --network <NETWORK_NAME> busybox","title":"Run busybox inside docker-compose"},{"location":"docker/commans/#containers","text":"docker container ls # docker ps docker container top { cont_name } # top command for cont docker container inspect { cont_name } # json with all cont parameters docker container inspect { cont_name } --format '{{ .NetworkSettings.IPAddress}}' docekr container stats # like watch command but for containers docker container start # start stopped cont docker container post { cont_name } # shows shared ports","title":"Containers"},{"location":"docker/commans/#network","text":"docker network ls docker network inspect { network_name } docker network create --driver docker network connect { network_name } docker network disconnect { network_name }","title":"Network"},{"location":"docker/commans/#reset","text":"","title":"Reset"},{"location":"docker/commans/#delete-all-running-and-stopped-containers","text":"docker container rm -f $( docker ps -aq )","title":"Delete all running and stopped containers"},{"location":"docker/commans/#unused-cont-prune-on-current-host","text":"docker system prune --all --force --volumes","title":"Unused cont prune on current host"},{"location":"docker/commans/#full-docker-prune-on-current-host","text":"docker stop $( docker container ls -a -q ) && docker system prune -a -f --volumes","title":"Full docker prune on current host"},{"location":"docker/commans/#print-last-containers-logs","text":"docker container logs --tail 100 web","title":"Print last container\u2019s logs"},{"location":"kubernetes/advanced_tools/","text":"","title":"Advanced tools"},{"location":"kubernetes/configMaps/","text":"ConfigMaps and Secrets \u00b6","title":"ConfigMaps and Secrets"},{"location":"kubernetes/configMaps/#configmaps-and-secrets","text":"","title":"ConfigMaps and Secrets"},{"location":"kubernetes/helm/","text":"Helm \u00b6 Helm package manager","title":"Helm"},{"location":"kubernetes/helm/#helm","text":"Helm package manager","title":"Helm"},{"location":"kubernetes/ingress/","text":"Ingress \u00b6 An Ingress is a collection of rules that allow inbound connections to reach the cluster Services. To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following: 1 2 3 4 5 TLS (Transport Layer Security) Name-based virtual hosting Fanout routing Loadbalancing Custom rules. With Ingress, users do not connect directly to a Service. Users reach the Ingress endpoint, and, from there, the request is forwarded to the desired Service. You can see an example of a sample Ingress definition below: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : virtual-host-ingress namespace : default spec : rules : - host : blue.example.com http : paths : - backend : serviceName : webserver-blue-svc servicePort : 80 - host : green.example.com http : paths : - backend : serviceName : webserver-green-svc servicePort : 80 In the example above, user requests to both blue.example.com and green.example.com would go to the same Ingress endpoint, and, from there, they would be forwarded to webserver-blue-svc, and webserver-green-svc, respectively. This is an example of a Name-Based Virtual Hosting Ingress rule. We can also have Fanout Ingress rules, when requests to example.com/blue and example.com/green would be forwarded to webserver-blue-svc and webserver-green-svc, respectively: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : fan-out-ingress namespace : default spec : rules : - host : example.com http : paths : - path : /blue backend : serviceName : webserver-blue-svc servicePort : 80 - path : /green backend : serviceName : webserver-green-svc servicePort : 80 The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller. An Ingress Controller is an application watching the Master Node's API server for changes in the Ingress resources and updates the Layer 7 Load Balancer accordingly. Kubernetes supports different Ingress Controllers, and, if needed, we can also build our own. GCE L7 Load Balancer Controller and Nginx Ingress Controller are commonly used Ingress Controllers. Other controllers are Istio, Kong, Traefik, etc. # ingress.yaml apiVersion : v1 kind : Namespace metadata : name : nginx-ingress --- apiVersion : v1 kind : Secret metadata : name : default-server-secret namespace : nginx-ingress type : Opaque data : tls.crt : LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= tls.key : LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBdi91RWM4b1JkMHUvZXVJTHNFK1RYZUprckxMMnNJNGFWaEMvYjVyYy9XMlRiNHEvClJOcktGMEdYaVN1eE9ycXgrajlnamx4NXFjdnhkenRKbXNFUkJ1Z1B0ME9hVGtIekhvb3FVWmcwZGxmZ1dkT0EKUTZMNTdlT1l0Q29VOUZ4amRXdzZUVVRJVUQ4R0JsRlNjSVo0b1hFTkhzbysyR3VTTWk2Zk1wTVM3YUhudzFtMApxWkdvRWEzWFNyZEJ6eGc2clhkcUNlUDlCMXl3VmRyYURiUzc1aGQzdUdETDU4cGszOVFqVUFQaHpxdmRoK1JWClZGNGJCaW9CbTVpeTlZTW1hWVhsMm0wTGZzeTZuUTRRdFFzdEdNVWozcGJtdlFmazJBNnljeGRFeFpkZFZsdmwKMm82MjBsMllxcHFDZEtCRThCay90elFIVTlKcU56cHpoOUJUTXdJREFRQUJBb0lCQVFDZklHbXowOHhRVmorNwpLZnZJUXQwQ0YzR2MxNld6eDhVNml4MHg4Mm15d1kxUUNlL3BzWE9LZlRxT1h1SENyUlp5TnUvZ2IvUUQ4bUFOCmxOMjRZTWl0TWRJODg5TEZoTkp3QU5OODJDeTczckM5bzVvUDlkazAvYzRIbjAzSkVYNzZ5QjgzQm9rR1FvYksKMjhMNk0rdHUzUmFqNjd6Vmc2d2szaEhrU0pXSzBwV1YrSjdrUkRWYmhDYUZhNk5nMUZNRWxhTlozVDhhUUtyQgpDUDNDeEFTdjYxWTk5TEI4KzNXWVFIK3NYaTVGM01pYVNBZ1BkQUk3WEh1dXFET1lvMU5PL0JoSGt1aVg2QnRtCnorNTZud2pZMy8yUytSRmNBc3JMTnIwMDJZZi9oY0IraVlDNzVWYmcydVd6WTY3TWdOTGQ5VW9RU3BDRkYrVm4KM0cyUnhybnhBb0dCQU40U3M0ZVlPU2huMVpQQjdhTUZsY0k2RHR2S2ErTGZTTXFyY2pOZjJlSEpZNnhubmxKdgpGenpGL2RiVWVTbWxSekR0WkdlcXZXaHFISy9iTjIyeWJhOU1WMDlRQ0JFTk5jNmtWajJTVHpUWkJVbEx4QzYrCk93Z0wyZHhKendWelU0VC84ajdHalRUN05BZVpFS2FvRHFyRG5BYWkyaW5oZU1JVWZHRXFGKzJyQW9HQkFOMVAKK0tZL0lsS3RWRzRKSklQNzBjUis3RmpyeXJpY05iWCtQVzUvOXFHaWxnY2grZ3l4b25BWlBpd2NpeDN3QVpGdwpaZC96ZFB2aTBkWEppc1BSZjRMazg5b2pCUmpiRmRmc2l5UmJYbyt3TFU4NUhRU2NGMnN5aUFPaTVBRHdVU0FkCm45YWFweUNweEFkREtERHdObit3ZFhtaTZ0OHRpSFRkK3RoVDhkaVpBb0dCQUt6Wis1bG9OOTBtYlF4VVh5YUwKMjFSUm9tMGJjcndsTmVCaWNFSmlzaEhYa2xpSVVxZ3hSZklNM2hhUVRUcklKZENFaHFsV01aV0xPb2I2NTNyZgo3aFlMSXM1ZUtka3o0aFRVdnpldm9TMHVXcm9CV2xOVHlGanIrSWhKZnZUc0hpOGdsU3FkbXgySkJhZUFVWUNXCndNdlQ4NmNLclNyNkQrZG8wS05FZzFsL0FvR0FlMkFVdHVFbFNqLzBmRzgrV3hHc1RFV1JqclRNUzRSUjhRWXQKeXdjdFA4aDZxTGxKUTRCWGxQU05rMXZLTmtOUkxIb2pZT2pCQTViYjhibXNVU1BlV09NNENoaFJ4QnlHbmR2eAphYkJDRkFwY0IvbEg4d1R0alVZYlN5T294ZGt5OEp0ek90ajJhS0FiZHd6NlArWDZDODhjZmxYVFo5MWpYL3RMCjF3TmRKS2tDZ1lCbyt0UzB5TzJ2SWFmK2UwSkN5TGhzVDQ5cTN3Zis2QWVqWGx2WDJ1VnRYejN5QTZnbXo5aCsKcDNlK2JMRUxwb3B0WFhNdUFRR0xhUkcrYlNNcjR5dERYbE5ZSndUeThXczNKY3dlSTdqZVp2b0ZpbmNvVlVIMwphdmxoTUVCRGYxSjltSDB5cDBwWUNaS2ROdHNvZEZtQktzVEtQMjJhTmtsVVhCS3gyZzR6cFE9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo= --- apiVersion : v1 kind : ServiceAccount metadata : name : nginx-ingress namespace : nginx-ingress --- kind : ConfigMap apiVersion : v1 metadata : name : nginx-config namespace : nginx-ingress data : --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : nginx-ingress namespace : nginx-ingress spec : replicas : 1 selector : matchLabels : app : nginx-ingress template : metadata : labels : app : nginx-ingress spec : serviceAccountName : nginx-ingress containers : - image : nginx/nginx-ingress:edge imagePullPolicy : Always name : nginx-ingress ports : - name : http containerPort : 80 - name : https containerPort : 443 env : - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name args : - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret --- apiVersion : v1 kind : Service metadata : name : nginx-ingress namespace : nginx-ingress spec : type : NodePort ports : - port : 80 targetPort : 80 protocol : TCP name : http - port : 443 targetPort : 443 protocol : TCP name : https selector : app : nginx-ingress externalIPs : - 172.17.0.44 # ingress-rules.yaml apiVersion : extensions/v1beta1 kind : Ingress metadata : name : webapp-ingress spec : rules : - host : my.kubernetes.example http : paths : - path : /webapp1 backend : serviceName : webapp1-svc servicePort : 80 - path : /webapp2 backend : serviceName : webapp2-svc servicePort : 80 - backend : serviceName : webapp3-svc servicePort : 80 $ kubectl get inggress","title":"Ingress"},{"location":"kubernetes/ingress/#ingress","text":"An Ingress is a collection of rules that allow inbound connections to reach the cluster Services. To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following: 1 2 3 4 5 TLS (Transport Layer Security) Name-based virtual hosting Fanout routing Loadbalancing Custom rules. With Ingress, users do not connect directly to a Service. Users reach the Ingress endpoint, and, from there, the request is forwarded to the desired Service. You can see an example of a sample Ingress definition below: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : virtual-host-ingress namespace : default spec : rules : - host : blue.example.com http : paths : - backend : serviceName : webserver-blue-svc servicePort : 80 - host : green.example.com http : paths : - backend : serviceName : webserver-green-svc servicePort : 80 In the example above, user requests to both blue.example.com and green.example.com would go to the same Ingress endpoint, and, from there, they would be forwarded to webserver-blue-svc, and webserver-green-svc, respectively. This is an example of a Name-Based Virtual Hosting Ingress rule. We can also have Fanout Ingress rules, when requests to example.com/blue and example.com/green would be forwarded to webserver-blue-svc and webserver-green-svc, respectively: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : fan-out-ingress namespace : default spec : rules : - host : example.com http : paths : - path : /blue backend : serviceName : webserver-blue-svc servicePort : 80 - path : /green backend : serviceName : webserver-green-svc servicePort : 80 The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller. An Ingress Controller is an application watching the Master Node's API server for changes in the Ingress resources and updates the Layer 7 Load Balancer accordingly. Kubernetes supports different Ingress Controllers, and, if needed, we can also build our own. GCE L7 Load Balancer Controller and Nginx Ingress Controller are commonly used Ingress Controllers. Other controllers are Istio, Kong, Traefik, etc. # ingress.yaml apiVersion : v1 kind : Namespace metadata : name : nginx-ingress --- apiVersion : v1 kind : Secret metadata : name : default-server-secret namespace : nginx-ingress type : Opaque data : tls.crt : LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= tls.key : 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 --- apiVersion : v1 kind : ServiceAccount metadata : name : nginx-ingress namespace : nginx-ingress --- kind : ConfigMap apiVersion : v1 metadata : name : nginx-config namespace : nginx-ingress data : --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : nginx-ingress namespace : nginx-ingress spec : replicas : 1 selector : matchLabels : app : nginx-ingress template : metadata : labels : app : nginx-ingress spec : serviceAccountName : nginx-ingress containers : - image : nginx/nginx-ingress:edge imagePullPolicy : Always name : nginx-ingress ports : - name : http containerPort : 80 - name : https containerPort : 443 env : - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name args : - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret --- apiVersion : v1 kind : Service metadata : name : nginx-ingress namespace : nginx-ingress spec : type : NodePort ports : - port : 80 targetPort : 80 protocol : TCP name : http - port : 443 targetPort : 443 protocol : TCP name : https selector : app : nginx-ingress externalIPs : - 172.17.0.44 # ingress-rules.yaml apiVersion : extensions/v1beta1 kind : Ingress metadata : name : webapp-ingress spec : rules : - host : my.kubernetes.example http : paths : - path : /webapp1 backend : serviceName : webapp1-svc servicePort : 80 - path : /webapp2 backend : serviceName : webapp2-svc servicePort : 80 - backend : serviceName : webapp3-svc servicePort : 80 $ kubectl get inggress","title":"Ingress"},{"location":"kubernetes/kubectl/","text":"kubectl cheatsheet \u00b6 Connection details \u00b6 To look at the connection details, we can either see the content of the ~/.kube/config file (on Linux) or run the following command: $ kubectl config view Cluster info \u00b6 kubectl cluster-info When not using the kubectl proxy, we need to authenticate to the API server when sending API requests. We can authenticate by providing a Bearer Token when issuing a curl, or by providing a set of keys and certificates. A Bearer Token is an access token which is generated by the authentication server (the API server on the master node) and given back to the client. Using that token, the client can connect back to the Kubernetes API server without providing further authentication details, and then, access resources. Get the token: \u00b6 $ TOKEN = $( kubectl describe secret -n kube-system $( kubectl get secrets -n kube-system | grep default | cut -f1 -d ' ' ) | grep -E '^token' | cut -f2 -d ':' | tr -d '\\t' | tr -d \" \" ) Get the API server endpoint: \u00b6 $ APISERVER = $( kubectl config view | grep https | cut -f 2 - -d \":\" | tr -d \" \" ) Access the API server \u00b6 $ curl $APISERVER --header \"Authorization: Bearer $TOKEN \" --insecure Instead of the access token, we can extract the client certificate, client key, and certificate authority data from the .kube/config file. Once extracted, they are encoded and then passed with a curl command for authentication. The new curl command looks similar to: $ curl $APISERVER --cert encoded-cert --key encoded-key --cacert encoded-ca List the Pods \u00b6 Along with their attached Labels \u00b6 With the -L option to the kubectl get pods command, we add extra columns in the output to list Pods with their attached Label keys and their values. In the following example, we are listing Pods with the Label keys k8s-app and label2 : $ kubectl get pods -L k8s-app,label2 NAME READY STATUS RESTARTS AGE K8S-APP LABEL2 webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 16m webserver Pods with a given Label \u00b6 To use a selector with the kubectl get pods command, we can use the -l option. In the following example, we are selecting all the Pods that have the k8s-app Label key set to value webserver : $ kubectl get pods -l k8s-app = webserver NAME READY STATUS RESTARTS AGE webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 23m webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 24m webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 23m Look at a Pod's Details \u00b6 We can look at an object's details using kubectl describe command. In the following example, you can see a Pod's description: $ kubectl describe pod webserver-c8f4d5fbc-ggqkj Name: webserver-c8f4d5fbc-ggqkj Namespace: default Priority: 0 Node: minikube/10.0.2.15 Start Time: Sat, 05 Oct 2019 14 :37:34 +0300 Labels: k8s-app = webserver pod-template-hash = c8f4d5fbc Annotations: <none> Status: Running IP: 172 .17.0.7 IPs: IP: 172 .17.0.7 Controlled By: ReplicaSet/webserver-c8f4d5fbc Containers: webserver: Container ID: docker://b7d6bd7ce9eaefe48523d486f7174ac748571546bec4d8674b89d4438c8707da Image: nginx:alpine Image ID: docker-pullable://nginx@sha256:77f340700d08fd45026823f44fc0010a5bd2237c2d049178b473cd2ad977d071 Port: <none> Host Port: <none> State: Running Started: Sat, 05 Oct 2019 14 :37:36 +0300 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-r6llk ( ro ) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-r6llk: Type: Secret ( a volume populated by a Secret ) SecretName: default-token-r6llk Optional: false QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled <unknown> default-scheduler Successfully assigned default/webserver-c8f4d5fbc-ggqkj to minikube Normal Pulled 19m kubelet, minikube Container image \"nginx:alpine\" already present on machine Normal Created 19m kubelet, minikube Created container webserver Normal Started 19m kubelet, minikube Started container webserver Exposing an Application \u00b6 For a NodePort ServiceType, Kubernetes opens up a static port on all the worker nodes. If we connect to that port from any node, we are proxied to the ClusterIP of the Service. Next, let's use the NodePort ServiceType while creating a Service. Create a webserver-svc.yaml file with the following content: apiVersion : v1 kind : Service metadata : name : web-service labels : run : web-service spec : type : NodePort ports : - port : 80 protocol : TCP selector : app : nginx Using kubectl, create the Service: $ kubectl create -f webserver-svc.yaml service/web-service created A more direct method of creating a Service is by exposing the previously created Deployment (this method requires an existing Deployment). Expose a Deployment with the kubectl expose command: $ kubectl expose deployment webserver --name = web-service --type = NodePort service/web-service exposed Create an NGINX Pod kubectl run --generator=run-pod/v1 nginx --image=nginx Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml label Create a deployment kubectl run --generator=deployment/v1beta1 nginx --image=nginx Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run -o yaml Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml Save it to a file - (If you need to modify or add some other details) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml >\u00c2 nginx-deployment.yaml more kubectl commands and manuals is here","title":"kubectl cheatsheet"},{"location":"kubernetes/kubectl/#kubectl-cheatsheet","text":"","title":"kubectl cheatsheet"},{"location":"kubernetes/kubectl/#connection-details","text":"To look at the connection details, we can either see the content of the ~/.kube/config file (on Linux) or run the following command: $ kubectl config view","title":"Connection details"},{"location":"kubernetes/kubectl/#cluster-info","text":"kubectl cluster-info When not using the kubectl proxy, we need to authenticate to the API server when sending API requests. We can authenticate by providing a Bearer Token when issuing a curl, or by providing a set of keys and certificates. A Bearer Token is an access token which is generated by the authentication server (the API server on the master node) and given back to the client. Using that token, the client can connect back to the Kubernetes API server without providing further authentication details, and then, access resources.","title":"Cluster info"},{"location":"kubernetes/kubectl/#get-the-token","text":"$ TOKEN = $( kubectl describe secret -n kube-system $( kubectl get secrets -n kube-system | grep default | cut -f1 -d ' ' ) | grep -E '^token' | cut -f2 -d ':' | tr -d '\\t' | tr -d \" \" )","title":"Get the token:"},{"location":"kubernetes/kubectl/#get-the-api-server-endpoint","text":"$ APISERVER = $( kubectl config view | grep https | cut -f 2 - -d \":\" | tr -d \" \" )","title":"Get the API server endpoint:"},{"location":"kubernetes/kubectl/#access-the-api-server","text":"$ curl $APISERVER --header \"Authorization: Bearer $TOKEN \" --insecure Instead of the access token, we can extract the client certificate, client key, and certificate authority data from the .kube/config file. Once extracted, they are encoded and then passed with a curl command for authentication. The new curl command looks similar to: $ curl $APISERVER --cert encoded-cert --key encoded-key --cacert encoded-ca","title":"Access the API server"},{"location":"kubernetes/kubectl/#list-the-pods","text":"","title":"List the Pods"},{"location":"kubernetes/kubectl/#along-with-their-attached-labels","text":"With the -L option to the kubectl get pods command, we add extra columns in the output to list Pods with their attached Label keys and their values. In the following example, we are listing Pods with the Label keys k8s-app and label2 : $ kubectl get pods -L k8s-app,label2 NAME READY STATUS RESTARTS AGE K8S-APP LABEL2 webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 16m webserver","title":"Along with their attached Labels"},{"location":"kubernetes/kubectl/#pods-with-a-given-label","text":"To use a selector with the kubectl get pods command, we can use the -l option. In the following example, we are selecting all the Pods that have the k8s-app Label key set to value webserver : $ kubectl get pods -l k8s-app = webserver NAME READY STATUS RESTARTS AGE webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 23m webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 24m webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 23m","title":"Pods with a given Label"},{"location":"kubernetes/kubectl/#look-at-a-pods-details","text":"We can look at an object's details using kubectl describe command. In the following example, you can see a Pod's description: $ kubectl describe pod webserver-c8f4d5fbc-ggqkj Name: webserver-c8f4d5fbc-ggqkj Namespace: default Priority: 0 Node: minikube/10.0.2.15 Start Time: Sat, 05 Oct 2019 14 :37:34 +0300 Labels: k8s-app = webserver pod-template-hash = c8f4d5fbc Annotations: <none> Status: Running IP: 172 .17.0.7 IPs: IP: 172 .17.0.7 Controlled By: ReplicaSet/webserver-c8f4d5fbc Containers: webserver: Container ID: docker://b7d6bd7ce9eaefe48523d486f7174ac748571546bec4d8674b89d4438c8707da Image: nginx:alpine Image ID: docker-pullable://nginx@sha256:77f340700d08fd45026823f44fc0010a5bd2237c2d049178b473cd2ad977d071 Port: <none> Host Port: <none> State: Running Started: Sat, 05 Oct 2019 14 :37:36 +0300 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-r6llk ( ro ) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-r6llk: Type: Secret ( a volume populated by a Secret ) SecretName: default-token-r6llk Optional: false QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled <unknown> default-scheduler Successfully assigned default/webserver-c8f4d5fbc-ggqkj to minikube Normal Pulled 19m kubelet, minikube Container image \"nginx:alpine\" already present on machine Normal Created 19m kubelet, minikube Created container webserver Normal Started 19m kubelet, minikube Started container webserver","title":"Look at a Pod's Details"},{"location":"kubernetes/kubectl/#exposing-an-application","text":"For a NodePort ServiceType, Kubernetes opens up a static port on all the worker nodes. If we connect to that port from any node, we are proxied to the ClusterIP of the Service. Next, let's use the NodePort ServiceType while creating a Service. Create a webserver-svc.yaml file with the following content: apiVersion : v1 kind : Service metadata : name : web-service labels : run : web-service spec : type : NodePort ports : - port : 80 protocol : TCP selector : app : nginx Using kubectl, create the Service: $ kubectl create -f webserver-svc.yaml service/web-service created A more direct method of creating a Service is by exposing the previously created Deployment (this method requires an existing Deployment). Expose a Deployment with the kubectl expose command: $ kubectl expose deployment webserver --name = web-service --type = NodePort service/web-service exposed Create an NGINX Pod kubectl run --generator=run-pod/v1 nginx --image=nginx Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml label Create a deployment kubectl run --generator=deployment/v1beta1 nginx --image=nginx Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run -o yaml Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml Save it to a file - (If you need to modify or add some other details) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml >\u00c2 nginx-deployment.yaml more kubectl commands and manuals is here","title":"Exposing an Application"},{"location":"kubernetes/monitoring/","text":"Services Monitoring Workflow \u00b6 Services Monitoring Workflow consists of three main components: 1 Collection agent, Metrics Server, and Dashboards. Typical workflow, including most common components: Monitoring agent collects node metrics. cAdvisor collects containers and pods metrics. Monitoring Aggregation service collects data from its own agent and cAdvisor. Data is stored in the monitoring system\u2019s storage. Monitoring aggregation service exposes metrics through APIs and dashboards. A Few Notes: Prometheus is the official monitoring server sponsored and incubated by CNCF. It integrates directly with cAdvisor. You don\u2019t need to install a 3 rd party agent to retrieve additional metrics about your containers. However, if you need deeper insights about each node, you need to install an agent of your choice \u2014 see Prometheus integrations and third-party exporters page. Almost all monitoring systems piggyback on Kubernetes scheduling and orchestration. For example, their agents are installed as DeomonSets and depend on Kubernetes scheduler to have an instance scheduled on each node. Most monitoring agents depend on Kubelet to collect container relevant metrics, which in turn depends on cAdvisor. Very few agents collect container relevant details independently. Most monitoring aggregation services depend on agents pushing metrics to them. Prometheus is an exception. It pulls metrics out of the installed agents. What to monitor \u00b6 Ideal Services Workflow depends on this factors: - collection of relevant metrics - perception of continuous changes inside the k8s cluster. A good pipeline should focus on collecting relevant metrics. There are plenty of agents that can collect OS and process-level metrics. But you will find very few out there that can collect details about containers running at a given node, such as the number of running containers, container state, docker engine metrics, etc. cAdvisor is the best agent for this job. Perception of continuous changes means that the monitoring pipeline is aware of different pods, containers instances and can relate them to their parent entities, i.e. Deployment, Statefulsets, Namespace, etc. It also means that the metrics server is aware of system-wide metrics that should be visible to users, such as the number of pending pods, nodes status, etc. TL;DR \u00b6 You need to differentiate between core metrics pipeline and the services pipeline. You should pick the best pipeline that works for your needs. The community official metrics collector tool is Prometheus. Use Grafana Dashboards for visualization . But not for alerting.","title":"Monitoring"},{"location":"kubernetes/monitoring/#services-monitoring-workflow","text":"Services Monitoring Workflow consists of three main components: 1 Collection agent, Metrics Server, and Dashboards. Typical workflow, including most common components: Monitoring agent collects node metrics. cAdvisor collects containers and pods metrics. Monitoring Aggregation service collects data from its own agent and cAdvisor. Data is stored in the monitoring system\u2019s storage. Monitoring aggregation service exposes metrics through APIs and dashboards. A Few Notes: Prometheus is the official monitoring server sponsored and incubated by CNCF. It integrates directly with cAdvisor. You don\u2019t need to install a 3 rd party agent to retrieve additional metrics about your containers. However, if you need deeper insights about each node, you need to install an agent of your choice \u2014 see Prometheus integrations and third-party exporters page. Almost all monitoring systems piggyback on Kubernetes scheduling and orchestration. For example, their agents are installed as DeomonSets and depend on Kubernetes scheduler to have an instance scheduled on each node. Most monitoring agents depend on Kubelet to collect container relevant metrics, which in turn depends on cAdvisor. Very few agents collect container relevant details independently. Most monitoring aggregation services depend on agents pushing metrics to them. Prometheus is an exception. It pulls metrics out of the installed agents.","title":"Services Monitoring Workflow"},{"location":"kubernetes/monitoring/#what-to-monitor","text":"Ideal Services Workflow depends on this factors: - collection of relevant metrics - perception of continuous changes inside the k8s cluster. A good pipeline should focus on collecting relevant metrics. There are plenty of agents that can collect OS and process-level metrics. But you will find very few out there that can collect details about containers running at a given node, such as the number of running containers, container state, docker engine metrics, etc. cAdvisor is the best agent for this job. Perception of continuous changes means that the monitoring pipeline is aware of different pods, containers instances and can relate them to their parent entities, i.e. Deployment, Statefulsets, Namespace, etc. It also means that the metrics server is aware of system-wide metrics that should be visible to users, such as the number of pending pods, nodes status, etc.","title":"What to monitor"},{"location":"kubernetes/monitoring/#tldr","text":"You need to differentiate between core metrics pipeline and the services pipeline. You should pick the best pipeline that works for your needs. The community official metrics collector tool is Prometheus. Use Grafana Dashboards for visualization . But not for alerting.","title":"TL;DR"},{"location":"kubernetes/networking/","text":"About Flanel: https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c A ServiceAccount is required to login. A ClusterRoleBinding is used to assign the new ServiceAccount (admin-user) the role of cluster-admin on the cluster. cat <<EOF | kubectl create -f - apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : kube-system --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : kube-system EOF This means they can control all aspects of Kubernetes. With ClusterRoleBinding and RBAC, different level of permissions can be defined based on security requirements. More information on creating a user for the Dashboard can be found in the Dashboard documentation. Once the ServiceAccount has been created, the token to login can be found with: kubectl -n kube-system describe secret $( kubectl -n kube-system get secret | grep admin-user | awk '{print $1}' ) cluster-ip.yaml apiVersion : v1 kind : Service metadata : name : webapp1-clusterip-svc labels : app : webapp1-clusterip spec : ports : - port : 80 selector : app : webapp1-clusterip --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-clusterip-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-clusterip spec : containers : - name : webapp1-clusterip-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- export CLUSTER_IP = $( kubectl get services/webapp1-clusterip-svc -o go-template = '{{(index .spec.clusterIP)}}' ) clusterip-target.yaml apiVersion : v1 kind : Service metadata : name : webapp1-clusterip-targetport-svc labels : app : webapp1-clusterip-targetport spec : ports : - port : 8080 targetPort : 80 selector : app : webapp1-clusterip-targetport --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-clusterip-targetport-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-clusterip-targetport spec : containers : - name : webapp1-clusterip-targetport-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- export CLUSTER_IP = $( kubectl get services/webapp1-clusterip-targetport-svc -o go-template = '{{(index .spec.clusterIP)}}' ) nodeport.yaml apiVersion : v1 kind : Service metadata : name : webapp1-nodeport-svc labels : app : webapp1-nodeport spec : type : NodePort ports : - port : 80 nodePort : 30080 selector : app : webapp1-nodeport --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-nodeport-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-nodeport spec : containers : - name : webapp1-nodeport-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- externalip.yaml apiVersion : v1 kind : Service metadata : name : webapp1-externalip-svc labels : app : webapp1-externalip spec : ports : - port : 80 externalIPs : - 172.17.0.82 selector : app : webapp1-externalip --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-externalip-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-externalip spec : containers : - name : webapp1-externalip-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- cloudprovider.yaml \u00b6 apiVersion : extensions/v1beta1 kind : DaemonSet metadata : name : kube-keepalived-vip namespace : kube-system spec : template : metadata : labels : name : kube-keepalived-vip spec : hostNetwork : true containers : - image : gcr.io/google_containers/kube-keepalived-vip:0.9 name : kube-keepalived-vip imagePullPolicy : Always securityContext : privileged : true volumeMounts : - mountPath : /lib/modules name : modules readOnly : true - mountPath : /dev name : dev # use downward API env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace # to use unicast args : - --services-configmap=kube-system/vip-configmap # unicast uses the ip of the nodes instead of multicast # this is useful if running in cloud providers (like AWS) #- --use-unicast=true volumes : - name : modules hostPath : path : /lib/modules - name : dev hostPath : path : /dev nodeSelector : # type: worker # adjust this to match your worker nodes --- ## We also create an empty ConfigMap to hold our config apiVersion : v1 kind : ConfigMap metadata : name : vip-configmap namespace : kube-system data : --- apiVersion : apps/v1beta1 kind : Deployment metadata : labels : app : keepalived-cloud-provider name : keepalived-cloud-provider namespace : kube-system spec : replicas : 1 revisionHistoryLimit : 2 selector : matchLabels : app : keepalived-cloud-provider strategy : type : RollingUpdate template : metadata : annotations : scheduler.alpha.kubernetes.io/critical-pod : \"\" scheduler.alpha.kubernetes.io/tolerations : '[{\"key\":\"CriticalAddonsOnly\", \"operator\":\"Exists\"}]' labels : app : keepalived-cloud-provider spec : containers : - name : keepalived-cloud-provider image : quay.io/munnerz/keepalived-cloud-provider:0.0.1 imagePullPolicy : IfNotPresent env : - name : KEEPALIVED_NAMESPACE value : kube-system - name : KEEPALIVED_CONFIG_MAP value : vip-configmap - name : KEEPALIVED_SERVICE_CIDR value : 10.10.0.0/26 # pick a CIDR that is explicitly reserved for keepalived volumeMounts : - name : certs mountPath : /etc/ssl/certs resources : requests : cpu : 200m livenessProbe : httpGet : path : /healthz port : 10252 host : 127.0.0.1 initialDelaySeconds : 15 timeoutSeconds : 15 failureThreshold : 8 volumes : - name : certs hostPath : path : /etc/ssl/certs loadbalancer.yaml \u00b6 apiVersion : v1 kind : Service metadata : name : webapp1-loadbalancer-svc labels : app : webapp1-loadbalancer spec : type : LoadBalancer ports : - port : 80 selector : app : webapp1-loadbalancer --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-loadbalancer-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-loadbalancer spec : containers : - name : webapp1-loadbalancer-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- export LoadBalancerIP = $( kubectl get services/webapp1-loadbalancer-svc -o go-template = '{{(index .status.loadBalancer.ingress 0).ip}}' )","title":"Networking"},{"location":"kubernetes/networking/#cloudprovideryaml","text":"apiVersion : extensions/v1beta1 kind : DaemonSet metadata : name : kube-keepalived-vip namespace : kube-system spec : template : metadata : labels : name : kube-keepalived-vip spec : hostNetwork : true containers : - image : gcr.io/google_containers/kube-keepalived-vip:0.9 name : kube-keepalived-vip imagePullPolicy : Always securityContext : privileged : true volumeMounts : - mountPath : /lib/modules name : modules readOnly : true - mountPath : /dev name : dev # use downward API env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace # to use unicast args : - --services-configmap=kube-system/vip-configmap # unicast uses the ip of the nodes instead of multicast # this is useful if running in cloud providers (like AWS) #- --use-unicast=true volumes : - name : modules hostPath : path : /lib/modules - name : dev hostPath : path : /dev nodeSelector : # type: worker # adjust this to match your worker nodes --- ## We also create an empty ConfigMap to hold our config apiVersion : v1 kind : ConfigMap metadata : name : vip-configmap namespace : kube-system data : --- apiVersion : apps/v1beta1 kind : Deployment metadata : labels : app : keepalived-cloud-provider name : keepalived-cloud-provider namespace : kube-system spec : replicas : 1 revisionHistoryLimit : 2 selector : matchLabels : app : keepalived-cloud-provider strategy : type : RollingUpdate template : metadata : annotations : scheduler.alpha.kubernetes.io/critical-pod : \"\" scheduler.alpha.kubernetes.io/tolerations : '[{\"key\":\"CriticalAddonsOnly\", \"operator\":\"Exists\"}]' labels : app : keepalived-cloud-provider spec : containers : - name : keepalived-cloud-provider image : quay.io/munnerz/keepalived-cloud-provider:0.0.1 imagePullPolicy : IfNotPresent env : - name : KEEPALIVED_NAMESPACE value : kube-system - name : KEEPALIVED_CONFIG_MAP value : vip-configmap - name : KEEPALIVED_SERVICE_CIDR value : 10.10.0.0/26 # pick a CIDR that is explicitly reserved for keepalived volumeMounts : - name : certs mountPath : /etc/ssl/certs resources : requests : cpu : 200m livenessProbe : httpGet : path : /healthz port : 10252 host : 127.0.0.1 initialDelaySeconds : 15 timeoutSeconds : 15 failureThreshold : 8 volumes : - name : certs hostPath : path : /etc/ssl/certs","title":"cloudprovider.yaml"},{"location":"kubernetes/networking/#loadbalanceryaml","text":"apiVersion : v1 kind : Service metadata : name : webapp1-loadbalancer-svc labels : app : webapp1-loadbalancer spec : type : LoadBalancer ports : - port : 80 selector : app : webapp1-loadbalancer --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-loadbalancer-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-loadbalancer spec : containers : - name : webapp1-loadbalancer-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- export LoadBalancerIP = $( kubectl get services/webapp1-loadbalancer-svc -o go-template = '{{(index .status.loadBalancer.ingress 0).ip}}' )","title":"loadbalancer.yaml"},{"location":"kubernetes/volume_types/","text":"Volume Types \u00b6 A directory which is mounted inside a Pod is backed by the underlying Volume Type. A Volume Type decides the properties of the directory, like size, content, default access modes, etc. Some examples of Volume Types are: emptyDir An empty Volume is created for the Pod as soon as it is scheduled on the worker node. The Volume's life is tightly coupled with the Pod. If the Pod is terminated, the content of emptyDir is deleted forever. hostPath With the hostPath Volume Type, we can share a directory from the host to the Pod. If the Pod is terminated, the content of the Volume is still available on the host. gcePersistentDisk With the gcePersistentDisk Volume Type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStore With the awsElasticBlockStore Volume Type, we can mount an AWS EBS Volume into a Pod. azureDisk With azureDisk we can mount a Microsoft Azure Data Disk into a Pod. azureFile With azureFile we can mount a Microsoft Azure File Volume into a Pod. cephfs With cephfs, an existing CephFS volume can be mounted into a Pod. When a Pod terminates, the volume is unmounted and the contents of the volume are preserved. nfs With nfs, we can mount an NFS share into a Pod. iscsi With iscsi, we can mount an iSCSI share into a Pod. secret With the secret Volume Type, we can pass sensitive information, such as passwords, to Pods. configMap With configMap objects, we can provide configuration data, or shell commands and arguments into a Pod. persistentVolumeClaim We can attach a PersistentVolume to a Pod using a persistentVolumeClaim. PersistentVolumes \u00b6 In a typical IT environment, storage is managed by the storage/system administrators. The end user will just receive instructions to use the storage but is not involved with the underlying storage management. In the containerized world, we would like to follow similar rules, but it becomes challenging, given the many Volume Types we have seen earlier. Kubernetes resolves this problem with the PersistentVolume (PV) subsystem, which provides APIs for users and administrators to manage and consume persistent storage. To manage the Volume, it uses the PersistentVolume API resource type, and to consume it, it uses the PersistentVolumeClaim API resource type. A Persistent Volume is a network-attached storage in the cluster, which is provisioned by the administrator. PersistentVolumes can be dynamically provisioned based on the StorageClass resource. A StorageClass contains pre-defined provisioners and parameters to create a PersistentVolume. Using PersistentVolumeClaims, a user sends the request for dynamic PV creation, which gets wired to the StorageClass resource. Some of the Volume Types that support managing storage using PersistentVolumes are: 1 2 3 4 5 6 7 GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CephFS NFS iSCSI You can learn more details about Volume Types in the Kubernetes documentation .","title":"Volume Types"},{"location":"kubernetes/volume_types/#volume-types","text":"A directory which is mounted inside a Pod is backed by the underlying Volume Type. A Volume Type decides the properties of the directory, like size, content, default access modes, etc. Some examples of Volume Types are: emptyDir An empty Volume is created for the Pod as soon as it is scheduled on the worker node. The Volume's life is tightly coupled with the Pod. If the Pod is terminated, the content of emptyDir is deleted forever. hostPath With the hostPath Volume Type, we can share a directory from the host to the Pod. If the Pod is terminated, the content of the Volume is still available on the host. gcePersistentDisk With the gcePersistentDisk Volume Type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStore With the awsElasticBlockStore Volume Type, we can mount an AWS EBS Volume into a Pod. azureDisk With azureDisk we can mount a Microsoft Azure Data Disk into a Pod. azureFile With azureFile we can mount a Microsoft Azure File Volume into a Pod. cephfs With cephfs, an existing CephFS volume can be mounted into a Pod. When a Pod terminates, the volume is unmounted and the contents of the volume are preserved. nfs With nfs, we can mount an NFS share into a Pod. iscsi With iscsi, we can mount an iSCSI share into a Pod. secret With the secret Volume Type, we can pass sensitive information, such as passwords, to Pods. configMap With configMap objects, we can provide configuration data, or shell commands and arguments into a Pod. persistentVolumeClaim We can attach a PersistentVolume to a Pod using a persistentVolumeClaim.","title":"Volume Types"},{"location":"kubernetes/volume_types/#persistentvolumes","text":"In a typical IT environment, storage is managed by the storage/system administrators. The end user will just receive instructions to use the storage but is not involved with the underlying storage management. In the containerized world, we would like to follow similar rules, but it becomes challenging, given the many Volume Types we have seen earlier. Kubernetes resolves this problem with the PersistentVolume (PV) subsystem, which provides APIs for users and administrators to manage and consume persistent storage. To manage the Volume, it uses the PersistentVolume API resource type, and to consume it, it uses the PersistentVolumeClaim API resource type. A Persistent Volume is a network-attached storage in the cluster, which is provisioned by the administrator. PersistentVolumes can be dynamically provisioned based on the StorageClass resource. A StorageClass contains pre-defined provisioners and parameters to create a PersistentVolume. Using PersistentVolumeClaims, a user sends the request for dynamic PV creation, which gets wired to the StorageClass resource. Some of the Volume Types that support managing storage using PersistentVolumes are: 1 2 3 4 5 6 7 GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CephFS NFS iSCSI You can learn more details about Volume Types in the Kubernetes documentation .","title":"PersistentVolumes"},{"location":"linux/","text":"Linux \u00b6","title":"Linux"},{"location":"linux/#linux","text":"","title":"Linux"},{"location":"linux/bash/","text":"The Art of Bash \u00b6 Fluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot. Meta \u00b6 Scope: This guide is for both beginners and experienced users. The goals are breadth (everything important), specificity (give concrete examples of the most common case), and brevity (avoid things that aren't essential or digressions you can easily look up elsewhere). Every tip is essential in some situation or significantly saves time over alternatives. This is written for Linux, with the exception of the \" macOS only \". The focus is on interactive Bash, though many tips apply to other shells and to general Bash scripting. It includes both \"standard\" Unix commands as well as ones that require special package installs -- so long as they are important enough to merit inclusion. Notes: To keep this to one page, content is implicitly included by reference. You're smart enough to look up more detail elsewhere once you know the idea or command to Google. Use apt , yum , dnf , pacman , pip or brew (as appropriate) to install new programs. Use Explainshell to get a helpful breakdown of what commands, options, pipes etc. do. Basics \u00b6 Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long. Alternate shells can be nice, but Bash is powerful and always available (learning only zsh, fish, etc., while tempting on your own laptop, restricts you in many situations, such as using existing servers). Learn at least one text-based editor well. The nano editor is one of the simplest for basic editing (opening, editing, saving, searching). However, for the power user in a text terminal, there is no substitute for Vim ( vi ), the hard-to-learn but venerable, fast, and full-featured editor. Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.) !!! Finding documentation: * Know how to read official documentation with man (for the inquisitive, man man lists the section numbers, e.g. 1 is \"regular\" commands, 5 is files/conventions, and 8 are for administration). Find man pages with apropos . * Know that some commands are not executables, but Bash builtins, and that you can get help on them with help and help -d . You can find out whether a command is an executable, shell builtin or an alias by using type command . * curl cheat.sh/command will give a brief \"cheat sheet\" with common examples of how to use a shell command. Learn about redirection of output and input using > and < and pipes using | . Know > overwrites the output file and >> appends. Learn about stdout and stderr. Learn about file glob expansion with * (and perhaps ? and [ ... ] ) and quoting and the difference between double \" and single ' quotes. (See more on variable expansion below.) Be familiar with Bash job management: & , ctrl-z , ctrl-c , jobs , fg , bg , kill , etc. Know ssh , and the basics of authentication without password, via ssh-agent , ssh-add , etc. Basic file management: ls and ls -l (in particular, learn what every column in ls -l means), less , head , tail and tail -f (or even better, less +F ), ln and ln -s (learn the differences and advantages of hard versus soft links), chown , chmod , du (for a quick summary of disk usage: du -hs * ). For filesystem management, df , mount , fdisk , mkfs , lsblk . Learn what an inode is ( ls -i or df -i ). Basic network management: ip or ifconfig , dig , traceroute , route . Learn and use a version control management system, such as git . Know regular expressions well, and the various flags to grep / egrep . The -i , -o , -v , -A , -B , and -C options are worth knowing. Learn to use apt-get , yum , dnf or pacman to find and install packages. And make sure you have pip to install Python-based command-line tools. Everyday use \u00b6 Use Tab to complete arguments or list all available commands and ctrl-r to search through command history (after pressing, type to search, press ctrl-r repeatedly to cycle through more matches, press Enter to execute the found command, or hit the right arrow to put the result in the current line to allow editing). Use ctrl-w to delete the last word, and ctrl-u to delete the content from current cursor back to the start of the line. Use alt-b and alt-f to move by word, ctrl-a to move cursor to beginning of line, ctrl-e to move cursor to end of line, ctrl-k to kill to the end of the line, ctrl-l to clear the screen. See man readline for all the default keybindings in Bash. There are a lot. For example alt-. cycles through previous arguments, and alt-* expands a glob. Alternatively, if you love vi-style key-bindings, use set -o vi (and set -o emacs to put it back). For editing long commands, after setting your editor (for example export EDITOR=vim ), ctrl-x ctrl-e will open the current command in an editor for multi-line editing. Or in vi style, escape-v . To see recent commands, use history . Follow with !n (where n is the command number) to execute again. There are also many abbreviations you can use, the most useful probably being !$ for last argument and !! for last command (see \"HISTORY EXPANSION\" in the man page). However, these are often easily replaced with ctrl-r and alt-. . Go to your home directory with cd . Access files relative to your home directory with the ~ prefix (e.g. ~/.bashrc ). In sh scripts refer to the home directory as $HOME . To go back to the previous working directory: cd - . If you are halfway through typing a command but change your mind, hit alt-# to add a # at the beginning and enter it as a comment (or use ctrl-a , # , enter ). You can then return to it later via command history. Use xargs (or parallel ). It's very powerful. Note you can control how many items execute per line ( -L ) as well as parallelism ( -P ). If you're not sure if it'll do the right thing, use xargs echo first. Also, -I{} is handy. Examples: find . -name '*.py' | xargs grep some_function cat hosts | xargs -I {} ssh root@ {} hostname pstree -p is a helpful display of the process tree. Use pgrep and pkill to find or signal processes by name ( -f is helpful). Know the various signals you can send processes. For example, to suspend a process, use kill -STOP [pid] . For the full list, see man 7 signal Use nohup or disown if you want a background process to keep running forever. Check what processes are listening via netstat -lntp or ss -plat (for TCP; add -u for UDP) or lsof -iTCP -sTCP:LISTEN -P -n (which also works on macOS). See also lsof and fuser for open sockets and files. See uptime or w to know how long the system has been running. Use alias to create shortcuts for commonly used commands. For example, alias ll='ls -latr' creates a new alias ll . Save aliases, shell settings, and functions you commonly use in ~/.bashrc , and arrange for login shells to source it . This will make your setup available in all your shell sessions. Put the settings of environment variables as well as commands that should be executed when you login in ~/.bash_profile . Separate configuration will be needed for shells you launch from graphical environment logins and cron jobs. Synchronize your configuration files (e.g. .bashrc and .bash_profile ) among various computers with Git. Understand that care is needed when variables and filenames include whitespace. Surround your Bash variables with quotes, e.g. \"$FOO\" . Prefer the -0 or -print0 options to enable null characters to delimit filenames, e.g. locate -0 pattern | xargs -0 ls -al or find / -print0 -type d | xargs -0 ls -al . To iterate on filenames containing whitespace in a for loop, set your IFS to be a newline only using IFS=$'\\n' . In scripts , use set -x (or the variant set -v , which logs raw input, including unexpanded variables and comments) for debugging output. Use strict modes unless you have a good reason not to: Use set -e to abort on errors (nonzero exit code). Use set -u to detect unset variable usages. Consider set -o pipefail too, to abort on errors within pipes (though read up on it more if you do, as this topic is a bit subtle). For more involved scripts, also use trap on EXIT or ERR. A useful habit is to start a script like this, which will make it detect and abort on common errors and print a message: set -euo pipefail trap \"echo 'error: Script failed: see failed command above'\" ERR In scripts , sub-shells (written with parentheses) are convenient ways to group commands. A common example is to temporarily move to a different working directory, e.g. # do something in current dir ( cd /some/other/dir && other-command ) # continue in original dir Note there are lots of kinds of variable expansion. Checking a variable exists: ${name:?error message} . For example, if a Bash script requires a single argument, just write input_file=${1:?usage: $0 input_file} . Using a default value if a variable is empty: ${name:-default} . If you want to have an additional (optional) parameter added to the previous example, you can use something like output_file=${2:-logfile} . If $2 is omitted and thus empty, output_file will be set to logfile . Arithmetic expansion: i=$(( (i + 1) % 5 )) . Sequences: {1..10} . Trimming of strings: ${var%suffix} and ${var#prefix} . For example if var=foo.pdf , then echo ${var%.pdf}.txt prints foo.txt . Brace expansion using { ... } can reduce having to re-type similar text and automate combinations of items. This is helpful in examples like mv foo.{txt,pdf} some-dir (which moves both files), cp somefile{,.bak} (which expands to cp somefile somefile.bak ) or mkdir -p test-{a,b,c}/subtest-{1,2,3} (which expands all possible combinations and creates a directory tree). Brace expansion is performed before any other expansion. The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. (For example, a range like {1..20} cannot be expressed with variables using {$a..$b} . Use seq or a for loop instead, e.g., seq $a $b or for((i=a; i<=b; i++)); do ... ; done .) The output of a command can be treated like a file via <(some command) (known as process substitution). For example, compare local /etc/hosts with a remote one: diff /etc/hosts < ( ssh somehost cat /etc/hosts ) In scripts you may want to put all of your code in curly braces. If the closing brace is missing, your script will be prevented from executing due to a syntax error. This makes sense when your script is going to be downloaded from the web, since it prevents partially downloaded scripts from executing: { # Your code here } A \"here document\" allows redirection of multiple lines of input as if from a file: cat <<EOF input on multiple lines EOF Redirect both standard output and standard error via: some-command >logfile 2>&1 or some-command &>logfile . Often, to ensure a command does not leave an open file handle to standard input, tying it to the terminal you are in, it is also good practice to add </dev/null . Use man ascii for a good ASCII table, with hex and decimal values. For general encoding info, man unicode , man utf-8 , and man latin1 are helpful. Use screen or tmux to multiplex the screen, especially useful on remote ssh sessions and to detach and re-attach to a session. byobu can enhance screen or tmux by providing more information and easier management. A more minimal alternative for session persistence only is dtach . In ssh , knowing how to port tunnel with -L or -D (and occasionally -R ) is useful, e.g. to access web sites from a remote server. It can be useful to make a few optimizations to your ssh configuration; for example, this ~/.ssh/config contains settings to avoid dropped connections in certain network environments, uses compression (which is helpful with scp over low-bandwidth connections), and multiplex channels to the same server with a local control file: TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes A few other options relevant to ssh are security sensitive and should be enabled with care, e.g. per subnet or host or in trusted networks: StrictHostKeyChecking=no , ForwardAgent=yes Consider mosh an alternative to ssh that uses UDP, avoiding dropped connections and adding convenience on the road (requires server-side setup). To get the permissions on a file in octal form, which is useful for system configuration but not available in ls and easy to bungle, use something like stat -c '%A %a %n' /etc/timezone For interactive selection of values from the output of another command, use percol or fzf . For interaction with files based on the output of another command (like git ), use fpp ( PathPicker ). For a simple web server for all files in the current directory (and subdirs), available to anyone on your network, use: python -m SimpleHTTPServer 7777 (for port 7777 and Python 2) and python -m http.server 7777 (for port 7777 and Python 3). For running a command as another user, use sudo . Defaults to running as root; use -u to specify another user. Use -i to login as that user (you will be asked for your password). For switching the shell to another user, use su username or su - username . The latter with \"-\" gets an environment as if another user just logged in. Omitting the username defaults to root. You will be asked for the password of the user you are switching to . Know about the 128K limit on command lines. This \"Argument list too long\" error is common when wildcard matching large numbers of files. (When this happens alternatives like find and xargs may help.) Processing files and data \u00b6 To locate a file by name in the current directory, find . -iname '*something*' (or similar). To find a file anywhere by name, use locate something (but bear in mind updatedb may not have indexed recently created files). For general searching through source or data files, there are several options more advanced or faster than grep -r , including (in rough order from older to newer) ack , ag (\"the silver searcher\"), and rg (ripgrep). To convert HTML to text: lynx -dump -stdin For Markdown, HTML, and all kinds of document conversion, try pandoc . For example, to convert a Markdown document to Word format: pandoc README.md --from markdown --to docx -o temp.docx If you must handle XML, xmlstarlet is old but good. For JSON, use jq . For interactive use, also see jid and jiq . For YAML, use shyaml . For Excel or CSV files, csvkit provides in2csv , csvcut , csvjoin , csvgrep , etc. For Amazon S3 , s3cmd is convenient and s4cmd is faster. Amazon's aws and the improved saws are essential for other AWS-related tasks. Know about sort and uniq , including uniq's -u and -d options -- see one-liners below. See also comm . Know about cut , paste , and join to manipulate text files. Many people use cut but forget about join . Know about wc to count newlines ( -l ), characters ( -m ), words ( -w ) and bytes ( -c ). Know about tee to copy from stdin to a file and also to stdout, as in ls -al | tee file.txt . For more complex calculations, including grouping, reversing fields, and statistical calculations, consider datamash . Know that locale affects a lot of command line tools in subtle ways, including sorting order (collation) and performance. Most Linux installations will set LANG or other locale variables to a local setting like US English. But be aware sorting will change if you change locale. And know i18n routines can make sort or other commands run many times slower. In some situations (such as the set operations or uniqueness operations below) you can safely ignore slow i18n routines entirely and use traditional byte-based sort order, using export LC_ALL=C . You can set a specific command's environment by prefixing its invocation with the environment variable settings, as in TZ=Pacific/Fiji date . Know basic awk and sed for simple data munging. See One-liners for examples. To replace all occurrences of a string in place, in one or more files: perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt To rename multiple files and/or search and replace within files, try repren . (In some cases the rename command also allows multiple renames, but be careful as its functionality is not the same on all Linux distributions.) # Full rename of filenames, directories, and contents foo -> bar: repren --full --preserve-case --from foo --to bar . # Recover backup files whatever.bak -> whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # Same as above, using rename, if available: rename 's/\\.bak$//' *.bak As the man page says, rsync really is a fast and extraordinarily versatile file copying tool. It's known for synchronizing between machines but is equally useful locally. When security restrictions allow, using rsync instead of scp allows recovery of a transfer without restarting from scratch. It also is among the fastest ways to delete large numbers of files: mkdir empty && rsync -r --delete empty/ some-dir && rmdir some-dir For monitoring progress when processing files, use pv , pycp , pmonitor , progress , rsync --progress , or, for block-level copying, dd status=progress . Use shuf to shuffle or select random lines from a file. Know sort 's options. For numbers, use -n , or -h for handling human-readable numbers (e.g. from du -h ). Know how keys work ( -t and -k ). In particular, watch out that you need to write -k1,1 to sort by only the first field; -k1 means sort according to the whole line. Stable sort ( sort -s ) can be useful. For example, to sort first by field 2, then secondarily by field 1, you can use sort -k1,1 | sort -s -k2,2 . If you ever need to write a tab literal in a command line in Bash (e.g. for the -t argument to sort), press ctrl-v [Tab] or write $'\\t' (the latter is better as you can copy/paste it). The standard tools for patching source code are diff and patch . See also diffstat for summary statistics of a diff and sdiff for a side-by-side diff. Note diff -r works for entire directories. Use diff -r tree1 tree2 | diffstat for a summary of changes. Use vimdiff to compare and edit files. For binary files, use hd , hexdump or xxd for simple hex dumps and bvi , hexedit or biew for binary editing. Also for binary files, strings (plus grep , etc.) lets you find bits of text. For binary diffs (delta compression), use xdelta3 . To convert text encodings, try iconv . Or uconv for more advanced use; it supports some advanced Unicode things. For example: # Displays hex codes or actual names of characters (useful for debugging): uconv -f utf-8 -t utf-8 -x '::Any-Hex;' < input.txt uconv -f utf-8 -t utf-8 -x '::Any-Name;' < input.txt # Lowercase and removes all accents (by expanding and dropping them): uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] >; ::Any-NFC;' < input.txt > output.txt To split files into pieces, see split (to split by size) and csplit (to split by a pattern). Date and time: To get the current date and time in the helpful ISO 8601 format, use date -u +\"%Y-%m-%dT%H:%M:%SZ\" (other options are problematic ). To manipulate date and time expressions, use dateadd , datediff , strptime etc. from dateutils . Use zless , zmore , zcat , and zgrep to operate on compressed files. File attributes are settable via chattr and offer a lower-level alternative to file permissions. For example, to protect against accidental file deletion the immutable flag: sudo chattr +i /critical/directory/or/file Use getfacl and setfacl to save and restore file permissions. For example: getfacl -R /some/path > permissions.txt setfacl --restore = permissions.txt To create empty files quickly, use truncate (creates sparse file ), fallocate (ext4, xfs, btrfs and ocfs2 filesystems), xfs_mkfile (almost any filesystems, comes in xfsprogs package), mkfile (for Unix-like systems like Solaris, Mac OS). System debugging \u00b6 For web debugging, curl and curl -I are handy, or their wget equivalents, or the more modern httpie . To know current cpu/disk status, the classic tools are top (or the better htop ), iostat , and iotop . Use iostat -mxz 15 for basic CPU and detailed per-partition disk stats and performance insight. For network connection details, use netstat and ss . For a quick overview of what's happening on a system, dstat is especially useful. For broadest overview with details, use glances . To know memory status, run and understand the output of free and vmstat . In particular, be aware the \"cached\" value is memory held by the Linux kernel as file cache, so effectively counts toward the \"free\" value. Java system debugging is a different kettle of fish, but a simple trick on Oracle's and some other JVMs is that you can run kill -3 <pid> and a full stack trace and heap summary (including generational garbage collection details, which can be highly informative) will be dumped to stderr/logs. The JDK's jps , jstat , jstack , jmap are useful. SJK tools are more advanced. Use mtr as a better traceroute, to identify network issues. For looking at why a disk is full, ncdu saves time over the usual commands like du -sh * . To find which socket or process is using bandwidth, try iftop or nethogs . The ab tool (comes with Apache) is helpful for quick-and-dirty checking of web server performance. For more complex load testing, try siege . For more serious network debugging, wireshark , tshark , or ngrep . Know about strace and ltrace . These can be helpful if a program is failing, hanging, or crashing, and you don't know why, or if you want to get a general idea of performance. Note the profiling option ( -c ), and the ability to attach to a running process ( -p ). Use trace child option ( -f ) to avoid missing important calls. Know about ldd to check shared libraries etc \u2014 but never run it on untrusted files . Know how to connect to a running process with gdb and get its stack traces. Use /proc . It's amazingly helpful sometimes when debugging live problems. Examples: /proc/cpuinfo , /proc/meminfo , /proc/cmdline , /proc/xxx/cwd , /proc/xxx/exe , /proc/xxx/fd/ , /proc/xxx/smaps (where xxx is the process id or pid). When debugging why something went wrong in the past, sar can be very helpful. It shows historic statistics on CPU, memory, network, etc. For deeper systems and performance analyses, look at stap ( SystemTap ), perf , and sysdig . Check what OS you're on with uname or uname -a (general Unix/kernel info) or lsb_release -a (Linux distro info). Use dmesg whenever something's acting really funny (it could be hardware or driver issues). If you delete a file and it doesn't free up expected disk space as reported by du , check whether the file is in use by a process: lsof | grep deleted | grep \"filename-of-my-big-file\" Obscure but useful \u00b6 expr : perform arithmetic or boolean operations or evaluate regular expressions m4 : simple macro processor yes : print a string a lot cal : nice calendar env : run a command (useful in scripts) printenv : print out environment variables (useful in debugging and scripts) look : find English words (or lines in a file) beginning with a string cut , paste and join : data manipulation fmt : format text paragraphs pr : format text into pages/columns fold : wrap lines of text column : format text fields into aligned, fixed-width columns or tables expand and unexpand : convert between tabs and spaces nl : add line numbers seq : print numbers bc : calculator factor : factor integers gpg : encrypt and sign files toe : table of term info entries nc : network debugging and data transfer socat : socket relay and tcp port forwarder (similar to netcat ) slurm : network traffic visualization dd : moving data between files or devices file : identify type of a file tree : display directories and subdirectories as a nesting tree; like ls but recursive stat : file info time : execute and time a command timeout : execute a command for specified amount of time and stop the process when the specified amount of time completes. lockfile : create semaphore file that can only be removed by rm -f logrotate : rotate, compress and mail logs. watch : run a command repeatedly, showing results and/or highlighting changes when-changed : runs any command you specify whenever it sees file changed. See inotifywait and entr as well. tac : print files in reverse comm : compare sorted files line by line strings : extract text from binary files tr : character translation or manipulation iconv or uconv : conversion for text encodings split and csplit : splitting files sponge : read all input before writing it, useful for reading from then writing to the same file, e.g., grep -v something some-file | sponge some-file units : unit conversions and calculations; converts furlongs per fortnight to twips per blink (see also /usr/share/units/definitions.units ) apg : generates random passwords xz : high-ratio file compression ldd : dynamic library info nm : symbols from object files ab or wrk : benchmarking web servers strace : system call debugging mtr : better traceroute for network debugging cssh : visual concurrent shell rsync : sync files and folders over SSH or in local file system wireshark and tshark : packet capture and network debugging ngrep : grep for the network layer host and dig : DNS lookups lsof : process file descriptor and socket info dstat : useful system stats glances : high level, multi-subsystem overview iostat : Disk usage stats mpstat : CPU usage stats vmstat : Memory usage stats htop : improved version of top last : login history w : who's logged on id : user/group identity info sar : historic system stats iftop or nethogs : network utilization by socket or process ss : socket statistics dmesg : boot and system error messages sysctl : view and configure Linux kernel parameters at run time hdparm : SATA/ATA disk manipulation/performance lsblk : list block devices: a tree view of your disks and disk partitions lshw , lscpu , lspci , lsusb , dmidecode : hardware information, including CPU, BIOS, RAID, graphics, devices, etc. lsmod and modinfo : List and show details of kernel modules. fortune , ddate , and sl : um, well, it depends on whether you consider steam locomotives and Zippy quotations \"useful\" macOS only \u00b6 These are items relevant only on macOS. Package management with brew (Homebrew) and/or port (MacPorts). These can be used to install on macOS many of the above commands. Copy output of any command to a desktop app with pbcopy and paste input from one with pbpaste . To enable the Option key in macOS Terminal as an alt key (such as used in the commands above like alt-b , alt-f , etc.), open Preferences -> Profiles -> Keyboard and select \"Use Option as Meta key\". To open a file with a desktop app, use open or open -a /Applications/Whatever.app . Spotlight: Search files with mdfind and list metadata (such as photo EXIF info) with mdls . Be aware macOS is based on BSD Unix, and many commands (for example ps , ls , tail , awk , sed ) have many subtle variations from Linux, which is largely influenced by System V-style Unix and GNU tools. You can often tell the difference by noting a man page has the heading \"BSD General Commands Manual.\" In some cases GNU versions can be installed, too (such as gawk and gsed for GNU awk and sed). If writing cross-platform Bash scripts, avoid such commands (for example, consider Python or perl ) or test carefully. To get macOS release information, use sw_vers . More resources \u00b6 awesome-shell : A curated list of shell tools and resources. awesome-osx-command-line : A more in-depth guide for the macOS command line. Strict mode for writing better shell scripts. shellcheck : A shell script static analysis tool. Essentially, lint for bash/sh/zsh. Filenames and Pathnames in Shell : The sadly complex minutiae on how to handle filenames correctly in shell scripts. Data Science at the Command Line : More commands and tools helpful for doing data science, from the book of the same name Disclaimer \u00b6 With the exception of very small tasks, code is written so others can read it. With power comes responsibility. The fact you can do something in Bash doesn't necessarily mean you should! ;) License \u00b6 This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .","title":"Art of Bash"},{"location":"linux/bash/#the-art-of-bash","text":"Fluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot.","title":"The Art of Bash"},{"location":"linux/bash/#meta","text":"Scope: This guide is for both beginners and experienced users. The goals are breadth (everything important), specificity (give concrete examples of the most common case), and brevity (avoid things that aren't essential or digressions you can easily look up elsewhere). Every tip is essential in some situation or significantly saves time over alternatives. This is written for Linux, with the exception of the \" macOS only \". The focus is on interactive Bash, though many tips apply to other shells and to general Bash scripting. It includes both \"standard\" Unix commands as well as ones that require special package installs -- so long as they are important enough to merit inclusion. Notes: To keep this to one page, content is implicitly included by reference. You're smart enough to look up more detail elsewhere once you know the idea or command to Google. Use apt , yum , dnf , pacman , pip or brew (as appropriate) to install new programs. Use Explainshell to get a helpful breakdown of what commands, options, pipes etc. do.","title":"Meta"},{"location":"linux/bash/#basics","text":"Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long. Alternate shells can be nice, but Bash is powerful and always available (learning only zsh, fish, etc., while tempting on your own laptop, restricts you in many situations, such as using existing servers). Learn at least one text-based editor well. The nano editor is one of the simplest for basic editing (opening, editing, saving, searching). However, for the power user in a text terminal, there is no substitute for Vim ( vi ), the hard-to-learn but venerable, fast, and full-featured editor. Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.) !!! Finding documentation: * Know how to read official documentation with man (for the inquisitive, man man lists the section numbers, e.g. 1 is \"regular\" commands, 5 is files/conventions, and 8 are for administration). Find man pages with apropos . * Know that some commands are not executables, but Bash builtins, and that you can get help on them with help and help -d . You can find out whether a command is an executable, shell builtin or an alias by using type command . * curl cheat.sh/command will give a brief \"cheat sheet\" with common examples of how to use a shell command. Learn about redirection of output and input using > and < and pipes using | . Know > overwrites the output file and >> appends. Learn about stdout and stderr. Learn about file glob expansion with * (and perhaps ? and [ ... ] ) and quoting and the difference between double \" and single ' quotes. (See more on variable expansion below.) Be familiar with Bash job management: & , ctrl-z , ctrl-c , jobs , fg , bg , kill , etc. Know ssh , and the basics of authentication without password, via ssh-agent , ssh-add , etc. Basic file management: ls and ls -l (in particular, learn what every column in ls -l means), less , head , tail and tail -f (or even better, less +F ), ln and ln -s (learn the differences and advantages of hard versus soft links), chown , chmod , du (for a quick summary of disk usage: du -hs * ). For filesystem management, df , mount , fdisk , mkfs , lsblk . Learn what an inode is ( ls -i or df -i ). Basic network management: ip or ifconfig , dig , traceroute , route . Learn and use a version control management system, such as git . Know regular expressions well, and the various flags to grep / egrep . The -i , -o , -v , -A , -B , and -C options are worth knowing. Learn to use apt-get , yum , dnf or pacman to find and install packages. And make sure you have pip to install Python-based command-line tools.","title":"Basics"},{"location":"linux/bash/#everyday-use","text":"Use Tab to complete arguments or list all available commands and ctrl-r to search through command history (after pressing, type to search, press ctrl-r repeatedly to cycle through more matches, press Enter to execute the found command, or hit the right arrow to put the result in the current line to allow editing). Use ctrl-w to delete the last word, and ctrl-u to delete the content from current cursor back to the start of the line. Use alt-b and alt-f to move by word, ctrl-a to move cursor to beginning of line, ctrl-e to move cursor to end of line, ctrl-k to kill to the end of the line, ctrl-l to clear the screen. See man readline for all the default keybindings in Bash. There are a lot. For example alt-. cycles through previous arguments, and alt-* expands a glob. Alternatively, if you love vi-style key-bindings, use set -o vi (and set -o emacs to put it back). For editing long commands, after setting your editor (for example export EDITOR=vim ), ctrl-x ctrl-e will open the current command in an editor for multi-line editing. Or in vi style, escape-v . To see recent commands, use history . Follow with !n (where n is the command number) to execute again. There are also many abbreviations you can use, the most useful probably being !$ for last argument and !! for last command (see \"HISTORY EXPANSION\" in the man page). However, these are often easily replaced with ctrl-r and alt-. . Go to your home directory with cd . Access files relative to your home directory with the ~ prefix (e.g. ~/.bashrc ). In sh scripts refer to the home directory as $HOME . To go back to the previous working directory: cd - . If you are halfway through typing a command but change your mind, hit alt-# to add a # at the beginning and enter it as a comment (or use ctrl-a , # , enter ). You can then return to it later via command history. Use xargs (or parallel ). It's very powerful. Note you can control how many items execute per line ( -L ) as well as parallelism ( -P ). If you're not sure if it'll do the right thing, use xargs echo first. Also, -I{} is handy. Examples: find . -name '*.py' | xargs grep some_function cat hosts | xargs -I {} ssh root@ {} hostname pstree -p is a helpful display of the process tree. Use pgrep and pkill to find or signal processes by name ( -f is helpful). Know the various signals you can send processes. For example, to suspend a process, use kill -STOP [pid] . For the full list, see man 7 signal Use nohup or disown if you want a background process to keep running forever. Check what processes are listening via netstat -lntp or ss -plat (for TCP; add -u for UDP) or lsof -iTCP -sTCP:LISTEN -P -n (which also works on macOS). See also lsof and fuser for open sockets and files. See uptime or w to know how long the system has been running. Use alias to create shortcuts for commonly used commands. For example, alias ll='ls -latr' creates a new alias ll . Save aliases, shell settings, and functions you commonly use in ~/.bashrc , and arrange for login shells to source it . This will make your setup available in all your shell sessions. Put the settings of environment variables as well as commands that should be executed when you login in ~/.bash_profile . Separate configuration will be needed for shells you launch from graphical environment logins and cron jobs. Synchronize your configuration files (e.g. .bashrc and .bash_profile ) among various computers with Git. Understand that care is needed when variables and filenames include whitespace. Surround your Bash variables with quotes, e.g. \"$FOO\" . Prefer the -0 or -print0 options to enable null characters to delimit filenames, e.g. locate -0 pattern | xargs -0 ls -al or find / -print0 -type d | xargs -0 ls -al . To iterate on filenames containing whitespace in a for loop, set your IFS to be a newline only using IFS=$'\\n' . In scripts , use set -x (or the variant set -v , which logs raw input, including unexpanded variables and comments) for debugging output. Use strict modes unless you have a good reason not to: Use set -e to abort on errors (nonzero exit code). Use set -u to detect unset variable usages. Consider set -o pipefail too, to abort on errors within pipes (though read up on it more if you do, as this topic is a bit subtle). For more involved scripts, also use trap on EXIT or ERR. A useful habit is to start a script like this, which will make it detect and abort on common errors and print a message: set -euo pipefail trap \"echo 'error: Script failed: see failed command above'\" ERR In scripts , sub-shells (written with parentheses) are convenient ways to group commands. A common example is to temporarily move to a different working directory, e.g. # do something in current dir ( cd /some/other/dir && other-command ) # continue in original dir Note there are lots of kinds of variable expansion. Checking a variable exists: ${name:?error message} . For example, if a Bash script requires a single argument, just write input_file=${1:?usage: $0 input_file} . Using a default value if a variable is empty: ${name:-default} . If you want to have an additional (optional) parameter added to the previous example, you can use something like output_file=${2:-logfile} . If $2 is omitted and thus empty, output_file will be set to logfile . Arithmetic expansion: i=$(( (i + 1) % 5 )) . Sequences: {1..10} . Trimming of strings: ${var%suffix} and ${var#prefix} . For example if var=foo.pdf , then echo ${var%.pdf}.txt prints foo.txt . Brace expansion using { ... } can reduce having to re-type similar text and automate combinations of items. This is helpful in examples like mv foo.{txt,pdf} some-dir (which moves both files), cp somefile{,.bak} (which expands to cp somefile somefile.bak ) or mkdir -p test-{a,b,c}/subtest-{1,2,3} (which expands all possible combinations and creates a directory tree). Brace expansion is performed before any other expansion. The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. (For example, a range like {1..20} cannot be expressed with variables using {$a..$b} . Use seq or a for loop instead, e.g., seq $a $b or for((i=a; i<=b; i++)); do ... ; done .) The output of a command can be treated like a file via <(some command) (known as process substitution). For example, compare local /etc/hosts with a remote one: diff /etc/hosts < ( ssh somehost cat /etc/hosts ) In scripts you may want to put all of your code in curly braces. If the closing brace is missing, your script will be prevented from executing due to a syntax error. This makes sense when your script is going to be downloaded from the web, since it prevents partially downloaded scripts from executing: { # Your code here } A \"here document\" allows redirection of multiple lines of input as if from a file: cat <<EOF input on multiple lines EOF Redirect both standard output and standard error via: some-command >logfile 2>&1 or some-command &>logfile . Often, to ensure a command does not leave an open file handle to standard input, tying it to the terminal you are in, it is also good practice to add </dev/null . Use man ascii for a good ASCII table, with hex and decimal values. For general encoding info, man unicode , man utf-8 , and man latin1 are helpful. Use screen or tmux to multiplex the screen, especially useful on remote ssh sessions and to detach and re-attach to a session. byobu can enhance screen or tmux by providing more information and easier management. A more minimal alternative for session persistence only is dtach . In ssh , knowing how to port tunnel with -L or -D (and occasionally -R ) is useful, e.g. to access web sites from a remote server. It can be useful to make a few optimizations to your ssh configuration; for example, this ~/.ssh/config contains settings to avoid dropped connections in certain network environments, uses compression (which is helpful with scp over low-bandwidth connections), and multiplex channels to the same server with a local control file: TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes A few other options relevant to ssh are security sensitive and should be enabled with care, e.g. per subnet or host or in trusted networks: StrictHostKeyChecking=no , ForwardAgent=yes Consider mosh an alternative to ssh that uses UDP, avoiding dropped connections and adding convenience on the road (requires server-side setup). To get the permissions on a file in octal form, which is useful for system configuration but not available in ls and easy to bungle, use something like stat -c '%A %a %n' /etc/timezone For interactive selection of values from the output of another command, use percol or fzf . For interaction with files based on the output of another command (like git ), use fpp ( PathPicker ). For a simple web server for all files in the current directory (and subdirs), available to anyone on your network, use: python -m SimpleHTTPServer 7777 (for port 7777 and Python 2) and python -m http.server 7777 (for port 7777 and Python 3). For running a command as another user, use sudo . Defaults to running as root; use -u to specify another user. Use -i to login as that user (you will be asked for your password). For switching the shell to another user, use su username or su - username . The latter with \"-\" gets an environment as if another user just logged in. Omitting the username defaults to root. You will be asked for the password of the user you are switching to . Know about the 128K limit on command lines. This \"Argument list too long\" error is common when wildcard matching large numbers of files. (When this happens alternatives like find and xargs may help.)","title":"Everyday use"},{"location":"linux/bash/#processing-files-and-data","text":"To locate a file by name in the current directory, find . -iname '*something*' (or similar). To find a file anywhere by name, use locate something (but bear in mind updatedb may not have indexed recently created files). For general searching through source or data files, there are several options more advanced or faster than grep -r , including (in rough order from older to newer) ack , ag (\"the silver searcher\"), and rg (ripgrep). To convert HTML to text: lynx -dump -stdin For Markdown, HTML, and all kinds of document conversion, try pandoc . For example, to convert a Markdown document to Word format: pandoc README.md --from markdown --to docx -o temp.docx If you must handle XML, xmlstarlet is old but good. For JSON, use jq . For interactive use, also see jid and jiq . For YAML, use shyaml . For Excel or CSV files, csvkit provides in2csv , csvcut , csvjoin , csvgrep , etc. For Amazon S3 , s3cmd is convenient and s4cmd is faster. Amazon's aws and the improved saws are essential for other AWS-related tasks. Know about sort and uniq , including uniq's -u and -d options -- see one-liners below. See also comm . Know about cut , paste , and join to manipulate text files. Many people use cut but forget about join . Know about wc to count newlines ( -l ), characters ( -m ), words ( -w ) and bytes ( -c ). Know about tee to copy from stdin to a file and also to stdout, as in ls -al | tee file.txt . For more complex calculations, including grouping, reversing fields, and statistical calculations, consider datamash . Know that locale affects a lot of command line tools in subtle ways, including sorting order (collation) and performance. Most Linux installations will set LANG or other locale variables to a local setting like US English. But be aware sorting will change if you change locale. And know i18n routines can make sort or other commands run many times slower. In some situations (such as the set operations or uniqueness operations below) you can safely ignore slow i18n routines entirely and use traditional byte-based sort order, using export LC_ALL=C . You can set a specific command's environment by prefixing its invocation with the environment variable settings, as in TZ=Pacific/Fiji date . Know basic awk and sed for simple data munging. See One-liners for examples. To replace all occurrences of a string in place, in one or more files: perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt To rename multiple files and/or search and replace within files, try repren . (In some cases the rename command also allows multiple renames, but be careful as its functionality is not the same on all Linux distributions.) # Full rename of filenames, directories, and contents foo -> bar: repren --full --preserve-case --from foo --to bar . # Recover backup files whatever.bak -> whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # Same as above, using rename, if available: rename 's/\\.bak$//' *.bak As the man page says, rsync really is a fast and extraordinarily versatile file copying tool. It's known for synchronizing between machines but is equally useful locally. When security restrictions allow, using rsync instead of scp allows recovery of a transfer without restarting from scratch. It also is among the fastest ways to delete large numbers of files: mkdir empty && rsync -r --delete empty/ some-dir && rmdir some-dir For monitoring progress when processing files, use pv , pycp , pmonitor , progress , rsync --progress , or, for block-level copying, dd status=progress . Use shuf to shuffle or select random lines from a file. Know sort 's options. For numbers, use -n , or -h for handling human-readable numbers (e.g. from du -h ). Know how keys work ( -t and -k ). In particular, watch out that you need to write -k1,1 to sort by only the first field; -k1 means sort according to the whole line. Stable sort ( sort -s ) can be useful. For example, to sort first by field 2, then secondarily by field 1, you can use sort -k1,1 | sort -s -k2,2 . If you ever need to write a tab literal in a command line in Bash (e.g. for the -t argument to sort), press ctrl-v [Tab] or write $'\\t' (the latter is better as you can copy/paste it). The standard tools for patching source code are diff and patch . See also diffstat for summary statistics of a diff and sdiff for a side-by-side diff. Note diff -r works for entire directories. Use diff -r tree1 tree2 | diffstat for a summary of changes. Use vimdiff to compare and edit files. For binary files, use hd , hexdump or xxd for simple hex dumps and bvi , hexedit or biew for binary editing. Also for binary files, strings (plus grep , etc.) lets you find bits of text. For binary diffs (delta compression), use xdelta3 . To convert text encodings, try iconv . Or uconv for more advanced use; it supports some advanced Unicode things. For example: # Displays hex codes or actual names of characters (useful for debugging): uconv -f utf-8 -t utf-8 -x '::Any-Hex;' < input.txt uconv -f utf-8 -t utf-8 -x '::Any-Name;' < input.txt # Lowercase and removes all accents (by expanding and dropping them): uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] >; ::Any-NFC;' < input.txt > output.txt To split files into pieces, see split (to split by size) and csplit (to split by a pattern). Date and time: To get the current date and time in the helpful ISO 8601 format, use date -u +\"%Y-%m-%dT%H:%M:%SZ\" (other options are problematic ). To manipulate date and time expressions, use dateadd , datediff , strptime etc. from dateutils . Use zless , zmore , zcat , and zgrep to operate on compressed files. File attributes are settable via chattr and offer a lower-level alternative to file permissions. For example, to protect against accidental file deletion the immutable flag: sudo chattr +i /critical/directory/or/file Use getfacl and setfacl to save and restore file permissions. For example: getfacl -R /some/path > permissions.txt setfacl --restore = permissions.txt To create empty files quickly, use truncate (creates sparse file ), fallocate (ext4, xfs, btrfs and ocfs2 filesystems), xfs_mkfile (almost any filesystems, comes in xfsprogs package), mkfile (for Unix-like systems like Solaris, Mac OS).","title":"Processing files and data"},{"location":"linux/bash/#system-debugging","text":"For web debugging, curl and curl -I are handy, or their wget equivalents, or the more modern httpie . To know current cpu/disk status, the classic tools are top (or the better htop ), iostat , and iotop . Use iostat -mxz 15 for basic CPU and detailed per-partition disk stats and performance insight. For network connection details, use netstat and ss . For a quick overview of what's happening on a system, dstat is especially useful. For broadest overview with details, use glances . To know memory status, run and understand the output of free and vmstat . In particular, be aware the \"cached\" value is memory held by the Linux kernel as file cache, so effectively counts toward the \"free\" value. Java system debugging is a different kettle of fish, but a simple trick on Oracle's and some other JVMs is that you can run kill -3 <pid> and a full stack trace and heap summary (including generational garbage collection details, which can be highly informative) will be dumped to stderr/logs. The JDK's jps , jstat , jstack , jmap are useful. SJK tools are more advanced. Use mtr as a better traceroute, to identify network issues. For looking at why a disk is full, ncdu saves time over the usual commands like du -sh * . To find which socket or process is using bandwidth, try iftop or nethogs . The ab tool (comes with Apache) is helpful for quick-and-dirty checking of web server performance. For more complex load testing, try siege . For more serious network debugging, wireshark , tshark , or ngrep . Know about strace and ltrace . These can be helpful if a program is failing, hanging, or crashing, and you don't know why, or if you want to get a general idea of performance. Note the profiling option ( -c ), and the ability to attach to a running process ( -p ). Use trace child option ( -f ) to avoid missing important calls. Know about ldd to check shared libraries etc \u2014 but never run it on untrusted files . Know how to connect to a running process with gdb and get its stack traces. Use /proc . It's amazingly helpful sometimes when debugging live problems. Examples: /proc/cpuinfo , /proc/meminfo , /proc/cmdline , /proc/xxx/cwd , /proc/xxx/exe , /proc/xxx/fd/ , /proc/xxx/smaps (where xxx is the process id or pid). When debugging why something went wrong in the past, sar can be very helpful. It shows historic statistics on CPU, memory, network, etc. For deeper systems and performance analyses, look at stap ( SystemTap ), perf , and sysdig . Check what OS you're on with uname or uname -a (general Unix/kernel info) or lsb_release -a (Linux distro info). Use dmesg whenever something's acting really funny (it could be hardware or driver issues). If you delete a file and it doesn't free up expected disk space as reported by du , check whether the file is in use by a process: lsof | grep deleted | grep \"filename-of-my-big-file\"","title":"System debugging"},{"location":"linux/bash/#obscure-but-useful","text":"expr : perform arithmetic or boolean operations or evaluate regular expressions m4 : simple macro processor yes : print a string a lot cal : nice calendar env : run a command (useful in scripts) printenv : print out environment variables (useful in debugging and scripts) look : find English words (or lines in a file) beginning with a string cut , paste and join : data manipulation fmt : format text paragraphs pr : format text into pages/columns fold : wrap lines of text column : format text fields into aligned, fixed-width columns or tables expand and unexpand : convert between tabs and spaces nl : add line numbers seq : print numbers bc : calculator factor : factor integers gpg : encrypt and sign files toe : table of term info entries nc : network debugging and data transfer socat : socket relay and tcp port forwarder (similar to netcat ) slurm : network traffic visualization dd : moving data between files or devices file : identify type of a file tree : display directories and subdirectories as a nesting tree; like ls but recursive stat : file info time : execute and time a command timeout : execute a command for specified amount of time and stop the process when the specified amount of time completes. lockfile : create semaphore file that can only be removed by rm -f logrotate : rotate, compress and mail logs. watch : run a command repeatedly, showing results and/or highlighting changes when-changed : runs any command you specify whenever it sees file changed. See inotifywait and entr as well. tac : print files in reverse comm : compare sorted files line by line strings : extract text from binary files tr : character translation or manipulation iconv or uconv : conversion for text encodings split and csplit : splitting files sponge : read all input before writing it, useful for reading from then writing to the same file, e.g., grep -v something some-file | sponge some-file units : unit conversions and calculations; converts furlongs per fortnight to twips per blink (see also /usr/share/units/definitions.units ) apg : generates random passwords xz : high-ratio file compression ldd : dynamic library info nm : symbols from object files ab or wrk : benchmarking web servers strace : system call debugging mtr : better traceroute for network debugging cssh : visual concurrent shell rsync : sync files and folders over SSH or in local file system wireshark and tshark : packet capture and network debugging ngrep : grep for the network layer host and dig : DNS lookups lsof : process file descriptor and socket info dstat : useful system stats glances : high level, multi-subsystem overview iostat : Disk usage stats mpstat : CPU usage stats vmstat : Memory usage stats htop : improved version of top last : login history w : who's logged on id : user/group identity info sar : historic system stats iftop or nethogs : network utilization by socket or process ss : socket statistics dmesg : boot and system error messages sysctl : view and configure Linux kernel parameters at run time hdparm : SATA/ATA disk manipulation/performance lsblk : list block devices: a tree view of your disks and disk partitions lshw , lscpu , lspci , lsusb , dmidecode : hardware information, including CPU, BIOS, RAID, graphics, devices, etc. lsmod and modinfo : List and show details of kernel modules. fortune , ddate , and sl : um, well, it depends on whether you consider steam locomotives and Zippy quotations \"useful\"","title":"Obscure but useful"},{"location":"linux/bash/#macos-only","text":"These are items relevant only on macOS. Package management with brew (Homebrew) and/or port (MacPorts). These can be used to install on macOS many of the above commands. Copy output of any command to a desktop app with pbcopy and paste input from one with pbpaste . To enable the Option key in macOS Terminal as an alt key (such as used in the commands above like alt-b , alt-f , etc.), open Preferences -> Profiles -> Keyboard and select \"Use Option as Meta key\". To open a file with a desktop app, use open or open -a /Applications/Whatever.app . Spotlight: Search files with mdfind and list metadata (such as photo EXIF info) with mdls . Be aware macOS is based on BSD Unix, and many commands (for example ps , ls , tail , awk , sed ) have many subtle variations from Linux, which is largely influenced by System V-style Unix and GNU tools. You can often tell the difference by noting a man page has the heading \"BSD General Commands Manual.\" In some cases GNU versions can be installed, too (such as gawk and gsed for GNU awk and sed). If writing cross-platform Bash scripts, avoid such commands (for example, consider Python or perl ) or test carefully. To get macOS release information, use sw_vers .","title":"macOS only"},{"location":"linux/bash/#more-resources","text":"awesome-shell : A curated list of shell tools and resources. awesome-osx-command-line : A more in-depth guide for the macOS command line. Strict mode for writing better shell scripts. shellcheck : A shell script static analysis tool. Essentially, lint for bash/sh/zsh. Filenames and Pathnames in Shell : The sadly complex minutiae on how to handle filenames correctly in shell scripts. Data Science at the Command Line : More commands and tools helpful for doing data science, from the book of the same name","title":"More resources"},{"location":"linux/bash/#disclaimer","text":"With the exception of very small tasks, code is written so others can read it. With power comes responsibility. The fact you can do something in Bash doesn't necessarily mean you should! ;)","title":"Disclaimer"},{"location":"linux/bash/#license","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .","title":"License"},{"location":"linux/one-liners/","text":"Useful bash one-liners \u00b6 Replace text in template \u00b6 Replace TEMPLATE_TEXT with $BUILD_NUMBER var inside filename.json.template and save it into filename.json sed \"s/TEMPLATE_TEXT/1. $BUILD_NUMBER /g\" \\ filename.json.template > filename.json A few examples of piecing together commands: \u00b6 It is remarkably helpful sometimes that you can do set intersection, union, and difference of text files via sort / uniq . Suppose a and b are text files that are already uniqued. This is fast, and works on files of arbitrary size, up to many gigabytes. (Sort is not limited by memory, though you may need to use the -T option if /tmp is on a small root partition.) See also the note about LC_ALL above and sort 's -u option (left out for clarity below). sort a b | uniq > c # c is a union b sort a b | uniq -d > c # c is a intersect b sort a b b | uniq -u > c # c is set difference a - b Pretty-print two JSON files, normalizing their syntax, then coloring and paginating the result: diff <(jq --sort-keys . < file1.json) <(jq --sort-keys . < file2.json) | colordiff | less -R Use grep . * to quickly examine the contents of all files in a directory (so each line is paired with the filename), or head -100 * (so each file has a heading). This can be useful for directories filled with config settings like those in /sys , /proc , /etc . Summing all numbers in the third column of a text file (this is probably 3X faster and 3X less code than equivalent Python): awk '{ x += $3 } END { print x }' myfile To see sizes/dates on a tree of files, this is like a recursive ls -l but is easier to read than ls -lR : find . -type f -ls Say you have a text file, like a web server log, and a certain value that appears on some lines, such as an acct_id parameter that is present in the URL. If you want a tally of how many requests for each acct_id : egrep -o 'acct_id=[0-9]+' access.log | cut -d = -f2 | sort | uniq -c | sort -rn To continuously monitor changes, use watch , e.g. check changes to files in a directory with watch -d -n 2 'ls -rtlh | tail' or to network settings while troubleshooting your wifi settings with watch -d -n 2 ifconfig . Run this function to get a random tip from this document (parses Markdown and extracts an item): function taocl () { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md | sed '/cowsay[.]png/d' | pandoc -f markdown -t html | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v \"(html/body/ul/li[count(p)>0])[ $RANDOM mod last()+1]\" | xmlstarlet unesc | fmt -80 | iconv -t US }","title":"Bash one-liners"},{"location":"linux/one-liners/#useful-bash-one-liners","text":"","title":"Useful bash one-liners"},{"location":"linux/one-liners/#replace-text-in-template","text":"Replace TEMPLATE_TEXT with $BUILD_NUMBER var inside filename.json.template and save it into filename.json sed \"s/TEMPLATE_TEXT/1. $BUILD_NUMBER /g\" \\ filename.json.template > filename.json","title":"Replace text in template"},{"location":"linux/one-liners/#a-few-examples-of-piecing-together-commands","text":"It is remarkably helpful sometimes that you can do set intersection, union, and difference of text files via sort / uniq . Suppose a and b are text files that are already uniqued. This is fast, and works on files of arbitrary size, up to many gigabytes. (Sort is not limited by memory, though you may need to use the -T option if /tmp is on a small root partition.) See also the note about LC_ALL above and sort 's -u option (left out for clarity below). sort a b | uniq > c # c is a union b sort a b | uniq -d > c # c is a intersect b sort a b b | uniq -u > c # c is set difference a - b Pretty-print two JSON files, normalizing their syntax, then coloring and paginating the result: diff <(jq --sort-keys . < file1.json) <(jq --sort-keys . < file2.json) | colordiff | less -R Use grep . * to quickly examine the contents of all files in a directory (so each line is paired with the filename), or head -100 * (so each file has a heading). This can be useful for directories filled with config settings like those in /sys , /proc , /etc . Summing all numbers in the third column of a text file (this is probably 3X faster and 3X less code than equivalent Python): awk '{ x += $3 } END { print x }' myfile To see sizes/dates on a tree of files, this is like a recursive ls -l but is easier to read than ls -lR : find . -type f -ls Say you have a text file, like a web server log, and a certain value that appears on some lines, such as an acct_id parameter that is present in the URL. If you want a tally of how many requests for each acct_id : egrep -o 'acct_id=[0-9]+' access.log | cut -d = -f2 | sort | uniq -c | sort -rn To continuously monitor changes, use watch , e.g. check changes to files in a directory with watch -d -n 2 'ls -rtlh | tail' or to network settings while troubleshooting your wifi settings with watch -d -n 2 ifconfig . Run this function to get a random tip from this document (parses Markdown and extracts an item): function taocl () { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md | sed '/cowsay[.]png/d' | pandoc -f markdown -t html | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v \"(html/body/ul/li[count(p)>0])[ $RANDOM mod last()+1]\" | xmlstarlet unesc | fmt -80 | iconv -t US }","title":"A few examples of piecing together commands:"},{"location":"miscellaneous/","text":"Notes \u00b6 This section is for my own notes, tests and tryouts. Maybe some ideas and drafts.","title":"Notes"},{"location":"miscellaneous/#notes","text":"This section is for my own notes, tests and tryouts. Maybe some ideas and drafts.","title":"Notes"},{"location":"miscellaneous/bookshelf/","text":"Useful Links \u00b6 r/devops Books \u00b6 My current have-to-be-read list \u00b6 The Kubernetes Scheduler Imperative vs. Declarative \u2014 a Kubernetes Tutorial Deploy a Kubernetes Cluster with Cluster Monitoring in Amazon EC2 using Rancher 10 Advanced Tricks with Docker 50 Useful Docker Tutorials, From Beginner to Advanced \u0417\u043d\u0430\u043a\u043e\u043c\u0441\u0442\u0432\u043e \u0441 Kubernetes Security as Standard in the Land of Kubernetes. 10 Data Structure, Algorithms, and Programming Courses to Crack Any Coding Interview CRUD App using Vue.js and Django Comparing Kubernetes CNI Providers: Flannel, Calico, Canal, and Weave","title":"Bookshelf"},{"location":"miscellaneous/bookshelf/#useful-links","text":"r/devops","title":"Useful Links"},{"location":"miscellaneous/bookshelf/#books","text":"","title":"Books"},{"location":"miscellaneous/bookshelf/#my-current-have-to-be-read-list","text":"The Kubernetes Scheduler Imperative vs. Declarative \u2014 a Kubernetes Tutorial Deploy a Kubernetes Cluster with Cluster Monitoring in Amazon EC2 using Rancher 10 Advanced Tricks with Docker 50 Useful Docker Tutorials, From Beginner to Advanced \u0417\u043d\u0430\u043a\u043e\u043c\u0441\u0442\u0432\u043e \u0441 Kubernetes Security as Standard in the Land of Kubernetes. 10 Data Structure, Algorithms, and Programming Courses to Crack Any Coding Interview CRUD App using Vue.js and Django Comparing Kubernetes CNI Providers: Flannel, Calico, Canal, and Weave","title":"My current have-to-be-read list"},{"location":"miscellaneous/makdocs/","text":"Some useful info about mkdocs \u00b6 Documentation links For full documentation visit -> mkdocs.org Docs about Material template -> github.com Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Examples \u00b6 Tables \u00b6 Listener Target Group Purpose Attachment TCP 80 environment -acme-http HTTP LE verification requests The autoscaling group for ECS instances populates this target group on port 80. TCP 443 environment -acme-https SSL offload for pm/owners requests ECS service environment -acme-proxy populates this target group on port 443. Code \u00b6 def fn (): pass Marked text \u00b6 This is marked line of text inside regular text Highlighting specific lines \u00b6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Keyboard shortcuts \u00b6 Ctrl + Alt + My Special Key Formula \u00b6 \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} Checklist \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Image as code \u00b6 Nice service to convert images to Base65: https://www.base64-image.de","title":"Makdocs helper"},{"location":"miscellaneous/makdocs/#some-useful-info-about-mkdocs","text":"Documentation links For full documentation visit -> mkdocs.org Docs about Material template -> github.com","title":"Some useful info about mkdocs"},{"location":"miscellaneous/makdocs/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"miscellaneous/makdocs/#project-layout","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"miscellaneous/makdocs/#examples","text":"","title":"Examples"},{"location":"miscellaneous/makdocs/#tables","text":"Listener Target Group Purpose Attachment TCP 80 environment -acme-http HTTP LE verification requests The autoscaling group for ECS instances populates this target group on port 80. TCP 443 environment -acme-https SSL offload for pm/owners requests ECS service environment -acme-proxy populates this target group on port 443.","title":"Tables"},{"location":"miscellaneous/makdocs/#code","text":"def fn (): pass","title":"Code"},{"location":"miscellaneous/makdocs/#marked-text","text":"This is marked line of text inside regular text","title":"Marked text"},{"location":"miscellaneous/makdocs/#highlighting-specific-lines","text":"\"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Highlighting specific lines"},{"location":"miscellaneous/makdocs/#keyboard-shortcuts","text":"Ctrl + Alt + My Special Key","title":"Keyboard shortcuts"},{"location":"miscellaneous/makdocs/#formula","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Formula"},{"location":"miscellaneous/makdocs/#checklist","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Checklist"},{"location":"miscellaneous/makdocs/#image-as-code","text":"Nice service to convert images to Base65: https://www.base64-image.de","title":"Image as code"}]}