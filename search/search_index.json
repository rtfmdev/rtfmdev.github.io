{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Devops manuals, tips and notes \u00b6 Hello and welcome! Here I'm collecting some interesting and relevant manuals and notes about technologies and processes I use in my work or study right now for future projects. All these materials I've collected for my personal use, but you are free to use it and post it anywhere. If you have any advices, recommendations, please write comments and I'll answer as soon as possible. My current technologies stack: \u00b6 Automate CI/CD processes with GitlabCI , Github Actions and Jenkins . Containerize all with Docker and orchestrate them with Kubernetes . Infrasructure as a Code with Terraform and Ansible . Service discovery, mesh and secret management with Consul , Vault and Nomad . Message distributing with Apache Kafka and RabbitMQ Setup and support monitoring environment with ELK , Prometheus and Grafana . Infrastructure monitoring and analytics with New Relic . AWS cloud services management: EC2, ECS, Fargate, RDS, CloudWatch, etc. Serverless AWS Lambda with Javascript and Python . Hardware and network maintenances, system scripts and backend API with Python . About all this technologies, but not only, I write or soon will write here something interestig! Stay tuned! These pages are always on developing state. Some texts could be changed or extended. Sorry, but I'm working... :)","title":"Main page"},{"location":"#devops-manuals-tips-and-notes","text":"Hello and welcome! Here I'm collecting some interesting and relevant manuals and notes about technologies and processes I use in my work or study right now for future projects. All these materials I've collected for my personal use, but you are free to use it and post it anywhere. If you have any advices, recommendations, please write comments and I'll answer as soon as possible.","title":"Devops manuals, tips and notes"},{"location":"#my-current-technologies-stack","text":"Automate CI/CD processes with GitlabCI , Github Actions and Jenkins . Containerize all with Docker and orchestrate them with Kubernetes . Infrasructure as a Code with Terraform and Ansible . Service discovery, mesh and secret management with Consul , Vault and Nomad . Message distributing with Apache Kafka and RabbitMQ Setup and support monitoring environment with ELK , Prometheus and Grafana . Infrastructure monitoring and analytics with New Relic . AWS cloud services management: EC2, ECS, Fargate, RDS, CloudWatch, etc. Serverless AWS Lambda with Javascript and Python . Hardware and network maintenances, system scripts and backend API with Python . About all this technologies, but not only, I write or soon will write here something interestig! Stay tuned! These pages are always on developing state. Some texts could be changed or extended. Sorry, but I'm working... :)","title":"My current technologies stack:"},{"location":"AWS_services/","text":"Amazon Web Services \u00b6 Coming very soon...","title":"AWS services"},{"location":"AWS_services/#amazon-web-services","text":"Coming very soon...","title":"Amazon Web Services"},{"location":"AWS_services/useful-tips/","text":"Useful tips for AWS Services \u00b6","title":"Useful Tips"},{"location":"AWS_services/useful-tips/#useful-tips-for-aws-services","text":"","title":"Useful tips for AWS Services"},{"location":"dev-tools/git/","text":"Git tips and tricks \u00b6 Sometimes you have to make very rare manipulations with your git repo, but can't remember how exactly it works. Here I'm collecting useful git use cases. Replace master with better branch \u00b6 1 2 3 git checkout master git reset --hard better_branch git push -f origin master Merge master to current branch \u00b6 Merge all changes from master to your current branch: 1 git pull origin master and resolve conflicts if needed. Merge with squash \u00b6 Merge from branch with squash will erase all commits history in the branch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Work in feature/1 branch git branch \"feature/1\" git checkout \"feature/1\" # Move to the master branch. git checkout master # Merge the \"feature/1\" branch and squash the commits. git merge --squash feature/1 # Resolve conflicts # Commit your changes and add a single commit message for all your commits. # You can omit the \"-m\" to have a template popping up based on your previous commit messages. git commit -m \"Feature 1 : 1, 2 et 3\" # Delete the \"feature/1\" branch that is no longer needed. git branch -D feature/1 Reset \u00b6 Soft \u00b6 You've done a few tiny commits and want them all be put into one commit: A -> B -> C -> D || A with ALL (B,C,D) -> E 1 2 3 git reset --soft A git commit -m \u201cmy new merged commit\u201d git push origin branch New commit E contains all of the files that were committed in B, C, D. Mixed \u00b6 You\u2019ve just pushed a few commits, but you want to go back and remove a couple of files in a previous commit. A -> B-> C -> D || A with ANY (B,C,D) -> E 1 git reset --mixed A Your branch head and index is pointing at A and all of your changes in B, C and D are there, but are untracked. Now you are free to add the files that you wish to add into a new commit. 1 2 3 git add <files> git commit -m \"updated commit\" git push origin branch The head is now at the new commit E, and any files that you\u2019ve not staged will still be in your working tree, ready to add into another commit or to do what you want with. Hard \u00b6 Do it only if you want to go back a few commits and get rid of every change you\u2019ve made since. 1 2 git reset --hard A git push origin branchname --force This will delete your commits from the remote branch history. Add submodule \u00b6 1 2 3 git submodule add <link_to_git_repo> <link_to_submodule_folder> git submodule init git submodule update Edit last commit message \u00b6 1 git commit --amend Add missed file to last commit \u00b6 1 2 git add missed-file.txt git commit --amend Move last commit from master to branch \u00b6 1 2 3 git branch branch-name git reset HEAD~ --hard git checkout branch-name Clean up local commits before pushing \u00b6 1 git rebase --interactive if you didn't specify any tracking information for this branch you will have to add upstream and remote branch information: 1 git rebase --interactive origin branch","title":"Git Tricks"},{"location":"dev-tools/git/#git-tips-and-tricks","text":"Sometimes you have to make very rare manipulations with your git repo, but can't remember how exactly it works. Here I'm collecting useful git use cases.","title":"Git tips and tricks"},{"location":"dev-tools/git/#replace-master-with-better-branch","text":"1 2 3 git checkout master git reset --hard better_branch git push -f origin master","title":"Replace master with better branch"},{"location":"dev-tools/git/#merge-master-to-current-branch","text":"Merge all changes from master to your current branch: 1 git pull origin master and resolve conflicts if needed.","title":"Merge master to current branch"},{"location":"dev-tools/git/#merge-with-squash","text":"Merge from branch with squash will erase all commits history in the branch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Work in feature/1 branch git branch \"feature/1\" git checkout \"feature/1\" # Move to the master branch. git checkout master # Merge the \"feature/1\" branch and squash the commits. git merge --squash feature/1 # Resolve conflicts # Commit your changes and add a single commit message for all your commits. # You can omit the \"-m\" to have a template popping up based on your previous commit messages. git commit -m \"Feature 1 : 1, 2 et 3\" # Delete the \"feature/1\" branch that is no longer needed. git branch -D feature/1","title":"Merge with squash"},{"location":"dev-tools/git/#reset","text":"","title":"Reset"},{"location":"dev-tools/git/#soft","text":"You've done a few tiny commits and want them all be put into one commit: A -> B -> C -> D || A with ALL (B,C,D) -> E 1 2 3 git reset --soft A git commit -m \u201cmy new merged commit\u201d git push origin branch New commit E contains all of the files that were committed in B, C, D.","title":"Soft"},{"location":"dev-tools/git/#mixed","text":"You\u2019ve just pushed a few commits, but you want to go back and remove a couple of files in a previous commit. A -> B-> C -> D || A with ANY (B,C,D) -> E 1 git reset --mixed A Your branch head and index is pointing at A and all of your changes in B, C and D are there, but are untracked. Now you are free to add the files that you wish to add into a new commit. 1 2 3 git add <files> git commit -m \"updated commit\" git push origin branch The head is now at the new commit E, and any files that you\u2019ve not staged will still be in your working tree, ready to add into another commit or to do what you want with.","title":"Mixed"},{"location":"dev-tools/git/#hard","text":"Do it only if you want to go back a few commits and get rid of every change you\u2019ve made since. 1 2 git reset --hard A git push origin branchname --force This will delete your commits from the remote branch history.","title":"Hard"},{"location":"dev-tools/git/#add-submodule","text":"1 2 3 git submodule add <link_to_git_repo> <link_to_submodule_folder> git submodule init git submodule update","title":"Add submodule"},{"location":"dev-tools/git/#edit-last-commit-message","text":"1 git commit --amend","title":"Edit last commit message"},{"location":"dev-tools/git/#add-missed-file-to-last-commit","text":"1 2 git add missed-file.txt git commit --amend","title":"Add missed file to last commit"},{"location":"dev-tools/git/#move-last-commit-from-master-to-branch","text":"1 2 3 git branch branch-name git reset HEAD~ --hard git checkout branch-name","title":"Move last commit from master to branch"},{"location":"dev-tools/git/#clean-up-local-commits-before-pushing","text":"1 git rebase --interactive if you didn't specify any tracking information for this branch you will have to add upstream and remote branch information: 1 git rebase --interactive origin branch","title":"Clean up local commits before pushing"},{"location":"dev-tools/javascript/","text":"","title":"Javascript"},{"location":"dev-tools/mongo/","text":"","title":"MongoDB"},{"location":"dev-tools/one-liners/","text":"Replase text in template \u00b6 Replase TEMPLATE_TEXT with $BUILD_NUMBER var inside filename.json.template and save it into filename.json 1 2 sed \"s/TEMPLATE_TEXT/1. $BUILD_NUMBER /g\" \\ filename.json.template > filename.json","title":"Bash oneliners"},{"location":"dev-tools/one-liners/#replase-text-in-template","text":"Replase TEMPLATE_TEXT with $BUILD_NUMBER var inside filename.json.template and save it into filename.json 1 2 sed \"s/TEMPLATE_TEXT/1. $BUILD_NUMBER /g\" \\ filename.json.template > filename.json","title":"Replase text in template"},{"location":"dev-tools/python/","text":"Python3 on MacOS in a right way \u00b6 Install pyenv 1 2 3 brew install pyenv pyenv install 3 .8.0 pyenv global 3 .8.0 Add the following to your configuration file ( .zshrc for me, possibly .bash_profile for you): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 export PATH = \"/Users/denis/.pyenv/shims: ${ PATH } \" export PYENV_SHELL = zsh source '/usr/local/Cellar/pyenv/1.2.15/libexec/../completions/pyenv.zsh' command pyenv rehash 2 >/dev/null pyenv () { local command command = \" ${ 1 :- } \" if [ \" $# \" -gt 0 ] ; then shift fi case \" $command \" in rehash | shell ) eval \" $( pyenv \"sh- $command \" \" $@ \" ) \" ;; * ) command pyenv \" $command \" \" $@ \" ;; esac } Now we know for certain that we're using Python 3.8.0 and pip will update alongside it without any manual aliasing between versions. Lambda, map, reduce, filter \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Lambda foo = lambda n : n ** 2 if n % 2 == 0 else n ** 3 foo ( 5 ) # Output: 125 # Map col = [ 6 , 10 , 12 , 15 , 19 ] mapped = map ( lambda x : x * 10 , col ) # mapped = [60, 100, 120, 150, 190] # Filter filtered = filter ( lambda x : x % 2 == 0 and x > 7 , col ) # filtered = [10, 12] # Reduce from functools import reduce reduced = reduce ( lambda x , y : x * y + 10 , col ) # reduced = 242450 globals() locals() \u00b6 1 2 3 4 type(<object>) -> <ClassName> isinstance(<object>, <ClassName>) -> Boolean globals() -> dict with global vars locals() -> dict with local vars Context managers \u00b6 1 2 with open ( \"example.txt\" , \"w\" ) as file : # logic Two special methods: enter () and after () . They take care of what happens when execution enters and exits your with block respectively.. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from datetime import datetime class DataManager (): def __init__ ( self ): self . file = None def __enter__ ( self ): now = str ( datetime . now ()) . split ( \".\" )[ 0 ] . replece ( \" \" , \"-\" ) . replace . ( \":\" , \"-\" ) filename = now + \"-DATA.txt\" self . file = open ( filename , \"w\" ) return self . file def __exit__ ( self , exc_type , exc_value , exc_traceback ): self . file . close () print ( \"File closed\" ) with DataManager () as data : data . write ( \"hello!\" ) Decorators \u00b6 1 2 3 4 5 6 7 8 9 10 11 def is_positive ( func ): def wrapper ( * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return func ( * args , ** kwargs ) return wrapper @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2 Same decorations with class and special method call (): 1 2 3 4 5 6 7 8 9 10 11 12 13 class is_positive (): def __init__ ( self , func ): self . function = func def __call__ ( self , * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return self . function ( * args , ** kwargs ) @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2 Generators \u00b6 1 2 3 4 5 6 7 8 def squares ( a , b ): i = a while i < b : y yield i ** 2 i += 1 s = squares ( 5 , 10 ) next ( s )","title":"Python"},{"location":"dev-tools/python/#python3-on-macos-in-a-right-way","text":"Install pyenv 1 2 3 brew install pyenv pyenv install 3 .8.0 pyenv global 3 .8.0 Add the following to your configuration file ( .zshrc for me, possibly .bash_profile for you): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 export PATH = \"/Users/denis/.pyenv/shims: ${ PATH } \" export PYENV_SHELL = zsh source '/usr/local/Cellar/pyenv/1.2.15/libexec/../completions/pyenv.zsh' command pyenv rehash 2 >/dev/null pyenv () { local command command = \" ${ 1 :- } \" if [ \" $# \" -gt 0 ] ; then shift fi case \" $command \" in rehash | shell ) eval \" $( pyenv \"sh- $command \" \" $@ \" ) \" ;; * ) command pyenv \" $command \" \" $@ \" ;; esac } Now we know for certain that we're using Python 3.8.0 and pip will update alongside it without any manual aliasing between versions.","title":"Python3 on MacOS in a right way"},{"location":"dev-tools/python/#lambda-map-reduce-filter","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Lambda foo = lambda n : n ** 2 if n % 2 == 0 else n ** 3 foo ( 5 ) # Output: 125 # Map col = [ 6 , 10 , 12 , 15 , 19 ] mapped = map ( lambda x : x * 10 , col ) # mapped = [60, 100, 120, 150, 190] # Filter filtered = filter ( lambda x : x % 2 == 0 and x > 7 , col ) # filtered = [10, 12] # Reduce from functools import reduce reduced = reduce ( lambda x , y : x * y + 10 , col ) # reduced = 242450","title":"Lambda, map, reduce, filter"},{"location":"dev-tools/python/#globals-locals","text":"1 2 3 4 type(<object>) -> <ClassName> isinstance(<object>, <ClassName>) -> Boolean globals() -> dict with global vars locals() -> dict with local vars","title":"globals() locals()"},{"location":"dev-tools/python/#context-managers","text":"1 2 with open ( \"example.txt\" , \"w\" ) as file : # logic Two special methods: enter () and after () . They take care of what happens when execution enters and exits your with block respectively.. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from datetime import datetime class DataManager (): def __init__ ( self ): self . file = None def __enter__ ( self ): now = str ( datetime . now ()) . split ( \".\" )[ 0 ] . replece ( \" \" , \"-\" ) . replace . ( \":\" , \"-\" ) filename = now + \"-DATA.txt\" self . file = open ( filename , \"w\" ) return self . file def __exit__ ( self , exc_type , exc_value , exc_traceback ): self . file . close () print ( \"File closed\" ) with DataManager () as data : data . write ( \"hello!\" )","title":"Context managers"},{"location":"dev-tools/python/#decorators","text":"1 2 3 4 5 6 7 8 9 10 11 def is_positive ( func ): def wrapper ( * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return func ( * args , ** kwargs ) return wrapper @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2 Same decorations with class and special method call (): 1 2 3 4 5 6 7 8 9 10 11 12 13 class is_positive (): def __init__ ( self , func ): self . function = func def __call__ ( self , * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return self . function ( * args , ** kwargs ) @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2","title":"Decorators"},{"location":"dev-tools/python/#generators","text":"1 2 3 4 5 6 7 8 def squares ( a , b ): i = a while i < b : y yield i ** 2 i += 1 s = squares ( 5 , 10 ) next ( s )","title":"Generators"},{"location":"dev-tools/terraform/","text":"Terraform Up&Running \u00b6 IAM User Policies \u00b6 Add the following Managed Policies to your IAM user, as shown: 1 2 3 4 5 6 AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonRDSFullAccess CloudWatchFullAccess IAMFullAccess 1 2 export AWS_ACCESS_KEY_ID =( your access key id ) export AWS_SECRET_ACCESS_KEY =( your secret access key )","title":"Terraform"},{"location":"dev-tools/terraform/#terraform-uprunning","text":"","title":"Terraform Up&amp;Running"},{"location":"dev-tools/terraform/#iam-user-policies","text":"Add the following Managed Policies to your IAM user, as shown: 1 2 3 4 5 6 AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonRDSFullAccess CloudWatchFullAccess IAMFullAccess 1 2 export AWS_ACCESS_KEY_ID =( your access key id ) export AWS_SECRET_ACCESS_KEY =( your secret access key )","title":"IAM User Policies"},{"location":"docker/","text":"Docker tips and tricks \u00b6 Portainer \u00b6 Portainer - is very useful GUI for managing Docker's hosts and clusters. Simply open port 9000 on your host, run container and open in your browser host_ip:9000, create user and manage all your containers. 1 2 3 docker container run -d \\ -p 9000 :9000 \\ -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer Install Docker \u00b6 The Docker installation package available in the official Ubuntu repository may not be the latest version. To ensure we get the latest version, we\u2019ll install Docker from the official Docker repository. To do that, we\u2019ll add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package. First, update your existing list of packages: 1 sudo apt update Next, install a few prerequisite packages which let apt use packages over HTTPS: 1 sudo apt install apt-transport-https ca-certificates curl software-properties-common Then add the GPG key for the official Docker repository to your system: 1 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add the Docker repository to APT sources: 1 2 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" Next, update the package database with the Docker packages from the newly added repo and install Docker: 1 2 sudo apt update sudo apt install docker-ce Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it\u2019s running: 1 sudo systemctl status docker The output should be similar to the following, showing that the service is active and running: Output: 1 2 3 4 5 6 7 8 9 \u25cf docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-07-05 15:08:39 UTC; 2min 55s ago Docs: https://docs.docker.com Main PID: 10096 (dockerd) Tasks: 16 CGroup: /system.slice/docker.service \u251c\u250010096 /usr/bin/dockerd -H fd:// \u2514\u250010113 docker-containerd --config /var/run/docker/containerd/containerd.toml Installing Docker now gives you not just the Docker service (daemon) but also the docker command line utility, or the Docker client. Understanding containers has to cover things like chroot, the container file system and layers, cgroup isolation, and talk about the pros/cons of containers. An explanation of how these work without a discussion of cgroups and how that facilitates process/memory/network isolation is going to sound weak to an interviewer. So read up on how cgroups are implemented in the kernel. Then learn about the different ways lxc containers work vs docker containers. Then learn about emerging container specification standards. Why are they happening? Understand what\u2019s going on both under the hood in the kernel and the shifts happening in the industry. chroot \u00b6 A chroot environment provides functionality similar to that of a virtual machine, but it is a lighter solution. The captive system doesn\u2019t need a hypervisor to be installed and configured, such as VirtualBox or Virtual Machine Manager. Nor does it need to have a kernel installed in the captive system. The captive system shares your existing kernel. Creating chroot environment \u00b6 We need a directory to act as the root directory of the chroot environment. So that we have a shorthand way of referring to that directory we\u2019ll create a variable and store the name of the directory in it. Here we\u2019re setting up a variable to store a path to the \u201ctestroot\u201d directory. It doesn\u2019t matter if this directory doesn\u2019t exist yet, we\u2019re going to create it soon. If the directory does exist, it should be empty. 1 2 chr = /home/dave/testroot mkdir -p $chr We need to create directories to hold the portions of the operating system our chroot environment will require. We\u2019re going to set up a minimalist Linux environment that uses Bash as the interactive shell. We\u2019ll also include the touch, rm, and ls commands. That will allow us to use all Bash\u2019s built-in commands and touch, rm, and ls. We\u2019ll be able to create, list and remove files, and use Bash. And\u2014in this simple example\u2014that\u2019s all. 1 2 3 mkdir -p $chr / { bin,lib,lib64 } cd $chr Let\u2019s copy the binaries that we need in our minimalist Linux environment from your regular \u201c/bin\u201d directory into our chroot \u201c/bin\u201d directory. The -v (verbose) option makes cp tell us what it is doing as it performs each copy action. 1 cp -v /bin/ { bash,touch,ls,rm } $chr Dependencies \u00b6 These binaries will have dependencies. We need to discover what they are and copy those files into our environment. This way, for example we can add all dependencies for /bin/bash: 1 2 list = \" $( ldd /bin/bash | egrep -o '/lib.*\\.[0-9]' ) \" for i in $list ; do cp -v --parents \" $i \" \" ${ chr } \" ; done Use that technique to capture the dependencies of each of the other commands. Or you can write one more loop through all your apps. chroot command \u00b6 The last of our dependencies are copied into our chroot environment. We\u2019re finally ready to use the chroot command. This command sets the root of the chroot environment, and specifies which application to run as the shell. 1 sudo chroot $chr /bin/bash Our chroot environment is now active. The terminal window prompt has changed, and the interactive shell is the being handled by the bash shell in our environment. Use exit to leave the chroot environment: 1 exit","title":"Docker tips"},{"location":"docker/#docker-tips-and-tricks","text":"","title":"Docker tips and tricks"},{"location":"docker/#portainer","text":"Portainer - is very useful GUI for managing Docker's hosts and clusters. Simply open port 9000 on your host, run container and open in your browser host_ip:9000, create user and manage all your containers. 1 2 3 docker container run -d \\ -p 9000 :9000 \\ -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer","title":"Portainer"},{"location":"docker/#install-docker","text":"The Docker installation package available in the official Ubuntu repository may not be the latest version. To ensure we get the latest version, we\u2019ll install Docker from the official Docker repository. To do that, we\u2019ll add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package. First, update your existing list of packages: 1 sudo apt update Next, install a few prerequisite packages which let apt use packages over HTTPS: 1 sudo apt install apt-transport-https ca-certificates curl software-properties-common Then add the GPG key for the official Docker repository to your system: 1 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add the Docker repository to APT sources: 1 2 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" Next, update the package database with the Docker packages from the newly added repo and install Docker: 1 2 sudo apt update sudo apt install docker-ce Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it\u2019s running: 1 sudo systemctl status docker The output should be similar to the following, showing that the service is active and running: Output: 1 2 3 4 5 6 7 8 9 \u25cf docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-07-05 15:08:39 UTC; 2min 55s ago Docs: https://docs.docker.com Main PID: 10096 (dockerd) Tasks: 16 CGroup: /system.slice/docker.service \u251c\u250010096 /usr/bin/dockerd -H fd:// \u2514\u250010113 docker-containerd --config /var/run/docker/containerd/containerd.toml Installing Docker now gives you not just the Docker service (daemon) but also the docker command line utility, or the Docker client. Understanding containers has to cover things like chroot, the container file system and layers, cgroup isolation, and talk about the pros/cons of containers. An explanation of how these work without a discussion of cgroups and how that facilitates process/memory/network isolation is going to sound weak to an interviewer. So read up on how cgroups are implemented in the kernel. Then learn about the different ways lxc containers work vs docker containers. Then learn about emerging container specification standards. Why are they happening? Understand what\u2019s going on both under the hood in the kernel and the shifts happening in the industry.","title":"Install Docker"},{"location":"docker/#chroot","text":"A chroot environment provides functionality similar to that of a virtual machine, but it is a lighter solution. The captive system doesn\u2019t need a hypervisor to be installed and configured, such as VirtualBox or Virtual Machine Manager. Nor does it need to have a kernel installed in the captive system. The captive system shares your existing kernel.","title":"chroot"},{"location":"docker/#creating-chroot-environment","text":"We need a directory to act as the root directory of the chroot environment. So that we have a shorthand way of referring to that directory we\u2019ll create a variable and store the name of the directory in it. Here we\u2019re setting up a variable to store a path to the \u201ctestroot\u201d directory. It doesn\u2019t matter if this directory doesn\u2019t exist yet, we\u2019re going to create it soon. If the directory does exist, it should be empty. 1 2 chr = /home/dave/testroot mkdir -p $chr We need to create directories to hold the portions of the operating system our chroot environment will require. We\u2019re going to set up a minimalist Linux environment that uses Bash as the interactive shell. We\u2019ll also include the touch, rm, and ls commands. That will allow us to use all Bash\u2019s built-in commands and touch, rm, and ls. We\u2019ll be able to create, list and remove files, and use Bash. And\u2014in this simple example\u2014that\u2019s all. 1 2 3 mkdir -p $chr / { bin,lib,lib64 } cd $chr Let\u2019s copy the binaries that we need in our minimalist Linux environment from your regular \u201c/bin\u201d directory into our chroot \u201c/bin\u201d directory. The -v (verbose) option makes cp tell us what it is doing as it performs each copy action. 1 cp -v /bin/ { bash,touch,ls,rm } $chr","title":"Creating chroot environment"},{"location":"docker/#dependencies","text":"These binaries will have dependencies. We need to discover what they are and copy those files into our environment. This way, for example we can add all dependencies for /bin/bash: 1 2 list = \" $( ldd /bin/bash | egrep -o '/lib.*\\.[0-9]' ) \" for i in $list ; do cp -v --parents \" $i \" \" ${ chr } \" ; done Use that technique to capture the dependencies of each of the other commands. Or you can write one more loop through all your apps.","title":"Dependencies"},{"location":"docker/#chroot-command","text":"The last of our dependencies are copied into our chroot environment. We\u2019re finally ready to use the chroot command. This command sets the root of the chroot environment, and specifies which application to run as the shell. 1 sudo chroot $chr /bin/bash Our chroot environment is now active. The terminal window prompt has changed, and the interactive shell is the being handled by the bash shell in our environment. Use exit to leave the chroot environment: 1 exit","title":"chroot command"},{"location":"docker/commans/","text":"Docker commands cheat-sheet \u00b6 Docker docs and manuals Run busybox inside docker-compose \u00b6 1 docker run --rm -it --network <NETWORK_NAME> busybox Containers \u00b6 1 2 3 4 5 6 7 docker container ls # docker ps docker container top { cont_name } # top command for cont docker container inspect { cont_name } # json with all cont parameters docker container inspect { cont_name } --format '{{ .NetworkSettings.IPAddress}}' docekr container stats # like watch command but for containers docker container start # start stopped cont docker container post { cont_name } # shows shared ports Network \u00b6 1 2 3 4 5 docker network ls docker network inspect { network_name } docker network create --driver docker network connect { network_name } docker network disconnect { network_name } Reset \u00b6 Delete all running and stopped containers \u00b6 1 docker container rm -f $( docker ps -aq ) Unused cont prune on current host \u00b6 1 docker system prune --all --force --volumes Full docker prune on current host \u00b6 1 docker stop $( docker container ls -a -q ) && docker system prune -a -f --volumes Print last container\u2019s logs \u00b6 1 docker container logs --tail 100 web","title":"Cheatsheet"},{"location":"docker/commans/#docker-commands-cheat-sheet","text":"Docker docs and manuals","title":"Docker commands cheat-sheet"},{"location":"docker/commans/#run-busybox-inside-docker-compose","text":"1 docker run --rm -it --network <NETWORK_NAME> busybox","title":"Run busybox inside docker-compose"},{"location":"docker/commans/#containers","text":"1 2 3 4 5 6 7 docker container ls # docker ps docker container top { cont_name } # top command for cont docker container inspect { cont_name } # json with all cont parameters docker container inspect { cont_name } --format '{{ .NetworkSettings.IPAddress}}' docekr container stats # like watch command but for containers docker container start # start stopped cont docker container post { cont_name } # shows shared ports","title":"Containers"},{"location":"docker/commans/#network","text":"1 2 3 4 5 docker network ls docker network inspect { network_name } docker network create --driver docker network connect { network_name } docker network disconnect { network_name }","title":"Network"},{"location":"docker/commans/#reset","text":"","title":"Reset"},{"location":"docker/commans/#delete-all-running-and-stopped-containers","text":"1 docker container rm -f $( docker ps -aq )","title":"Delete all running and stopped containers"},{"location":"docker/commans/#unused-cont-prune-on-current-host","text":"1 docker system prune --all --force --volumes","title":"Unused cont prune on current host"},{"location":"docker/commans/#full-docker-prune-on-current-host","text":"1 docker stop $( docker container ls -a -q ) && docker system prune -a -f --volumes","title":"Full docker prune on current host"},{"location":"docker/commans/#print-last-containers-logs","text":"1 docker container logs --tail 100 web","title":"Print last container\u2019s logs"},{"location":"kubernetes/advanced_tools/","text":"","title":"Advanced tools"},{"location":"kubernetes/configMaps/","text":"ConfigMaps and Secrets \u00b6","title":"ConfigMaps and Secrets"},{"location":"kubernetes/configMaps/#configmaps-and-secrets","text":"","title":"ConfigMaps and Secrets"},{"location":"kubernetes/helm/","text":"Helm \u00b6 Helm package manager","title":"Helm"},{"location":"kubernetes/helm/#helm","text":"Helm package manager","title":"Helm"},{"location":"kubernetes/ingress/","text":"Ingress \u00b6 An Ingress is a collection of rules that allow inbound connections to reach the cluster Services. To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following: 1 2 3 4 5 TLS (Transport Layer Security) Name-based virtual hosting Fanout routing Loadbalancing Custom rules. With Ingress, users do not connect directly to a Service. Users reach the Ingress endpoint, and, from there, the request is forwarded to the desired Service. You can see an example of a sample Ingress definition below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : virtual-host-ingress namespace : default spec : rules : - host : blue.example.com http : paths : - backend : serviceName : webserver-blue-svc servicePort : 80 - host : green.example.com http : paths : - backend : serviceName : webserver-green-svc servicePort : 80 In the example above, user requests to both blue.example.com and green.example.com would go to the same Ingress endpoint, and, from there, they would be forwarded to webserver-blue-svc, and webserver-green-svc, respectively. This is an example of a Name-Based Virtual Hosting Ingress rule. We can also have Fanout Ingress rules, when requests to example.com/blue and example.com/green would be forwarded to webserver-blue-svc and webserver-green-svc, respectively: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : fan-out-ingress namespace : default spec : rules : - host : example.com http : paths : - path : /blue backend : serviceName : webserver-blue-svc servicePort : 80 - path : /green backend : serviceName : webserver-green-svc servicePort : 80 The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller. An Ingress Controller is an application watching the Master Node's API server for changes in the Ingress resources and updates the Layer 7 Load Balancer accordingly. Kubernetes supports different Ingress Controllers, and, if needed, we can also build our own. GCE L7 Load Balancer Controller and Nginx Ingress Controller are commonly used Ingress Controllers. Other controllers are Istio, Kong, Traefik, etc. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 # ingress.yaml apiVersion : v1 kind : Namespace metadata : name : nginx-ingress --- apiVersion : v1 kind : Secret metadata : name : default-server-secret namespace : nginx-ingress type : Opaque data : tls.crt : LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= tls.key : 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 --- apiVersion : v1 kind : ServiceAccount metadata : name : nginx-ingress namespace : nginx-ingress --- kind : ConfigMap apiVersion : v1 metadata : name : nginx-config namespace : nginx-ingress data : --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : nginx-ingress namespace : nginx-ingress spec : replicas : 1 selector : matchLabels : app : nginx-ingress template : metadata : labels : app : nginx-ingress spec : serviceAccountName : nginx-ingress containers : - image : nginx/nginx-ingress:edge imagePullPolicy : Always name : nginx-ingress ports : - name : http containerPort : 80 - name : https containerPort : 443 env : - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name args : - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret --- apiVersion : v1 kind : Service metadata : name : nginx-ingress namespace : nginx-ingress spec : type : NodePort ports : - port : 80 targetPort : 80 protocol : TCP name : http - port : 443 targetPort : 443 protocol : TCP name : https selector : app : nginx-ingress externalIPs : - 172.17.0.44 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # ingress-rules.yaml apiVersion : extensions/v1beta1 kind : Ingress metadata : name : webapp-ingress spec : rules : - host : my.kubernetes.example http : paths : - path : /webapp1 backend : serviceName : webapp1-svc servicePort : 80 - path : /webapp2 backend : serviceName : webapp2-svc servicePort : 80 - backend : serviceName : webapp3-svc servicePort : 80 1 $ kubectl get inggress","title":"Ingress"},{"location":"kubernetes/ingress/#ingress","text":"An Ingress is a collection of rules that allow inbound connections to reach the cluster Services. To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following: 1 2 3 4 5 TLS (Transport Layer Security) Name-based virtual hosting Fanout routing Loadbalancing Custom rules. With Ingress, users do not connect directly to a Service. Users reach the Ingress endpoint, and, from there, the request is forwarded to the desired Service. You can see an example of a sample Ingress definition below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : virtual-host-ingress namespace : default spec : rules : - host : blue.example.com http : paths : - backend : serviceName : webserver-blue-svc servicePort : 80 - host : green.example.com http : paths : - backend : serviceName : webserver-green-svc servicePort : 80 In the example above, user requests to both blue.example.com and green.example.com would go to the same Ingress endpoint, and, from there, they would be forwarded to webserver-blue-svc, and webserver-green-svc, respectively. This is an example of a Name-Based Virtual Hosting Ingress rule. We can also have Fanout Ingress rules, when requests to example.com/blue and example.com/green would be forwarded to webserver-blue-svc and webserver-green-svc, respectively: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : fan-out-ingress namespace : default spec : rules : - host : example.com http : paths : - path : /blue backend : serviceName : webserver-blue-svc servicePort : 80 - path : /green backend : serviceName : webserver-green-svc servicePort : 80 The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller. An Ingress Controller is an application watching the Master Node's API server for changes in the Ingress resources and updates the Layer 7 Load Balancer accordingly. Kubernetes supports different Ingress Controllers, and, if needed, we can also build our own. GCE L7 Load Balancer Controller and Nginx Ingress Controller are commonly used Ingress Controllers. Other controllers are Istio, Kong, Traefik, etc. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 # ingress.yaml apiVersion : v1 kind : Namespace metadata : name : nginx-ingress --- apiVersion : v1 kind : Secret metadata : name : default-server-secret namespace : nginx-ingress type : Opaque data : tls.crt : LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= tls.key : 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 --- apiVersion : v1 kind : ServiceAccount metadata : name : nginx-ingress namespace : nginx-ingress --- kind : ConfigMap apiVersion : v1 metadata : name : nginx-config namespace : nginx-ingress data : --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : nginx-ingress namespace : nginx-ingress spec : replicas : 1 selector : matchLabels : app : nginx-ingress template : metadata : labels : app : nginx-ingress spec : serviceAccountName : nginx-ingress containers : - image : nginx/nginx-ingress:edge imagePullPolicy : Always name : nginx-ingress ports : - name : http containerPort : 80 - name : https containerPort : 443 env : - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name args : - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret --- apiVersion : v1 kind : Service metadata : name : nginx-ingress namespace : nginx-ingress spec : type : NodePort ports : - port : 80 targetPort : 80 protocol : TCP name : http - port : 443 targetPort : 443 protocol : TCP name : https selector : app : nginx-ingress externalIPs : - 172.17.0.44 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # ingress-rules.yaml apiVersion : extensions/v1beta1 kind : Ingress metadata : name : webapp-ingress spec : rules : - host : my.kubernetes.example http : paths : - path : /webapp1 backend : serviceName : webapp1-svc servicePort : 80 - path : /webapp2 backend : serviceName : webapp2-svc servicePort : 80 - backend : serviceName : webapp3-svc servicePort : 80 1 $ kubectl get inggress","title":"Ingress"},{"location":"kubernetes/kubectl/","text":"kubectl cheatsheet \u00b6 Connection details \u00b6 To look at the connection details, we can either see the content of the ~/.kube/config file (on Linux) or run the following command: 1 $ kubectl config view Cluster info \u00b6 1 kubectl cluster-info When not using the kubectl proxy, we need to authenticate to the API server when sending API requests. We can authenticate by providing a Bearer Token when issuing a curl, or by providing a set of keys and certificates. A Bearer Token is an access token which is generated by the authentication server (the API server on the master node) and given back to the client. Using that token, the client can connect back to the Kubernetes API server without providing further authentication details, and then, access resources. Get the token: \u00b6 1 $ TOKEN = $( kubectl describe secret -n kube-system $( kubectl get secrets -n kube-system | grep default | cut -f1 -d ' ' ) | grep -E '^token' | cut -f2 -d ':' | tr -d '\\t' | tr -d \" \" ) Get the API server endpoint: \u00b6 1 $ APISERVER = $( kubectl config view | grep https | cut -f 2 - -d \":\" | tr -d \" \" ) Access the API server \u00b6 1 $ curl $APISERVER --header \"Authorization: Bearer $TOKEN \" --insecure Instead of the access token, we can extract the client certificate, client key, and certificate authority data from the .kube/config file. Once extracted, they are encoded and then passed with a curl command for authentication. The new curl command looks similar to: 1 $ curl $APISERVER --cert encoded-cert --key encoded-key --cacert encoded-ca List the Pods \u00b6 Along with their attached Labels \u00b6 With the -L option to the kubectl get pods command, we add extra columns in the output to list Pods with their attached Label keys and their values. In the following example, we are listing Pods with the Label keys k8s-app and label2 : 1 2 3 4 5 $ kubectl get pods -L k8s-app,label2 NAME READY STATUS RESTARTS AGE K8S-APP LABEL2 webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 16m webserver Pods with a given Label \u00b6 To use a selector with the kubectl get pods command, we can use the -l option. In the following example, we are selecting all the Pods that have the k8s-app Label key set to value webserver : 1 2 3 4 5 $ kubectl get pods -l k8s-app = webserver NAME READY STATUS RESTARTS AGE webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 23m webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 24m webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 23m Look at a Pod's Details \u00b6 We can look at an object's details using kubectl describe command. In the following example, you can see a Pod's description: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 $ kubectl describe pod webserver-c8f4d5fbc-ggqkj Name: webserver-c8f4d5fbc-ggqkj Namespace: default Priority: 0 Node: minikube/10.0.2.15 Start Time: Sat, 05 Oct 2019 14 :37:34 +0300 Labels: k8s-app = webserver pod-template-hash = c8f4d5fbc Annotations: <none> Status: Running IP: 172 .17.0.7 IPs: IP: 172 .17.0.7 Controlled By: ReplicaSet/webserver-c8f4d5fbc Containers: webserver: Container ID: docker://b7d6bd7ce9eaefe48523d486f7174ac748571546bec4d8674b89d4438c8707da Image: nginx:alpine Image ID: docker-pullable://nginx@sha256:77f340700d08fd45026823f44fc0010a5bd2237c2d049178b473cd2ad977d071 Port: <none> Host Port: <none> State: Running Started: Sat, 05 Oct 2019 14 :37:36 +0300 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-r6llk ( ro ) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-r6llk: Type: Secret ( a volume populated by a Secret ) SecretName: default-token-r6llk Optional: false QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled <unknown> default-scheduler Successfully assigned default/webserver-c8f4d5fbc-ggqkj to minikube Normal Pulled 19m kubelet, minikube Container image \"nginx:alpine\" already present on machine Normal Created 19m kubelet, minikube Created container webserver Normal Started 19m kubelet, minikube Started container webserver Exposing an Application \u00b6 For a NodePort ServiceType, Kubernetes opens up a static port on all the worker nodes. If we connect to that port from any node, we are proxied to the ClusterIP of the Service. Next, let's use the NodePort ServiceType while creating a Service. Create a webserver-svc.yaml file with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : Service metadata : name : web-service labels : run : web-service spec : type : NodePort ports : - port : 80 protocol : TCP selector : app : nginx Using kubectl, create the Service: 1 2 $ kubectl create -f webserver-svc.yaml service/web-service created A more direct method of creating a Service is by exposing the previously created Deployment (this method requires an existing Deployment). Expose a Deployment with the kubectl expose command: 1 2 $ kubectl expose deployment webserver --name = web-service --type = NodePort service/web-service exposed Create an NGINX Pod kubectl run --generator=run-pod/v1 nginx --image=nginx Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml label Create a deployment kubectl run --generator=deployment/v1beta1 nginx --image=nginx Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run -o yaml Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml Save it to a file - (If you need to modify or add some other details) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml >\u00c2 nginx-deployment.yaml more kubectl commands and manuals is here","title":"kubectl cheatsheet"},{"location":"kubernetes/kubectl/#kubectl-cheatsheet","text":"","title":"kubectl cheatsheet"},{"location":"kubernetes/kubectl/#connection-details","text":"To look at the connection details, we can either see the content of the ~/.kube/config file (on Linux) or run the following command: 1 $ kubectl config view","title":"Connection details"},{"location":"kubernetes/kubectl/#cluster-info","text":"1 kubectl cluster-info When not using the kubectl proxy, we need to authenticate to the API server when sending API requests. We can authenticate by providing a Bearer Token when issuing a curl, or by providing a set of keys and certificates. A Bearer Token is an access token which is generated by the authentication server (the API server on the master node) and given back to the client. Using that token, the client can connect back to the Kubernetes API server without providing further authentication details, and then, access resources.","title":"Cluster info"},{"location":"kubernetes/kubectl/#get-the-token","text":"1 $ TOKEN = $( kubectl describe secret -n kube-system $( kubectl get secrets -n kube-system | grep default | cut -f1 -d ' ' ) | grep -E '^token' | cut -f2 -d ':' | tr -d '\\t' | tr -d \" \" )","title":"Get the token:"},{"location":"kubernetes/kubectl/#get-the-api-server-endpoint","text":"1 $ APISERVER = $( kubectl config view | grep https | cut -f 2 - -d \":\" | tr -d \" \" )","title":"Get the API server endpoint:"},{"location":"kubernetes/kubectl/#access-the-api-server","text":"1 $ curl $APISERVER --header \"Authorization: Bearer $TOKEN \" --insecure Instead of the access token, we can extract the client certificate, client key, and certificate authority data from the .kube/config file. Once extracted, they are encoded and then passed with a curl command for authentication. The new curl command looks similar to: 1 $ curl $APISERVER --cert encoded-cert --key encoded-key --cacert encoded-ca","title":"Access the API server"},{"location":"kubernetes/kubectl/#list-the-pods","text":"","title":"List the Pods"},{"location":"kubernetes/kubectl/#along-with-their-attached-labels","text":"With the -L option to the kubectl get pods command, we add extra columns in the output to list Pods with their attached Label keys and their values. In the following example, we are listing Pods with the Label keys k8s-app and label2 : 1 2 3 4 5 $ kubectl get pods -L k8s-app,label2 NAME READY STATUS RESTARTS AGE K8S-APP LABEL2 webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 16m webserver","title":"Along with their attached Labels"},{"location":"kubernetes/kubectl/#pods-with-a-given-label","text":"To use a selector with the kubectl get pods command, we can use the -l option. In the following example, we are selecting all the Pods that have the k8s-app Label key set to value webserver : 1 2 3 4 5 $ kubectl get pods -l k8s-app = webserver NAME READY STATUS RESTARTS AGE webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 23m webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 24m webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 23m","title":"Pods with a given Label"},{"location":"kubernetes/kubectl/#look-at-a-pods-details","text":"We can look at an object's details using kubectl describe command. In the following example, you can see a Pod's description: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 $ kubectl describe pod webserver-c8f4d5fbc-ggqkj Name: webserver-c8f4d5fbc-ggqkj Namespace: default Priority: 0 Node: minikube/10.0.2.15 Start Time: Sat, 05 Oct 2019 14 :37:34 +0300 Labels: k8s-app = webserver pod-template-hash = c8f4d5fbc Annotations: <none> Status: Running IP: 172 .17.0.7 IPs: IP: 172 .17.0.7 Controlled By: ReplicaSet/webserver-c8f4d5fbc Containers: webserver: Container ID: docker://b7d6bd7ce9eaefe48523d486f7174ac748571546bec4d8674b89d4438c8707da Image: nginx:alpine Image ID: docker-pullable://nginx@sha256:77f340700d08fd45026823f44fc0010a5bd2237c2d049178b473cd2ad977d071 Port: <none> Host Port: <none> State: Running Started: Sat, 05 Oct 2019 14 :37:36 +0300 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-r6llk ( ro ) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-r6llk: Type: Secret ( a volume populated by a Secret ) SecretName: default-token-r6llk Optional: false QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled <unknown> default-scheduler Successfully assigned default/webserver-c8f4d5fbc-ggqkj to minikube Normal Pulled 19m kubelet, minikube Container image \"nginx:alpine\" already present on machine Normal Created 19m kubelet, minikube Created container webserver Normal Started 19m kubelet, minikube Started container webserver","title":"Look at a Pod's Details"},{"location":"kubernetes/kubectl/#exposing-an-application","text":"For a NodePort ServiceType, Kubernetes opens up a static port on all the worker nodes. If we connect to that port from any node, we are proxied to the ClusterIP of the Service. Next, let's use the NodePort ServiceType while creating a Service. Create a webserver-svc.yaml file with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : Service metadata : name : web-service labels : run : web-service spec : type : NodePort ports : - port : 80 protocol : TCP selector : app : nginx Using kubectl, create the Service: 1 2 $ kubectl create -f webserver-svc.yaml service/web-service created A more direct method of creating a Service is by exposing the previously created Deployment (this method requires an existing Deployment). Expose a Deployment with the kubectl expose command: 1 2 $ kubectl expose deployment webserver --name = web-service --type = NodePort service/web-service exposed Create an NGINX Pod kubectl run --generator=run-pod/v1 nginx --image=nginx Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml label Create a deployment kubectl run --generator=deployment/v1beta1 nginx --image=nginx Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run -o yaml Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml Save it to a file - (If you need to modify or add some other details) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml >\u00c2 nginx-deployment.yaml more kubectl commands and manuals is here","title":"Exposing an Application"},{"location":"kubernetes/monitoring/","text":"Services Monitoring Workflow \u00b6 Services Monitoring Workflow consists of three main components: 1 Collection agent, Metrics Server, and Dashboards. Typical workflow, including most common components: Monitoring agent collects node metrics. cAdvisor collects containers and pods metrics. Monitoring Aggregation service collects data from its own agent and cAdvisor. Data is stored in the monitoring system\u2019s storage. Monitoring aggregation service exposes metrics through APIs and dashboards. A Few Notes: Prometheus is the official monitoring server sponsored and incubated by CNCF. It integrates directly with cAdvisor. You don\u2019t need to install a 3 rd party agent to retrieve additional metrics about your containers. However, if you need deeper insights about each node, you need to install an agent of your choice \u2014 see Prometheus integrations and third-party exporters page. Almost all monitoring systems piggyback on Kubernetes scheduling and orchestration. For example, their agents are installed as DeomonSets and depend on Kubernetes scheduler to have an instance scheduled on each node. Most monitoring agents depend on Kubelet to collect container relevant metrics, which in turn depends on cAdvisor. Very few agents collect container relevant details independently. Most monitoring aggregation services depend on agents pushing metrics to them. Prometheus is an exception. It pulls metrics out of the installed agents. What to monitor \u00b6 Ideal Services Workflow depends on this factors: - collection of relevant metrics - perception of continuous changes inside the k8s cluster. A good pipeline should focus on collecting relevant metrics. There are plenty of agents that can collect OS and process-level metrics. But you will find very few out there that can collect details about containers running at a given node, such as the number of running containers, container state, docker engine metrics, etc. cAdvisor is the best agent for this job. Perception of continuous changes means that the monitoring pipeline is aware of different pods, containers instances and can relate them to their parent entities, i.e. Deployment, Statefulsets, Namespace, etc. It also means that the metrics server is aware of system-wide metrics that should be visible to users, such as the number of pending pods, nodes status, etc. TL;DR \u00b6 You need to differentiate between core metrics pipeline and the services pipeline. You should pick the best pipeline that works for your needs. The community official metrics collector tool is Prometheus. Use Grafana Dashboards for visualization . But not for alerting.","title":"Monitoring"},{"location":"kubernetes/monitoring/#services-monitoring-workflow","text":"Services Monitoring Workflow consists of three main components: 1 Collection agent, Metrics Server, and Dashboards. Typical workflow, including most common components: Monitoring agent collects node metrics. cAdvisor collects containers and pods metrics. Monitoring Aggregation service collects data from its own agent and cAdvisor. Data is stored in the monitoring system\u2019s storage. Monitoring aggregation service exposes metrics through APIs and dashboards. A Few Notes: Prometheus is the official monitoring server sponsored and incubated by CNCF. It integrates directly with cAdvisor. You don\u2019t need to install a 3 rd party agent to retrieve additional metrics about your containers. However, if you need deeper insights about each node, you need to install an agent of your choice \u2014 see Prometheus integrations and third-party exporters page. Almost all monitoring systems piggyback on Kubernetes scheduling and orchestration. For example, their agents are installed as DeomonSets and depend on Kubernetes scheduler to have an instance scheduled on each node. Most monitoring agents depend on Kubelet to collect container relevant metrics, which in turn depends on cAdvisor. Very few agents collect container relevant details independently. Most monitoring aggregation services depend on agents pushing metrics to them. Prometheus is an exception. It pulls metrics out of the installed agents.","title":"Services Monitoring Workflow"},{"location":"kubernetes/monitoring/#what-to-monitor","text":"Ideal Services Workflow depends on this factors: - collection of relevant metrics - perception of continuous changes inside the k8s cluster. A good pipeline should focus on collecting relevant metrics. There are plenty of agents that can collect OS and process-level metrics. But you will find very few out there that can collect details about containers running at a given node, such as the number of running containers, container state, docker engine metrics, etc. cAdvisor is the best agent for this job. Perception of continuous changes means that the monitoring pipeline is aware of different pods, containers instances and can relate them to their parent entities, i.e. Deployment, Statefulsets, Namespace, etc. It also means that the metrics server is aware of system-wide metrics that should be visible to users, such as the number of pending pods, nodes status, etc.","title":"What to monitor"},{"location":"kubernetes/monitoring/#tldr","text":"You need to differentiate between core metrics pipeline and the services pipeline. You should pick the best pipeline that works for your needs. The community official metrics collector tool is Prometheus. Use Grafana Dashboards for visualization . But not for alerting.","title":"TL;DR"},{"location":"kubernetes/networking/","text":"About Flanel: https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c A ServiceAccount is required to login. A ClusterRoleBinding is used to assign the new ServiceAccount (admin-user) the role of cluster-admin on the cluster. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat <<EOF | kubectl create -f - apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : kube-system --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : kube-system EOF This means they can control all aspects of Kubernetes. With ClusterRoleBinding and RBAC, different level of permissions can be defined based on security requirements. More information on creating a user for the Dashboard can be found in the Dashboard documentation. Once the ServiceAccount has been created, the token to login can be found with: 1 kubectl -n kube-system describe secret $( kubectl -n kube-system get secret | grep admin-user | awk '{print $1}' ) cluster-ip.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion : v1 kind : Service metadata : name : webapp1-clusterip-svc labels : app : webapp1-clusterip spec : ports : - port : 80 selector : app : webapp1-clusterip --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-clusterip-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-clusterip spec : containers : - name : webapp1-clusterip-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- 1 export CLUSTER_IP = $( kubectl get services/webapp1-clusterip-svc -o go-template = '{{(index .spec.clusterIP)}}' ) clusterip-target.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : v1 kind : Service metadata : name : webapp1-clusterip-targetport-svc labels : app : webapp1-clusterip-targetport spec : ports : - port : 8080 targetPort : 80 selector : app : webapp1-clusterip-targetport --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-clusterip-targetport-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-clusterip-targetport spec : containers : - name : webapp1-clusterip-targetport-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- 1 export CLUSTER_IP = $( kubectl get services/webapp1-clusterip-targetport-svc -o go-template = '{{(index .spec.clusterIP)}}' ) nodeport.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion : v1 kind : Service metadata : name : webapp1-nodeport-svc labels : app : webapp1-nodeport spec : type : NodePort ports : - port : 80 nodePort : 30080 selector : app : webapp1-nodeport --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-nodeport-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-nodeport spec : containers : - name : webapp1-nodeport-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- externalip.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion : v1 kind : Service metadata : name : webapp1-externalip-svc labels : app : webapp1-externalip spec : ports : - port : 80 externalIPs : - 172.17.0.82 selector : app : webapp1-externalip --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-externalip-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-externalip spec : containers : - name : webapp1-externalip-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- cloudprovider.yaml \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 apiVersion : extensions/v1beta1 kind : DaemonSet metadata : name : kube-keepalived-vip namespace : kube-system spec : template : metadata : labels : name : kube-keepalived-vip spec : hostNetwork : true containers : - image : gcr.io/google_containers/kube-keepalived-vip:0.9 name : kube-keepalived-vip imagePullPolicy : Always securityContext : privileged : true volumeMounts : - mountPath : /lib/modules name : modules readOnly : true - mountPath : /dev name : dev # use downward API env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace # to use unicast args : - --services-configmap=kube-system/vip-configmap # unicast uses the ip of the nodes instead of multicast # this is useful if running in cloud providers (like AWS) #- --use-unicast=true volumes : - name : modules hostPath : path : /lib/modules - name : dev hostPath : path : /dev nodeSelector : # type: worker # adjust this to match your worker nodes --- ## We also create an empty ConfigMap to hold our config apiVersion : v1 kind : ConfigMap metadata : name : vip-configmap namespace : kube-system data : --- apiVersion : apps/v1beta1 kind : Deployment metadata : labels : app : keepalived-cloud-provider name : keepalived-cloud-provider namespace : kube-system spec : replicas : 1 revisionHistoryLimit : 2 selector : matchLabels : app : keepalived-cloud-provider strategy : type : RollingUpdate template : metadata : annotations : scheduler.alpha.kubernetes.io/critical-pod : \"\" scheduler.alpha.kubernetes.io/tolerations : '[{\"key\":\"CriticalAddonsOnly\", \"operator\":\"Exists\"}]' labels : app : keepalived-cloud-provider spec : containers : - name : keepalived-cloud-provider image : quay.io/munnerz/keepalived-cloud-provider:0.0.1 imagePullPolicy : IfNotPresent env : - name : KEEPALIVED_NAMESPACE value : kube-system - name : KEEPALIVED_CONFIG_MAP value : vip-configmap - name : KEEPALIVED_SERVICE_CIDR value : 10.10.0.0/26 # pick a CIDR that is explicitly reserved for keepalived volumeMounts : - name : certs mountPath : /etc/ssl/certs resources : requests : cpu : 200m livenessProbe : httpGet : path : /healthz port : 10252 host : 127.0.0.1 initialDelaySeconds : 15 timeoutSeconds : 15 failureThreshold : 8 volumes : - name : certs hostPath : path : /etc/ssl/certs loadbalancer.yaml \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : v1 kind : Service metadata : name : webapp1-loadbalancer-svc labels : app : webapp1-loadbalancer spec : type : LoadBalancer ports : - port : 80 selector : app : webapp1-loadbalancer --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-loadbalancer-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-loadbalancer spec : containers : - name : webapp1-loadbalancer-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- 1 export LoadBalancerIP = $( kubectl get services/webapp1-loadbalancer-svc -o go-template = '{{(index .status.loadBalancer.ingress 0).ip}}' )","title":"Networking"},{"location":"kubernetes/networking/#cloudprovideryaml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 apiVersion : extensions/v1beta1 kind : DaemonSet metadata : name : kube-keepalived-vip namespace : kube-system spec : template : metadata : labels : name : kube-keepalived-vip spec : hostNetwork : true containers : - image : gcr.io/google_containers/kube-keepalived-vip:0.9 name : kube-keepalived-vip imagePullPolicy : Always securityContext : privileged : true volumeMounts : - mountPath : /lib/modules name : modules readOnly : true - mountPath : /dev name : dev # use downward API env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace # to use unicast args : - --services-configmap=kube-system/vip-configmap # unicast uses the ip of the nodes instead of multicast # this is useful if running in cloud providers (like AWS) #- --use-unicast=true volumes : - name : modules hostPath : path : /lib/modules - name : dev hostPath : path : /dev nodeSelector : # type: worker # adjust this to match your worker nodes --- ## We also create an empty ConfigMap to hold our config apiVersion : v1 kind : ConfigMap metadata : name : vip-configmap namespace : kube-system data : --- apiVersion : apps/v1beta1 kind : Deployment metadata : labels : app : keepalived-cloud-provider name : keepalived-cloud-provider namespace : kube-system spec : replicas : 1 revisionHistoryLimit : 2 selector : matchLabels : app : keepalived-cloud-provider strategy : type : RollingUpdate template : metadata : annotations : scheduler.alpha.kubernetes.io/critical-pod : \"\" scheduler.alpha.kubernetes.io/tolerations : '[{\"key\":\"CriticalAddonsOnly\", \"operator\":\"Exists\"}]' labels : app : keepalived-cloud-provider spec : containers : - name : keepalived-cloud-provider image : quay.io/munnerz/keepalived-cloud-provider:0.0.1 imagePullPolicy : IfNotPresent env : - name : KEEPALIVED_NAMESPACE value : kube-system - name : KEEPALIVED_CONFIG_MAP value : vip-configmap - name : KEEPALIVED_SERVICE_CIDR value : 10.10.0.0/26 # pick a CIDR that is explicitly reserved for keepalived volumeMounts : - name : certs mountPath : /etc/ssl/certs resources : requests : cpu : 200m livenessProbe : httpGet : path : /healthz port : 10252 host : 127.0.0.1 initialDelaySeconds : 15 timeoutSeconds : 15 failureThreshold : 8 volumes : - name : certs hostPath : path : /etc/ssl/certs","title":"cloudprovider.yaml"},{"location":"kubernetes/networking/#loadbalanceryaml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : v1 kind : Service metadata : name : webapp1-loadbalancer-svc labels : app : webapp1-loadbalancer spec : type : LoadBalancer ports : - port : 80 selector : app : webapp1-loadbalancer --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-loadbalancer-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-loadbalancer spec : containers : - name : webapp1-loadbalancer-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- 1 export LoadBalancerIP = $( kubectl get services/webapp1-loadbalancer-svc -o go-template = '{{(index .status.loadBalancer.ingress 0).ip}}' )","title":"loadbalancer.yaml"},{"location":"kubernetes/volume_types/","text":"Volume Types \u00b6 A directory which is mounted inside a Pod is backed by the underlying Volume Type. A Volume Type decides the properties of the directory, like size, content, default access modes, etc. Some examples of Volume Types are: emptyDir An empty Volume is created for the Pod as soon as it is scheduled on the worker node. The Volume's life is tightly coupled with the Pod. If the Pod is terminated, the content of emptyDir is deleted forever. hostPath With the hostPath Volume Type, we can share a directory from the host to the Pod. If the Pod is terminated, the content of the Volume is still available on the host. gcePersistentDisk With the gcePersistentDisk Volume Type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStore With the awsElasticBlockStore Volume Type, we can mount an AWS EBS Volume into a Pod. azureDisk With azureDisk we can mount a Microsoft Azure Data Disk into a Pod. azureFile With azureFile we can mount a Microsoft Azure File Volume into a Pod. cephfs With cephfs, an existing CephFS volume can be mounted into a Pod. When a Pod terminates, the volume is unmounted and the contents of the volume are preserved. nfs With nfs, we can mount an NFS share into a Pod. iscsi With iscsi, we can mount an iSCSI share into a Pod. secret With the secret Volume Type, we can pass sensitive information, such as passwords, to Pods. configMap With configMap objects, we can provide configuration data, or shell commands and arguments into a Pod. persistentVolumeClaim We can attach a PersistentVolume to a Pod using a persistentVolumeClaim. PersistentVolumes \u00b6 In a typical IT environment, storage is managed by the storage/system administrators. The end user will just receive instructions to use the storage but is not involved with the underlying storage management. In the containerized world, we would like to follow similar rules, but it becomes challenging, given the many Volume Types we have seen earlier. Kubernetes resolves this problem with the PersistentVolume (PV) subsystem, which provides APIs for users and administrators to manage and consume persistent storage. To manage the Volume, it uses the PersistentVolume API resource type, and to consume it, it uses the PersistentVolumeClaim API resource type. A Persistent Volume is a network-attached storage in the cluster, which is provisioned by the administrator. PersistentVolumes can be dynamically provisioned based on the StorageClass resource. A StorageClass contains pre-defined provisioners and parameters to create a PersistentVolume. Using PersistentVolumeClaims, a user sends the request for dynamic PV creation, which gets wired to the StorageClass resource. Some of the Volume Types that support managing storage using PersistentVolumes are: 1 2 3 4 5 6 7 GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CephFS NFS iSCSI You can learn more details about Volume Types in the Kubernetes documentation .","title":"Volume Types"},{"location":"kubernetes/volume_types/#volume-types","text":"A directory which is mounted inside a Pod is backed by the underlying Volume Type. A Volume Type decides the properties of the directory, like size, content, default access modes, etc. Some examples of Volume Types are: emptyDir An empty Volume is created for the Pod as soon as it is scheduled on the worker node. The Volume's life is tightly coupled with the Pod. If the Pod is terminated, the content of emptyDir is deleted forever. hostPath With the hostPath Volume Type, we can share a directory from the host to the Pod. If the Pod is terminated, the content of the Volume is still available on the host. gcePersistentDisk With the gcePersistentDisk Volume Type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStore With the awsElasticBlockStore Volume Type, we can mount an AWS EBS Volume into a Pod. azureDisk With azureDisk we can mount a Microsoft Azure Data Disk into a Pod. azureFile With azureFile we can mount a Microsoft Azure File Volume into a Pod. cephfs With cephfs, an existing CephFS volume can be mounted into a Pod. When a Pod terminates, the volume is unmounted and the contents of the volume are preserved. nfs With nfs, we can mount an NFS share into a Pod. iscsi With iscsi, we can mount an iSCSI share into a Pod. secret With the secret Volume Type, we can pass sensitive information, such as passwords, to Pods. configMap With configMap objects, we can provide configuration data, or shell commands and arguments into a Pod. persistentVolumeClaim We can attach a PersistentVolume to a Pod using a persistentVolumeClaim.","title":"Volume Types"},{"location":"kubernetes/volume_types/#persistentvolumes","text":"In a typical IT environment, storage is managed by the storage/system administrators. The end user will just receive instructions to use the storage but is not involved with the underlying storage management. In the containerized world, we would like to follow similar rules, but it becomes challenging, given the many Volume Types we have seen earlier. Kubernetes resolves this problem with the PersistentVolume (PV) subsystem, which provides APIs for users and administrators to manage and consume persistent storage. To manage the Volume, it uses the PersistentVolume API resource type, and to consume it, it uses the PersistentVolumeClaim API resource type. A Persistent Volume is a network-attached storage in the cluster, which is provisioned by the administrator. PersistentVolumes can be dynamically provisioned based on the StorageClass resource. A StorageClass contains pre-defined provisioners and parameters to create a PersistentVolume. Using PersistentVolumeClaims, a user sends the request for dynamic PV creation, which gets wired to the StorageClass resource. Some of the Volume Types that support managing storage using PersistentVolumes are: 1 2 3 4 5 6 7 GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CephFS NFS iSCSI You can learn more details about Volume Types in the Kubernetes documentation .","title":"PersistentVolumes"},{"location":"linux/","text":"Linux \u00b6 Coming soon...","title":"Linux"},{"location":"linux/#linux","text":"Coming soon...","title":"Linux"},{"location":"miscellaneous/","text":"Notes \u00b6 This section is for my own notes, tests and tryouts. Maybe some ideas and drafts.","title":"Notes"},{"location":"miscellaneous/#notes","text":"This section is for my own notes, tests and tryouts. Maybe some ideas and drafts.","title":"Notes"},{"location":"miscellaneous/bookshelf/","text":"Useful Links \u00b6 r/devops Books \u00b6 My current have-to-be-read list \u00b6 The Kubernetes Scheduler Imperative vs. Declarative \u2014 a Kubernetes Tutorial Deploy a Kubernetes Cluster with Cluster Monitoring in Amazon EC2 using Rancher 10 Advanced Tricks with Docker 50 Useful Docker Tutorials, From Beginner to Advanced \u0417\u043d\u0430\u043a\u043e\u043c\u0441\u0442\u0432\u043e \u0441 Kubernetes Security as Standard in the Land of Kubernetes. 10 Data Structure, Algorithms, and Programming Courses to Crack Any Coding Interview CRUD App using Vue.js and Django Comparing Kubernetes CNI Providers: Flannel, Calico, Canal, and Weave","title":"Bookshelf"},{"location":"miscellaneous/bookshelf/#useful-links","text":"r/devops","title":"Useful Links"},{"location":"miscellaneous/bookshelf/#books","text":"","title":"Books"},{"location":"miscellaneous/bookshelf/#my-current-have-to-be-read-list","text":"The Kubernetes Scheduler Imperative vs. Declarative \u2014 a Kubernetes Tutorial Deploy a Kubernetes Cluster with Cluster Monitoring in Amazon EC2 using Rancher 10 Advanced Tricks with Docker 50 Useful Docker Tutorials, From Beginner to Advanced \u0417\u043d\u0430\u043a\u043e\u043c\u0441\u0442\u0432\u043e \u0441 Kubernetes Security as Standard in the Land of Kubernetes. 10 Data Structure, Algorithms, and Programming Courses to Crack Any Coding Interview CRUD App using Vue.js and Django Comparing Kubernetes CNI Providers: Flannel, Calico, Canal, and Weave","title":"My current have-to-be-read list"},{"location":"miscellaneous/makdocs/","text":"Some useful info about mkdocs \u00b6 Documentation links For full documentation visit -> mkdocs.org Docs about Material template -> github.com Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Examples \u00b6 Tables \u00b6 Listener Target Group Purpose Attachment TCP 80 environment -acme-http HTTP LE verification requests The autoscaling group for ECS instances populates this target group on port 80. TCP 443 environment -acme-https SSL offload for pm/owners requests ECS service environment -acme-proxy populates this target group on port 443. Code \u00b6 1 2 def fn (): pass Marked text \u00b6 This is marked line of text inside regular text Highlighting specific lines \u00b6 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Keyboard shortcuts \u00b6 Ctrl + Alt + My Special Key Formula \u00b6 \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} Checklist \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Image as code \u00b6","title":"Makdocs helper"},{"location":"miscellaneous/makdocs/#some-useful-info-about-mkdocs","text":"Documentation links For full documentation visit -> mkdocs.org Docs about Material template -> github.com","title":"Some useful info about mkdocs"},{"location":"miscellaneous/makdocs/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"miscellaneous/makdocs/#project-layout","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"miscellaneous/makdocs/#examples","text":"","title":"Examples"},{"location":"miscellaneous/makdocs/#tables","text":"Listener Target Group Purpose Attachment TCP 80 environment -acme-http HTTP LE verification requests The autoscaling group for ECS instances populates this target group on port 80. TCP 443 environment -acme-https SSL offload for pm/owners requests ECS service environment -acme-proxy populates this target group on port 443.","title":"Tables"},{"location":"miscellaneous/makdocs/#code","text":"1 2 def fn (): pass","title":"Code"},{"location":"miscellaneous/makdocs/#marked-text","text":"This is marked line of text inside regular text","title":"Marked text"},{"location":"miscellaneous/makdocs/#highlighting-specific-lines","text":"1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Highlighting specific lines"},{"location":"miscellaneous/makdocs/#keyboard-shortcuts","text":"Ctrl + Alt + My Special Key","title":"Keyboard shortcuts"},{"location":"miscellaneous/makdocs/#formula","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Formula"},{"location":"miscellaneous/makdocs/#checklist","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Checklist"},{"location":"miscellaneous/makdocs/#image-as-code","text":"","title":"Image as code"}]}