{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Devops manuals, tips and notes \u00b6 Hello and welcome! Here I'm collecting some interesting and relevant manuals and notes about technologies and processes I use in my work or studying now for future projects. All these materials I've collected and created for my personal use only, but you are free to use it and post it anywhere. Thanks for links to original... :) If you have any advices, recommendations, please write comments and I'll answer as soon as possible. My current technologies stack: \u00b6 Automate CI/CD processes with GitlabCI , Github Actions and Jenkins . Containerize all with Docker and orchestrate them with Kubernetes . Infrastructure as a Code with Terraform and Ansible . Service discovery, mesh and secret management with Consul , Vault and Nomad . Message distributing with Apache Kafka and RabbitMQ Setup and support monitoring environment with ELK , Prometheus and Grafana . Infrastructure monitoring and analytics with New Relic . AWS cloud services management: EC2, ECS, Fargate, RDS, CloudWatch, etc. Serverless AWS Lambda with Javascript and Python . Hardware and network maintenances, system scripts and backend API with Python . About all these technologies, but not only, I write or soon will write here something interesting! Stay tuned! These pages are always on developing state. Some texts could be changed or extended. Sorry, but I'm working... :)","title":"Main page"},{"location":"#devops-manuals-tips-and-notes","text":"Hello and welcome! Here I'm collecting some interesting and relevant manuals and notes about technologies and processes I use in my work or studying now for future projects. All these materials I've collected and created for my personal use only, but you are free to use it and post it anywhere. Thanks for links to original... :) If you have any advices, recommendations, please write comments and I'll answer as soon as possible.","title":"Devops manuals, tips and notes"},{"location":"#my-current-technologies-stack","text":"Automate CI/CD processes with GitlabCI , Github Actions and Jenkins . Containerize all with Docker and orchestrate them with Kubernetes . Infrastructure as a Code with Terraform and Ansible . Service discovery, mesh and secret management with Consul , Vault and Nomad . Message distributing with Apache Kafka and RabbitMQ Setup and support monitoring environment with ELK , Prometheus and Grafana . Infrastructure monitoring and analytics with New Relic . AWS cloud services management: EC2, ECS, Fargate, RDS, CloudWatch, etc. Serverless AWS Lambda with Javascript and Python . Hardware and network maintenances, system scripts and backend API with Python . About all these technologies, but not only, I write or soon will write here something interesting! Stay tuned! These pages are always on developing state. Some texts could be changed or extended. Sorry, but I'm working... :)","title":"My current technologies stack:"},{"location":"AWS_services/","text":"Amazon Web Services \u00b6 Coming very soon...","title":"AWS services"},{"location":"AWS_services/#amazon-web-services","text":"Coming very soon...","title":"Amazon Web Services"},{"location":"AWS_services/fargate/","text":"Trapping SIG inside Fargate and ECS containers \u00b6 Wrap any container, which you want to trap signals inside. In my example I use custom SIG handler for prevent Consul Agent sidecar's early shutdown. Timeout can be set through env variable EXIT_TIMEOUT. Dockerfile FROM consul:1.6.2 COPY entrypoint.sh ./ RUN chmod +x entrypoint.sh ENTRYPOINT [ \"./entrypoint.sh\" ] entrypoint.sh #!/bin/sh set -x pid = 0 timeout = 0 if [ ! -z $EXIT_TIMEOUT ] ; then timeout = $EXIT_TIMEOUT fi # Signals-handler sig_handler () { if [ $pid -ne 0 ] ; then echo \" $1 : Waiting $2 sec...\" sleep $2 s kill - $1 \" $pid \" wait \" $pid \" fi exit $(( 128 + $3 )) ; } # Setup handlers for SIGTERM and SIGINT signals. # On callback, kill the last background process, # which is `tail -f /dev/null` and execute the specified handler. # Handler will kill with trapped signal process number from $pid variable. trap 'kill ${!}; sig_handler SIGTERM ${timeout} 15' SIGTERM trap 'kill ${!}; sig_handler SIGINT ${timeout} 2' SIGINT # Run original entrypoint script with received parametrs docker-entrypoint.sh \" $@ \" & pid = \" $! \" # Wait forever while true do tail -f /dev/null & wait ${ ! } done Pay attention! Fargate and ECS containers use different way for stop_timeout definition. Fargate defines it inside json task definition parameter, but ECS container make it through env variable ECS_CONTAINER_STOP_TIMEOUT inside it's container. Also, be careful with these parameters and wrapper trap timeout value. It has to be equal or less than stop_timeout minus real time for container's graceful shutdown. Time-based autoscaling on Fargate \u00b6 Sometimes we want to turn off our staging environments at night to save some money. If your infrastructure use a lot of Fargate containers - you can set there cron scheduler for every task you have. Only have to remember about API Call limits. If you setup many timers to trigger at the same time - they could be throttled. AWS wont explain exact numbers, but after many checks and fails - we've found out that this number is about 100. And AWS don't want to increase it by our request to support team... Maybe we are not big enough for them. :) Set parameters \u00b6 $ export ECS_CLUSTER_NAME ={ YOUR_ECS_CLUSTER_NAME } $ export ECS_SERVICE_NAME ={ YOUR_ECS_SERVICE_NAME } RegisterScalableTarget \u00b6 $ aws application-autoscaling register-scalable-target --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --min-capacity 1 \\ --max-capacity 3 PutScheduledAction \u00b6 $ export SCALE_OUT_ACTION_NAME = fargate-time-based-scale-out # configure scaling out $ aws application-autoscaling put-scheduled-action --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scheduled-action-name ${ SCALE_OUT_ACTION_NAME } \\ --schedule \"cron(50 23 * * ? *)\" \\ # every day at 8:50am JST --scalable-target-action MinCapacity = 3 ,MaxCapacity = 10 $ export SCALE_IN_ACTION_NAME = fargate-time-based-scale-in # configure scaling in $ aws application-autoscaling put-scheduled-action --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scheduled-action-name ${ SCALE_IN_ACTION_NAME } \\ --schedule \"cron(10 9 * * ? *)\" \\ # every day at 6:10pm JST --scalable-target-action MinCapacity = 1 ,MaxCapacity = 1 DeleteScheduledAction \u00b6 $ aws application-autoscaling delete-scheduled-action --service-namespace ecs \\ --scheduled-action-name ${ SCALE_OUT_ACTION_NAME } \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scalable-dimension ecs:service:DesiredCount $ aws application-autoscaling delete-scheduled-action --service-namespace ecs \\ --scheduled-action-name ${ SCALE_IN_ACTION_NAME } \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scalable-dimension ecs:service:DesiredCount DescribeScheduledActions \u00b6 $ aws application-autoscaling describe-scheduled-actions --service-namespace ecs \\ --scheduled-action-names ${ SCALE_IN_ACTION_NAME } ${ SCALE_OUT_ACTION_NAME } See also \u00b6 https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html Terraform example \u00b6 resource \"aws_appautoscaling_target\" \"tgt\" { service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" role_arn = \"${var.role_arn}\" min_capacity = 1 max_capacity = 1 lifecycle = { create_before_destroy = true } } // Night OFF ( capacity 0 ) scheduler at 21 : 00 UTC resource \"aws_appautoscaling_scheduled_action\" \"night_off\" { name = \"${var.service}-night-off-timer\" service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" schedule = \"cron(0 21 * * ? *)\" scalable_target_action { min_capacity = 0 max_capacity = 0 } depends_on = [ \"aws_appautoscaling_target.tgt\" ] } // Day ON scheduler ( capacity 1 ) at 5 : 00 UTC resource \"aws_appautoscaling_scheduled_action\" \"day_on\" { name = \"${var.service}-day-on-timer\" service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" schedule = \"cron(0 5 * * ? *)\" scalable_target_action { min_capacity = 1 max_capacity = 1 } depends_on = [ \"aws_appautoscaling_target.tgt\" ] }","title":"ECS Fargate"},{"location":"AWS_services/fargate/#trapping-sig-inside-fargate-and-ecs-containers","text":"Wrap any container, which you want to trap signals inside. In my example I use custom SIG handler for prevent Consul Agent sidecar's early shutdown. Timeout can be set through env variable EXIT_TIMEOUT. Dockerfile FROM consul:1.6.2 COPY entrypoint.sh ./ RUN chmod +x entrypoint.sh ENTRYPOINT [ \"./entrypoint.sh\" ] entrypoint.sh #!/bin/sh set -x pid = 0 timeout = 0 if [ ! -z $EXIT_TIMEOUT ] ; then timeout = $EXIT_TIMEOUT fi # Signals-handler sig_handler () { if [ $pid -ne 0 ] ; then echo \" $1 : Waiting $2 sec...\" sleep $2 s kill - $1 \" $pid \" wait \" $pid \" fi exit $(( 128 + $3 )) ; } # Setup handlers for SIGTERM and SIGINT signals. # On callback, kill the last background process, # which is `tail -f /dev/null` and execute the specified handler. # Handler will kill with trapped signal process number from $pid variable. trap 'kill ${!}; sig_handler SIGTERM ${timeout} 15' SIGTERM trap 'kill ${!}; sig_handler SIGINT ${timeout} 2' SIGINT # Run original entrypoint script with received parametrs docker-entrypoint.sh \" $@ \" & pid = \" $! \" # Wait forever while true do tail -f /dev/null & wait ${ ! } done Pay attention! Fargate and ECS containers use different way for stop_timeout definition. Fargate defines it inside json task definition parameter, but ECS container make it through env variable ECS_CONTAINER_STOP_TIMEOUT inside it's container. Also, be careful with these parameters and wrapper trap timeout value. It has to be equal or less than stop_timeout minus real time for container's graceful shutdown.","title":"Trapping SIG inside Fargate and ECS containers"},{"location":"AWS_services/fargate/#time-based-autoscaling-on-fargate","text":"Sometimes we want to turn off our staging environments at night to save some money. If your infrastructure use a lot of Fargate containers - you can set there cron scheduler for every task you have. Only have to remember about API Call limits. If you setup many timers to trigger at the same time - they could be throttled. AWS wont explain exact numbers, but after many checks and fails - we've found out that this number is about 100. And AWS don't want to increase it by our request to support team... Maybe we are not big enough for them. :)","title":"Time-based autoscaling on Fargate"},{"location":"AWS_services/fargate/#set-parameters","text":"$ export ECS_CLUSTER_NAME ={ YOUR_ECS_CLUSTER_NAME } $ export ECS_SERVICE_NAME ={ YOUR_ECS_SERVICE_NAME }","title":"Set parameters"},{"location":"AWS_services/fargate/#registerscalabletarget","text":"$ aws application-autoscaling register-scalable-target --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --min-capacity 1 \\ --max-capacity 3","title":"RegisterScalableTarget"},{"location":"AWS_services/fargate/#putscheduledaction","text":"$ export SCALE_OUT_ACTION_NAME = fargate-time-based-scale-out # configure scaling out $ aws application-autoscaling put-scheduled-action --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scheduled-action-name ${ SCALE_OUT_ACTION_NAME } \\ --schedule \"cron(50 23 * * ? *)\" \\ # every day at 8:50am JST --scalable-target-action MinCapacity = 3 ,MaxCapacity = 10 $ export SCALE_IN_ACTION_NAME = fargate-time-based-scale-in # configure scaling in $ aws application-autoscaling put-scheduled-action --service-namespace ecs \\ --scalable-dimension ecs:service:DesiredCount \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scheduled-action-name ${ SCALE_IN_ACTION_NAME } \\ --schedule \"cron(10 9 * * ? *)\" \\ # every day at 6:10pm JST --scalable-target-action MinCapacity = 1 ,MaxCapacity = 1","title":"PutScheduledAction"},{"location":"AWS_services/fargate/#deletescheduledaction","text":"$ aws application-autoscaling delete-scheduled-action --service-namespace ecs \\ --scheduled-action-name ${ SCALE_OUT_ACTION_NAME } \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scalable-dimension ecs:service:DesiredCount $ aws application-autoscaling delete-scheduled-action --service-namespace ecs \\ --scheduled-action-name ${ SCALE_IN_ACTION_NAME } \\ --resource-id service/ ${ ECS_CLUSTER_NAME } / ${ ECS_SERVICE_NAME } \\ --scalable-dimension ecs:service:DesiredCount","title":"DeleteScheduledAction"},{"location":"AWS_services/fargate/#describescheduledactions","text":"$ aws application-autoscaling describe-scheduled-actions --service-namespace ecs \\ --scheduled-action-names ${ SCALE_IN_ACTION_NAME } ${ SCALE_OUT_ACTION_NAME }","title":"DescribeScheduledActions"},{"location":"AWS_services/fargate/#see-also","text":"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html","title":"See also"},{"location":"AWS_services/fargate/#terraform-example","text":"resource \"aws_appautoscaling_target\" \"tgt\" { service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" role_arn = \"${var.role_arn}\" min_capacity = 1 max_capacity = 1 lifecycle = { create_before_destroy = true } } // Night OFF ( capacity 0 ) scheduler at 21 : 00 UTC resource \"aws_appautoscaling_scheduled_action\" \"night_off\" { name = \"${var.service}-night-off-timer\" service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" schedule = \"cron(0 21 * * ? *)\" scalable_target_action { min_capacity = 0 max_capacity = 0 } depends_on = [ \"aws_appautoscaling_target.tgt\" ] } // Day ON scheduler ( capacity 1 ) at 5 : 00 UTC resource \"aws_appautoscaling_scheduled_action\" \"day_on\" { name = \"${var.service}-day-on-timer\" service_namespace = \"ecs\" resource_id = \"service/${var.cluster}/${var.service}\" scalable_dimension = \"ecs:service:DesiredCount\" schedule = \"cron(0 5 * * ? *)\" scalable_target_action { min_capacity = 1 max_capacity = 1 } depends_on = [ \"aws_appautoscaling_target.tgt\" ] }","title":"Terraform example"},{"location":"AWS_services/useful-tips/","text":"Useful tips for AWS Services \u00b6 AWS IP Address Ranges \u00b6 Amazon Web Services (AWS) publishes its current IP address ranges in JSON format. To view the current ranges, download the .json file. To maintain history, save successive versions of the .json file on your system. To determine whether there have been changes since the last time that you saved the file, check the publication time in the current file and compare it to the publication time in the last file that you saved. #!/usr/bin/env python import requests ip_ranges = requests . get ( 'https://ip-ranges.amazonaws.com/ip-ranges.json' ) . json ()[ 'prefixes' ] amazon_ips = [ item [ 'ip_prefix' ] for item in ip_ranges if item [ \"service\" ] == \"AMAZON\" ] ec2_ips = [ item [ 'ip_prefix' ] for item in ip_ranges if item [ \"service\" ] == \"EC2\" ] amazon_ips_less_ec2 = [] for ip in amazon_ips : if ip not in ec2_ips : amazon_ips_less_ec2 . append ( ip ) for ip in amazon_ips_less_ec2 : print ( str ( ip ))","title":"Useful Tips"},{"location":"AWS_services/useful-tips/#useful-tips-for-aws-services","text":"","title":"Useful tips for AWS Services"},{"location":"AWS_services/useful-tips/#aws-ip-address-ranges","text":"Amazon Web Services (AWS) publishes its current IP address ranges in JSON format. To view the current ranges, download the .json file. To maintain history, save successive versions of the .json file on your system. To determine whether there have been changes since the last time that you saved the file, check the publication time in the current file and compare it to the publication time in the last file that you saved. #!/usr/bin/env python import requests ip_ranges = requests . get ( 'https://ip-ranges.amazonaws.com/ip-ranges.json' ) . json ()[ 'prefixes' ] amazon_ips = [ item [ 'ip_prefix' ] for item in ip_ranges if item [ \"service\" ] == \"AMAZON\" ] ec2_ips = [ item [ 'ip_prefix' ] for item in ip_ranges if item [ \"service\" ] == \"EC2\" ] amazon_ips_less_ec2 = [] for ip in amazon_ips : if ip not in ec2_ips : amazon_ips_less_ec2 . append ( ip ) for ip in amazon_ips_less_ec2 : print ( str ( ip ))","title":"AWS IP Address Ranges"},{"location":"dev-tools/git/","text":"Git tips and tricks \u00b6 Sometimes you have to make very rare manipulations with your git repo, but can't remember how exactly it works. Here I'm collecting useful git use cases. Rebase previous N to one \u00b6 If you need to unite last N commits on current brunch run this rebase command with your number of last commits git rebase -i HEAD~4 After it opens a rebase file, check here the last commit with \"p\" letter and other ones with \"s\" letter. Save it. And save next opened file with new commit message file too. These steps will do single commit with all changes from this N commits. It could be useful if you don't want to show in remote repo all the history of your commits on the branch. Or maybe want to hide some of them. Undoing last commit \u00b6 git reset --soft HEAD~1 If you don't want to keep these changes, simply use the --hard flag. Be sure to only do this when you're sure you don't need these changes anymore. git reset --hard HEAD~1 Show diff between branches \u00b6 This shows only the changes between current local branch and the remote master branch, and ignores any changes in the local branch that came from merge commits. git diff origin/master... Replace master with better branch \u00b6 git checkout master git reset --hard better_branch git push -f origin master Merge master to current branch \u00b6 Merge all changes from master to your current branch: git pull origin master and resolve conflicts if needed. Merge with squash \u00b6 Merge from branch with squash will erase all commits history in the branch # Work in feature/1 branch git branch \"feature/1\" git checkout \"feature/1\" # Move to the master branch. git checkout master # Merge the \"feature/1\" branch and squash the commits. git merge --squash feature/1 # Resolve conflicts # Commit your changes and add a single commit message for all your commits. # You can omit the \"-m\" to have a template popping up based on your previous commit messages. git commit -m \"Feature 1 : 1, 2 et 3\" # Delete the \"feature/1\" branch that is no longer needed. git branch -D feature/1 Reset \u00b6 Soft \u00b6 You've done a few tiny commits and want them all be put into one commit: A -> B -> C -> D || A with ALL (B,C,D) -> E git reset --soft A git commit -m \u201cmy new merged commit\u201d git push origin branch New commit E contains all of the files that were committed in B, C, D. Mixed \u00b6 You\u2019ve just pushed a few commits, but you want to go back and remove a couple of files in a previous commit. A -> B-> C -> D || A with ANY (B,C,D) -> E git reset --mixed A Your branch head and index is pointing at A and all of your changes in B, C and D are there, but are untracked. Now you are free to add the files that you wish to add into a new commit. git add <files> git commit -m \"updated commit\" git push origin branch The head is now at the new commit E, and any files that you\u2019ve not staged will still be in your working tree, ready to add into another commit or to do what you want with. Hard \u00b6 Do it only if you want to go back a few commits and get rid of every change you\u2019ve made since. git reset --hard A git push origin branchname --force This will delete your commits from the remote branch history. Add submodule \u00b6 git submodule add <link_to_git_repo> <link_to_submodule_folder> git submodule init git submodule update Edit last commit message \u00b6 1 git commit --amend Add missed file to last commit \u00b6 1 2 git add missed-file.txt git commit --amend Move last commit from master to branch \u00b6 1 2 3 git branch branch-name git reset HEAD~ --hard git checkout branch-name Clean up local commits before pushing \u00b6 git rebase --interactive if you didn't specify any tracking information for this branch you will have to add upstream and remote branch information: git rebase --interactive origin branch","title":"Git Tricks"},{"location":"dev-tools/git/#git-tips-and-tricks","text":"Sometimes you have to make very rare manipulations with your git repo, but can't remember how exactly it works. Here I'm collecting useful git use cases.","title":"Git tips and tricks"},{"location":"dev-tools/git/#rebase-previous-n-to-one","text":"If you need to unite last N commits on current brunch run this rebase command with your number of last commits git rebase -i HEAD~4 After it opens a rebase file, check here the last commit with \"p\" letter and other ones with \"s\" letter. Save it. And save next opened file with new commit message file too. These steps will do single commit with all changes from this N commits. It could be useful if you don't want to show in remote repo all the history of your commits on the branch. Or maybe want to hide some of them.","title":"Rebase previous N to one"},{"location":"dev-tools/git/#undoing-last-commit","text":"git reset --soft HEAD~1 If you don't want to keep these changes, simply use the --hard flag. Be sure to only do this when you're sure you don't need these changes anymore. git reset --hard HEAD~1","title":"Undoing last commit"},{"location":"dev-tools/git/#show-diff-between-branches","text":"This shows only the changes between current local branch and the remote master branch, and ignores any changes in the local branch that came from merge commits. git diff origin/master...","title":"Show diff between branches"},{"location":"dev-tools/git/#replace-master-with-better-branch","text":"git checkout master git reset --hard better_branch git push -f origin master","title":"Replace master with better branch"},{"location":"dev-tools/git/#merge-master-to-current-branch","text":"Merge all changes from master to your current branch: git pull origin master and resolve conflicts if needed.","title":"Merge master to current branch"},{"location":"dev-tools/git/#merge-with-squash","text":"Merge from branch with squash will erase all commits history in the branch # Work in feature/1 branch git branch \"feature/1\" git checkout \"feature/1\" # Move to the master branch. git checkout master # Merge the \"feature/1\" branch and squash the commits. git merge --squash feature/1 # Resolve conflicts # Commit your changes and add a single commit message for all your commits. # You can omit the \"-m\" to have a template popping up based on your previous commit messages. git commit -m \"Feature 1 : 1, 2 et 3\" # Delete the \"feature/1\" branch that is no longer needed. git branch -D feature/1","title":"Merge with squash"},{"location":"dev-tools/git/#reset","text":"","title":"Reset"},{"location":"dev-tools/git/#soft","text":"You've done a few tiny commits and want them all be put into one commit: A -> B -> C -> D || A with ALL (B,C,D) -> E git reset --soft A git commit -m \u201cmy new merged commit\u201d git push origin branch New commit E contains all of the files that were committed in B, C, D.","title":"Soft"},{"location":"dev-tools/git/#mixed","text":"You\u2019ve just pushed a few commits, but you want to go back and remove a couple of files in a previous commit. A -> B-> C -> D || A with ANY (B,C,D) -> E git reset --mixed A Your branch head and index is pointing at A and all of your changes in B, C and D are there, but are untracked. Now you are free to add the files that you wish to add into a new commit. git add <files> git commit -m \"updated commit\" git push origin branch The head is now at the new commit E, and any files that you\u2019ve not staged will still be in your working tree, ready to add into another commit or to do what you want with.","title":"Mixed"},{"location":"dev-tools/git/#hard","text":"Do it only if you want to go back a few commits and get rid of every change you\u2019ve made since. git reset --hard A git push origin branchname --force This will delete your commits from the remote branch history.","title":"Hard"},{"location":"dev-tools/git/#add-submodule","text":"git submodule add <link_to_git_repo> <link_to_submodule_folder> git submodule init git submodule update","title":"Add submodule"},{"location":"dev-tools/git/#edit-last-commit-message","text":"1 git commit --amend","title":"Edit last commit message"},{"location":"dev-tools/git/#add-missed-file-to-last-commit","text":"1 2 git add missed-file.txt git commit --amend","title":"Add missed file to last commit"},{"location":"dev-tools/git/#move-last-commit-from-master-to-branch","text":"1 2 3 git branch branch-name git reset HEAD~ --hard git checkout branch-name","title":"Move last commit from master to branch"},{"location":"dev-tools/git/#clean-up-local-commits-before-pushing","text":"git rebase --interactive if you didn't specify any tracking information for this branch you will have to add upstream and remote branch information: git rebase --interactive origin branch","title":"Clean up local commits before pushing"},{"location":"dev-tools/javascript/","text":"","title":"Javascript"},{"location":"dev-tools/jenkins/","text":"Jenkins tricks \u00b6 Kill stacked job \u00b6 Sometimes Jenkins job could stack without response, and can't be killed from the UI. Go to Manage Jenkins -> Script Console and paste there one of these code-blocks. Change job name and build number to relevant ones and run it. Jenkins . instance . getItemByFullName ( \"CI/rc-preprod\" ) . getBuildByNumber ( 895 ) . finish ( hudson . model . Result . ABORTED , new java . io . IOException ( \"Aborting build\" ) ); or def jobname = \"CI/rc-preprod\" def buildnum = 895 def job = Jenkins . instance . getItemByFullName ( jobname ) for ( build in job . builds ) { if ( buildnum == build . getNumber (). toInteger ()){ if ( build . isBuilding ()){ build . doStop (); build . doKill (); } } }","title":"Jenkins"},{"location":"dev-tools/jenkins/#jenkins-tricks","text":"","title":"Jenkins tricks"},{"location":"dev-tools/jenkins/#kill-stacked-job","text":"Sometimes Jenkins job could stack without response, and can't be killed from the UI. Go to Manage Jenkins -> Script Console and paste there one of these code-blocks. Change job name and build number to relevant ones and run it. Jenkins . instance . getItemByFullName ( \"CI/rc-preprod\" ) . getBuildByNumber ( 895 ) . finish ( hudson . model . Result . ABORTED , new java . io . IOException ( \"Aborting build\" ) ); or def jobname = \"CI/rc-preprod\" def buildnum = 895 def job = Jenkins . instance . getItemByFullName ( jobname ) for ( build in job . builds ) { if ( buildnum == build . getNumber (). toInteger ()){ if ( build . isBuilding ()){ build . doStop (); build . doKill (); } } }","title":"Kill stacked job"},{"location":"dev-tools/mongo/","text":"","title":"MongoDB"},{"location":"dev-tools/nginx/","text":"NGINX proxy pitfalls related with DNS resolving \u00b6 If you're using proxy_pass and your endpoint's IPs can vary in time, please read this article to avoid misunderstandings about how nginx works. TL;DR \u00b6 If you want to force nginx resolve your endpoints, you should: Use variables within proxy_pass directive, e.g. proxy_pass https://$endpoint/; , where $endpoint can be manually set or extracted from location regex. Make sure that your endpoint isn't used on another locations w/o variables, because in this case resolving won't work. To fix this move endpoint domain to the upstream or use variables in the proxy_pass in all locations to make resolving works. You can have both resolve and non-resolve locations for same domain When a variable is used in proxy_pass directive, the location header is not longer adjusted. To get around this, simply set proxy_redirect Explanatory example \u00b6 location /api/ { proxy_pass http://api.com/ ; } In this case nginx will resolve api.com only once at startup (or reload). But there are some cases when your endpoint can be resolved to any IP, e.g. if you're using load balancer which doing magic failover via DNS mapping. If api.com will point to another IP your proxying will fail. Finding the solution \u00b6 Add a resolver directive \u00b6 You can check official nginx documentation and find there resolver directive description: location /api/ { resolver 8 .8.8.8 ; proxy_pass https://api.com/ ; } No, it will not work. Even this will not work: location /api/ { resolver 8 .8.8.8 valid=1s ; proxy_pass https://api.com/ ; } It's because of nginx doesn't respect resolver directive in this case. It will resolve api.com only at startup (or reload) by system resolver (/etc/resolv.conf), even if real TTL of A/AAAA record api.com is 1s. Add variables \u00b6 You can google a bit and find that nginx try to resolve proxy endpoint with variables . Also official documentation for proxy_pass directive notices this too . Ok, I think this should be noticed in the resolver description, but let's try anyway: location = /proxy/ { set $endpoint proxy.com ; resolver 8 .8.8.8 valid=10s ; proxy_pass https:// $endpoint/ ; } Works as expected, nginx will query proxy.com every 10s on particular requests. These configurations works too: set $endpoint api.com ; location ~ ^/api/(.*)$ { resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } location ~ ^/(?<dest_proxy>[\\w-]+)(?:/(?<path_proxy>.*))? { resolver 8 .8.8.8 ipv6=off valid=60s ; proxy_pass https:// ${dest_proxy}.example.com/${path_proxy}$is_args$args ; } Notice that nginx will start even without resolver directive, but will fail with 502 at runtime, because \"no resolver defined to resolve\". Caveats \u00b6 location = /api_version/ { proxy_pass https://api.com/version/ ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } In this case nginx will resolve api.com once at startup with system resolver and then will never do re-resolve even for /api/ requests. Example with /api_version/ is just synthetic example, you can use more complex scenarios with headers set, etc. Use variables everywhere to make it work as expected: location = /api_version/ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/version/ ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } You can move set and resolver to the server or http (or use include ) directives to avoid copy-paste (also I assume that it will increase performance a bit, but I haven't tested it). If response from proxy contains Location header, as in the case of a redirect, nginx will automatically replace these values as needed. However, if variables are used in proxy_pass , this must be done explicitly via proxy_redirect : location = /api_version/ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/version/ ; proxy_redirect https:// $endpoint/ / ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; proxy_redirect https:// $endpoint/ / ; } Single line in nginx docs that mention it: The default parameter is not permitted if proxy_pass is specified using variables. Upstreams \u00b6 If you're using nginx plus, you can use resolve parameter, check out documentation . I assume that it will be efficient, because documentation says \"monitors changes of the IP addresses that correspond to a domain name of the server\", while solutions listed above will query DNS on the particular requests. But if you're using open source nginx, no honey is available for you. No money \u2014 no honey. Two in one \u00b6 You can have both resolve and non-resolve locations for same domain upstream proxy { server proxy.com : 443 ; } server { listen 80 ; server_name fillo.me ; location = /proxy-with-resolve/ { set $endpoint proxy.com ; resolver 8 .8.8.8 valid=1s ; proxy_pass https:// $endpoint/ ; } location = /proxy-without-resolve/ { proxy_pass https://proxy/ ; proxy_set_header Host proxy.com ; } } Yes, http://fillo.me/proxy-with-resolve/ will resolve proxy.com every 1s on particular requests, while http://fillo.me/proxy-without-resolve/ will not resolve proxy.com (nginx will resolve proxy.com at startup/reload once). This magic works because upstream directive is used. Another example: upstream api_version { server version.api.com : 443 ; } server { listen 80 ; server_name fillo.me ; location = /api_version/ { proxy_pass https://api_version/version/ ; proxy_set_header Host version.api.com ; } location ~ ^/api/(?<dest_proxy>[\\w-]+)(?:/(?<path_proxy>.*))? { resolver 8 .8.8.8 valid=60s ; proxy_pass https:// ${dest_proxy}.api.com/${path_proxy}$is_args$args ; } } * If you will open http://fillo.me/api_version/ then no resolve will be done, because of nginx resolved version.api.com at startup. * If you will open http://fillo.me/api/version/version/ then it will work as expected, nginx will resolve version.api.com every 60s on particular request. * If you will open http://fillo.me/api/checkout/items/ then it will work as expected, nginx will resolve checkout.api.com every 60s on particular request. Tested on \u00b6 1.9.6 1.10.1 Although I think it works for many other versions. Further research \u00b6 This issue says that changing HTTPS to the HTTP helps. Check how protocol changes affects examples above. Compare performance with and without resolving. Compare performance with different variables scopes. How to force upstream resolving.","title":"Nginx"},{"location":"dev-tools/nginx/#nginx-proxy-pitfalls-related-with-dns-resolving","text":"If you're using proxy_pass and your endpoint's IPs can vary in time, please read this article to avoid misunderstandings about how nginx works.","title":"NGINX proxy pitfalls related with DNS resolving"},{"location":"dev-tools/nginx/#tldr","text":"If you want to force nginx resolve your endpoints, you should: Use variables within proxy_pass directive, e.g. proxy_pass https://$endpoint/; , where $endpoint can be manually set or extracted from location regex. Make sure that your endpoint isn't used on another locations w/o variables, because in this case resolving won't work. To fix this move endpoint domain to the upstream or use variables in the proxy_pass in all locations to make resolving works. You can have both resolve and non-resolve locations for same domain When a variable is used in proxy_pass directive, the location header is not longer adjusted. To get around this, simply set proxy_redirect","title":"TL;DR"},{"location":"dev-tools/nginx/#explanatory-example","text":"location /api/ { proxy_pass http://api.com/ ; } In this case nginx will resolve api.com only once at startup (or reload). But there are some cases when your endpoint can be resolved to any IP, e.g. if you're using load balancer which doing magic failover via DNS mapping. If api.com will point to another IP your proxying will fail.","title":"Explanatory example"},{"location":"dev-tools/nginx/#finding-the-solution","text":"","title":"Finding the solution"},{"location":"dev-tools/nginx/#add-a-resolver-directive","text":"You can check official nginx documentation and find there resolver directive description: location /api/ { resolver 8 .8.8.8 ; proxy_pass https://api.com/ ; } No, it will not work. Even this will not work: location /api/ { resolver 8 .8.8.8 valid=1s ; proxy_pass https://api.com/ ; } It's because of nginx doesn't respect resolver directive in this case. It will resolve api.com only at startup (or reload) by system resolver (/etc/resolv.conf), even if real TTL of A/AAAA record api.com is 1s.","title":"Add a resolver directive"},{"location":"dev-tools/nginx/#add-variables","text":"You can google a bit and find that nginx try to resolve proxy endpoint with variables . Also official documentation for proxy_pass directive notices this too . Ok, I think this should be noticed in the resolver description, but let's try anyway: location = /proxy/ { set $endpoint proxy.com ; resolver 8 .8.8.8 valid=10s ; proxy_pass https:// $endpoint/ ; } Works as expected, nginx will query proxy.com every 10s on particular requests. These configurations works too: set $endpoint api.com ; location ~ ^/api/(.*)$ { resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } location ~ ^/(?<dest_proxy>[\\w-]+)(?:/(?<path_proxy>.*))? { resolver 8 .8.8.8 ipv6=off valid=60s ; proxy_pass https:// ${dest_proxy}.example.com/${path_proxy}$is_args$args ; } Notice that nginx will start even without resolver directive, but will fail with 502 at runtime, because \"no resolver defined to resolve\".","title":"Add variables"},{"location":"dev-tools/nginx/#caveats","text":"location = /api_version/ { proxy_pass https://api.com/version/ ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } In this case nginx will resolve api.com once at startup with system resolver and then will never do re-resolve even for /api/ requests. Example with /api_version/ is just synthetic example, you can use more complex scenarios with headers set, etc. Use variables everywhere to make it work as expected: location = /api_version/ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/version/ ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; } You can move set and resolver to the server or http (or use include ) directives to avoid copy-paste (also I assume that it will increase performance a bit, but I haven't tested it). If response from proxy contains Location header, as in the case of a redirect, nginx will automatically replace these values as needed. However, if variables are used in proxy_pass , this must be done explicitly via proxy_redirect : location = /api_version/ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/version/ ; proxy_redirect https:// $endpoint/ / ; } location ~ ^/api/(.*)$ { set $endpoint api.com ; resolver 8 .8.8.8 valid=60s ; proxy_pass https:// $endpoint/$1$is_args$args ; proxy_redirect https:// $endpoint/ / ; } Single line in nginx docs that mention it: The default parameter is not permitted if proxy_pass is specified using variables.","title":"Caveats"},{"location":"dev-tools/nginx/#upstreams","text":"If you're using nginx plus, you can use resolve parameter, check out documentation . I assume that it will be efficient, because documentation says \"monitors changes of the IP addresses that correspond to a domain name of the server\", while solutions listed above will query DNS on the particular requests. But if you're using open source nginx, no honey is available for you. No money \u2014 no honey.","title":"Upstreams"},{"location":"dev-tools/nginx/#two-in-one","text":"You can have both resolve and non-resolve locations for same domain upstream proxy { server proxy.com : 443 ; } server { listen 80 ; server_name fillo.me ; location = /proxy-with-resolve/ { set $endpoint proxy.com ; resolver 8 .8.8.8 valid=1s ; proxy_pass https:// $endpoint/ ; } location = /proxy-without-resolve/ { proxy_pass https://proxy/ ; proxy_set_header Host proxy.com ; } } Yes, http://fillo.me/proxy-with-resolve/ will resolve proxy.com every 1s on particular requests, while http://fillo.me/proxy-without-resolve/ will not resolve proxy.com (nginx will resolve proxy.com at startup/reload once). This magic works because upstream directive is used. Another example: upstream api_version { server version.api.com : 443 ; } server { listen 80 ; server_name fillo.me ; location = /api_version/ { proxy_pass https://api_version/version/ ; proxy_set_header Host version.api.com ; } location ~ ^/api/(?<dest_proxy>[\\w-]+)(?:/(?<path_proxy>.*))? { resolver 8 .8.8.8 valid=60s ; proxy_pass https:// ${dest_proxy}.api.com/${path_proxy}$is_args$args ; } } * If you will open http://fillo.me/api_version/ then no resolve will be done, because of nginx resolved version.api.com at startup. * If you will open http://fillo.me/api/version/version/ then it will work as expected, nginx will resolve version.api.com every 60s on particular request. * If you will open http://fillo.me/api/checkout/items/ then it will work as expected, nginx will resolve checkout.api.com every 60s on particular request.","title":"Two in one"},{"location":"dev-tools/nginx/#tested-on","text":"1.9.6 1.10.1 Although I think it works for many other versions.","title":"Tested on"},{"location":"dev-tools/nginx/#further-research","text":"This issue says that changing HTTPS to the HTTP helps. Check how protocol changes affects examples above. Compare performance with and without resolving. Compare performance with different variables scopes. How to force upstream resolving.","title":"Further research"},{"location":"dev-tools/one-liners/","text":"Replase text in template \u00b6 Replase TEMPLATE_TEXT with $BUILD_NUMBER var inside filename.json.template and save it into filename.json sed \"s/TEMPLATE_TEXT/1. $BUILD_NUMBER /g\" \\ filename.json.template > filename.json","title":"Bash oneliners"},{"location":"dev-tools/one-liners/#replase-text-in-template","text":"Replase TEMPLATE_TEXT with $BUILD_NUMBER var inside filename.json.template and save it into filename.json sed \"s/TEMPLATE_TEXT/1. $BUILD_NUMBER /g\" \\ filename.json.template > filename.json","title":"Replase text in template"},{"location":"dev-tools/python/","text":"Python3 on MacOS in a right way \u00b6 Install pyenv brew install pyenv pyenv install 3 .8.0 pyenv global 3 .8.0 Add the following to your configuration file ( .zshrc for me, possibly .bash_profile for you): export PATH = \"/Users/denis/.pyenv/shims: ${ PATH } \" export PYENV_SHELL = zsh source '/usr/local/Cellar/pyenv/1.2.15/libexec/../completions/pyenv.zsh' command pyenv rehash 2 >/dev/null pyenv () { local command command = \" ${ 1 :- } \" if [ \" $# \" -gt 0 ] ; then shift fi case \" $command \" in rehash | shell ) eval \" $( pyenv \"sh- $command \" \" $@ \" ) \" ;; * ) command pyenv \" $command \" \" $@ \" ;; esac } Now we know for certain that we're using Python 3.8.0 and pip will update alongside it without any manual aliasing between versions. Lambda, map, reduce, filter \u00b6 # Lambda foo = lambda n : n ** 2 if n % 2 == 0 else n ** 3 foo ( 5 ) # Output: 125 # Map col = [ 6 , 10 , 12 , 15 , 19 ] mapped = map ( lambda x : x * 10 , col ) # mapped = [60, 100, 120, 150, 190] # Filter filtered = filter ( lambda x : x % 2 == 0 and x > 7 , col ) # filtered = [10, 12] # Reduce from functools import reduce reduced = reduce ( lambda x , y : x * y + 10 , col ) # reduced = 242450 globals() locals() \u00b6 1 2 3 4 type(<object>) -> <ClassName> isinstance(<object>, <ClassName>) -> Boolean globals() -> dict with global vars locals() -> dict with local vars Context managers \u00b6 with open ( \"example.txt\" , \"w\" ) as file : # logic Two special methods: enter () and after () . They take care of what happens when execution enters and exits your with block respectively.. from datetime import datetime class DataManager (): def __init__ ( self ): self . file = None def __enter__ ( self ): now = str ( datetime . now ()) . split ( \".\" )[ 0 ] . replece ( \" \" , \"-\" ) . replace . ( \":\" , \"-\" ) filename = now + \"-DATA.txt\" self . file = open ( filename , \"w\" ) return self . file def __exit__ ( self , exc_type , exc_value , exc_traceback ): self . file . close () print ( \"File closed\" ) with DataManager () as data : data . write ( \"hello!\" ) Decorators \u00b6 def is_positive ( func ): def wrapper ( * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return func ( * args , ** kwargs ) return wrapper @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2 Same decorations with class and special method call (): class is_positive (): def __init__ ( self , func ): self . function = func def __call__ ( self , * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return self . function ( * args , ** kwargs ) @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2 Generators \u00b6 def squares ( a , b ): i = a while i < b : y yield i ** 2 i += 1 s = squares ( 5 , 10 ) next ( s ) urllib module for web requests \u00b6 You can do a lot of neat stuff with it e.g. access API resources, make different types of HTTP requests like GET, POST, PUT, DELETE etc. Simple request to google.com to see what we get. import urllib.request with urllib . request . urlopen ( \"https://www.google.com/\" ) as url : print ( url . read ( 300 ) . decode ( \"utf-8\" )) GET request \u00b6 import urllib.request req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' ) with urllib . request . urlopen ( req ) as resp : print ( resp . read () . decode ( 'utf-8' )) POST request \u00b6 import json import urllib.request # This is our Todo data = { \"userId\" : 101 , \"id\" : 100 , \"title\" : \"This is a POST request\" , \"completed\" : True } # Dump the todo object as a json string data = json . dumps ( data ) req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/' , data = bytes ( data . encode ( \"utf-8\" )), method = \"POST\" ) # Add the appropriate header. req . add_header ( \"Content-type\" , \"application/json; charset=UTF-8\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data ) PUT request \u00b6 import json import urllib.request # This is our Todo data = { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"This is a PUT request\" , \"completed\" : False } # Dump the todo object as a json string data = json . dumps ( data ) req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' , data = bytes ( data . encode ( \"utf-8\" )), method = \"PUT\" ) # Add the appropriate header. req . add_header ( \"Content-type\" , \"application/json; charset=UTF-8\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data ) DELETE request \u00b6 import json import urllib.request req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' , method = \"DELETE\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data )","title":"Python"},{"location":"dev-tools/python/#python3-on-macos-in-a-right-way","text":"Install pyenv brew install pyenv pyenv install 3 .8.0 pyenv global 3 .8.0 Add the following to your configuration file ( .zshrc for me, possibly .bash_profile for you): export PATH = \"/Users/denis/.pyenv/shims: ${ PATH } \" export PYENV_SHELL = zsh source '/usr/local/Cellar/pyenv/1.2.15/libexec/../completions/pyenv.zsh' command pyenv rehash 2 >/dev/null pyenv () { local command command = \" ${ 1 :- } \" if [ \" $# \" -gt 0 ] ; then shift fi case \" $command \" in rehash | shell ) eval \" $( pyenv \"sh- $command \" \" $@ \" ) \" ;; * ) command pyenv \" $command \" \" $@ \" ;; esac } Now we know for certain that we're using Python 3.8.0 and pip will update alongside it without any manual aliasing between versions.","title":"Python3 on MacOS in a right way"},{"location":"dev-tools/python/#lambda-map-reduce-filter","text":"# Lambda foo = lambda n : n ** 2 if n % 2 == 0 else n ** 3 foo ( 5 ) # Output: 125 # Map col = [ 6 , 10 , 12 , 15 , 19 ] mapped = map ( lambda x : x * 10 , col ) # mapped = [60, 100, 120, 150, 190] # Filter filtered = filter ( lambda x : x % 2 == 0 and x > 7 , col ) # filtered = [10, 12] # Reduce from functools import reduce reduced = reduce ( lambda x , y : x * y + 10 , col ) # reduced = 242450","title":"Lambda, map, reduce, filter"},{"location":"dev-tools/python/#globals-locals","text":"1 2 3 4 type(<object>) -> <ClassName> isinstance(<object>, <ClassName>) -> Boolean globals() -> dict with global vars locals() -> dict with local vars","title":"globals() locals()"},{"location":"dev-tools/python/#context-managers","text":"with open ( \"example.txt\" , \"w\" ) as file : # logic Two special methods: enter () and after () . They take care of what happens when execution enters and exits your with block respectively.. from datetime import datetime class DataManager (): def __init__ ( self ): self . file = None def __enter__ ( self ): now = str ( datetime . now ()) . split ( \".\" )[ 0 ] . replece ( \" \" , \"-\" ) . replace . ( \":\" , \"-\" ) filename = now + \"-DATA.txt\" self . file = open ( filename , \"w\" ) return self . file def __exit__ ( self , exc_type , exc_value , exc_traceback ): self . file . close () print ( \"File closed\" ) with DataManager () as data : data . write ( \"hello!\" )","title":"Context managers"},{"location":"dev-tools/python/#decorators","text":"def is_positive ( func ): def wrapper ( * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return func ( * args , ** kwargs ) return wrapper @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2 Same decorations with class and special method call (): class is_positive (): def __init__ ( self , func ): self . function = func def __call__ ( self , * args , ** kwargs ): for arg in args : if arg < 0 : raise Exception ( \"Negative Number!\" ) return self . function ( * args , ** kwargs ) @is_positive def my_function ( agr1 , agr2 ): return arg1 + arg2","title":"Decorators"},{"location":"dev-tools/python/#generators","text":"def squares ( a , b ): i = a while i < b : y yield i ** 2 i += 1 s = squares ( 5 , 10 ) next ( s )","title":"Generators"},{"location":"dev-tools/python/#urllib-module-for-web-requests","text":"You can do a lot of neat stuff with it e.g. access API resources, make different types of HTTP requests like GET, POST, PUT, DELETE etc. Simple request to google.com to see what we get. import urllib.request with urllib . request . urlopen ( \"https://www.google.com/\" ) as url : print ( url . read ( 300 ) . decode ( \"utf-8\" ))","title":"urllib module for web requests"},{"location":"dev-tools/python/#get-request","text":"import urllib.request req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' ) with urllib . request . urlopen ( req ) as resp : print ( resp . read () . decode ( 'utf-8' ))","title":"GET request"},{"location":"dev-tools/python/#post-request","text":"import json import urllib.request # This is our Todo data = { \"userId\" : 101 , \"id\" : 100 , \"title\" : \"This is a POST request\" , \"completed\" : True } # Dump the todo object as a json string data = json . dumps ( data ) req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/' , data = bytes ( data . encode ( \"utf-8\" )), method = \"POST\" ) # Add the appropriate header. req . add_header ( \"Content-type\" , \"application/json; charset=UTF-8\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data )","title":"POST request"},{"location":"dev-tools/python/#put-request","text":"import json import urllib.request # This is our Todo data = { \"userId\" : 1 , \"id\" : 1 , \"title\" : \"This is a PUT request\" , \"completed\" : False } # Dump the todo object as a json string data = json . dumps ( data ) req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' , data = bytes ( data . encode ( \"utf-8\" )), method = \"PUT\" ) # Add the appropriate header. req . add_header ( \"Content-type\" , \"application/json; charset=UTF-8\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data )","title":"PUT request"},{"location":"dev-tools/python/#delete-request","text":"import json import urllib.request req = urllib . request . Request ( url = 'https://jsonplaceholder.typicode.com/todos/1' , method = \"DELETE\" ) with urllib . request . urlopen ( req ) as resp : response_data = json . loads ( resp . read () . decode ( \"utf-8\" )) print ( response_data )","title":"DELETE request"},{"location":"dev-tools/terraform/","text":"Terraform Up&Running \u00b6 IAM User Policies \u00b6 Add the following Managed Policies to your IAM user, as shown: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonRDSFullAccess CloudWatchFullAccess IAMFullAccess export AWS_ACCESS_KEY_ID =( your access key id ) export AWS_SECRET_ACCESS_KEY =( your secret access key )","title":"Terraform"},{"location":"dev-tools/terraform/#terraform-uprunning","text":"","title":"Terraform Up&amp;Running"},{"location":"dev-tools/terraform/#iam-user-policies","text":"Add the following Managed Policies to your IAM user, as shown: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonRDSFullAccess CloudWatchFullAccess IAMFullAccess export AWS_ACCESS_KEY_ID =( your access key id ) export AWS_SECRET_ACCESS_KEY =( your secret access key )","title":"IAM User Policies"},{"location":"docker/","text":"Docker tips and tricks \u00b6 Portainer \u00b6 Portainer - is very useful GUI for managing Docker's hosts and clusters. Simply open port 9000 on your host, run container and open in your browser host_ip:9000, create user and manage all your containers. docker container run -d \\ -p 9000 :9000 \\ -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer Install Docker \u00b6 The Docker installation package available in the official Ubuntu repository may not be the latest version. To ensure we get the latest version, we\u2019ll install Docker from the official Docker repository. To do that, we\u2019ll add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package. First, update your existing list of packages: sudo apt update Next, install a few prerequisite packages which let apt use packages over HTTPS: sudo apt install apt-transport-https ca-certificates curl software-properties-common Then add the GPG key for the official Docker repository to your system: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add the Docker repository to APT sources: sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" Next, update the package database with the Docker packages from the newly added repo and install Docker: sudo apt update sudo apt install docker-ce Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it\u2019s running: sudo systemctl status docker The output should be similar to the following, showing that the service is active and running: Output: \u25cf docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-07-05 15:08:39 UTC; 2min 55s ago Docs: https://docs.docker.com Main PID: 10096 (dockerd) Tasks: 16 CGroup: /system.slice/docker.service \u251c\u250010096 /usr/bin/dockerd -H fd:// \u2514\u250010113 docker-containerd --config /var/run/docker/containerd/containerd.toml Installing Docker now gives you not just the Docker service (daemon) but also the docker command line utility, or the Docker client. For more comfortable work from your user acc - create the docker group: sudo groupadd docker Add your user to the docker group: sudo usermod -aG docker $USER Understanding containers has to cover things like chroot, the container file system and layers, cgroup isolation, and talk about the pros/cons of containers. An explanation of how these work without a discussion of cgroups and how that facilitates process/memory/network isolation is going to sound weak to an interviewer. So read up on how cgroups are implemented in the kernel. Then learn about the different ways lxc containers work vs docker containers. Then learn about emerging container specification standards. Why are they happening? Understand what\u2019s going on both under the hood in the kernel and the shifts happening in the industry. chroot \u00b6 A chroot environment provides functionality similar to that of a virtual machine, but it is a lighter solution. The captive system doesn\u2019t need a hypervisor to be installed and configured, such as VirtualBox or Virtual Machine Manager. Nor does it need to have a kernel installed in the captive system. The captive system shares your existing kernel. Creating chroot environment \u00b6 We need a directory to act as the root directory of the chroot environment. So that we have a shorthand way of referring to that directory we\u2019ll create a variable and store the name of the directory in it. Here we\u2019re setting up a variable to store a path to the \u201ctestroot\u201d directory. It doesn\u2019t matter if this directory doesn\u2019t exist yet, we\u2019re going to create it soon. If the directory does exist, it should be empty. chr = /home/dave/testroot mkdir -p $chr We need to create directories to hold the portions of the operating system our chroot environment will require. We\u2019re going to set up a minimalist Linux environment that uses Bash as the interactive shell. We\u2019ll also include the touch, rm, and ls commands. That will allow us to use all Bash\u2019s built-in commands and touch, rm, and ls. We\u2019ll be able to create, list and remove files, and use Bash. And\u2014in this simple example\u2014that\u2019s all. mkdir -p $chr / { bin,lib,lib64 } cd $chr Let\u2019s copy the binaries that we need in our minimalist Linux environment from your regular \u201c/bin\u201d directory into our chroot \u201c/bin\u201d directory. The -v (verbose) option makes cp tell us what it is doing as it performs each copy action. cp -v /bin/ { bash,touch,ls,rm } $chr Dependencies \u00b6 These binaries will have dependencies. We need to discover what they are and copy those files into our environment. This way, for example we can add all dependencies for /bin/bash: list = \" $( ldd /bin/bash | egrep -o '/lib.*\\.[0-9]' ) \" for i in $list ; do cp -v --parents \" $i \" \" ${ chr } \" ; done Use that technique to capture the dependencies of each of the other commands. Or you can write one more loop through all your apps. chroot command \u00b6 The last of our dependencies are copied into our chroot environment. We\u2019re finally ready to use the chroot command. This command sets the root of the chroot environment, and specifies which application to run as the shell. sudo chroot $chr /bin/bash Our chroot environment is now active. The terminal window prompt has changed, and the interactive shell is the being handled by the bash shell in our environment. Use exit to leave the chroot environment: exit","title":"Docker tips"},{"location":"docker/#docker-tips-and-tricks","text":"","title":"Docker tips and tricks"},{"location":"docker/#portainer","text":"Portainer - is very useful GUI for managing Docker's hosts and clusters. Simply open port 9000 on your host, run container and open in your browser host_ip:9000, create user and manage all your containers. docker container run -d \\ -p 9000 :9000 \\ -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer","title":"Portainer"},{"location":"docker/#install-docker","text":"The Docker installation package available in the official Ubuntu repository may not be the latest version. To ensure we get the latest version, we\u2019ll install Docker from the official Docker repository. To do that, we\u2019ll add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package. First, update your existing list of packages: sudo apt update Next, install a few prerequisite packages which let apt use packages over HTTPS: sudo apt install apt-transport-https ca-certificates curl software-properties-common Then add the GPG key for the official Docker repository to your system: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add the Docker repository to APT sources: sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\" Next, update the package database with the Docker packages from the newly added repo and install Docker: sudo apt update sudo apt install docker-ce Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it\u2019s running: sudo systemctl status docker The output should be similar to the following, showing that the service is active and running: Output: \u25cf docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-07-05 15:08:39 UTC; 2min 55s ago Docs: https://docs.docker.com Main PID: 10096 (dockerd) Tasks: 16 CGroup: /system.slice/docker.service \u251c\u250010096 /usr/bin/dockerd -H fd:// \u2514\u250010113 docker-containerd --config /var/run/docker/containerd/containerd.toml Installing Docker now gives you not just the Docker service (daemon) but also the docker command line utility, or the Docker client. For more comfortable work from your user acc - create the docker group: sudo groupadd docker Add your user to the docker group: sudo usermod -aG docker $USER Understanding containers has to cover things like chroot, the container file system and layers, cgroup isolation, and talk about the pros/cons of containers. An explanation of how these work without a discussion of cgroups and how that facilitates process/memory/network isolation is going to sound weak to an interviewer. So read up on how cgroups are implemented in the kernel. Then learn about the different ways lxc containers work vs docker containers. Then learn about emerging container specification standards. Why are they happening? Understand what\u2019s going on both under the hood in the kernel and the shifts happening in the industry.","title":"Install Docker"},{"location":"docker/#chroot","text":"A chroot environment provides functionality similar to that of a virtual machine, but it is a lighter solution. The captive system doesn\u2019t need a hypervisor to be installed and configured, such as VirtualBox or Virtual Machine Manager. Nor does it need to have a kernel installed in the captive system. The captive system shares your existing kernel.","title":"chroot"},{"location":"docker/#creating-chroot-environment","text":"We need a directory to act as the root directory of the chroot environment. So that we have a shorthand way of referring to that directory we\u2019ll create a variable and store the name of the directory in it. Here we\u2019re setting up a variable to store a path to the \u201ctestroot\u201d directory. It doesn\u2019t matter if this directory doesn\u2019t exist yet, we\u2019re going to create it soon. If the directory does exist, it should be empty. chr = /home/dave/testroot mkdir -p $chr We need to create directories to hold the portions of the operating system our chroot environment will require. We\u2019re going to set up a minimalist Linux environment that uses Bash as the interactive shell. We\u2019ll also include the touch, rm, and ls commands. That will allow us to use all Bash\u2019s built-in commands and touch, rm, and ls. We\u2019ll be able to create, list and remove files, and use Bash. And\u2014in this simple example\u2014that\u2019s all. mkdir -p $chr / { bin,lib,lib64 } cd $chr Let\u2019s copy the binaries that we need in our minimalist Linux environment from your regular \u201c/bin\u201d directory into our chroot \u201c/bin\u201d directory. The -v (verbose) option makes cp tell us what it is doing as it performs each copy action. cp -v /bin/ { bash,touch,ls,rm } $chr","title":"Creating chroot environment"},{"location":"docker/#dependencies","text":"These binaries will have dependencies. We need to discover what they are and copy those files into our environment. This way, for example we can add all dependencies for /bin/bash: list = \" $( ldd /bin/bash | egrep -o '/lib.*\\.[0-9]' ) \" for i in $list ; do cp -v --parents \" $i \" \" ${ chr } \" ; done Use that technique to capture the dependencies of each of the other commands. Or you can write one more loop through all your apps.","title":"Dependencies"},{"location":"docker/#chroot-command","text":"The last of our dependencies are copied into our chroot environment. We\u2019re finally ready to use the chroot command. This command sets the root of the chroot environment, and specifies which application to run as the shell. sudo chroot $chr /bin/bash Our chroot environment is now active. The terminal window prompt has changed, and the interactive shell is the being handled by the bash shell in our environment. Use exit to leave the chroot environment: exit","title":"chroot command"},{"location":"docker/commans/","text":"Docker commands cheat-sheet \u00b6 Docker docs and manuals Run busybox inside docker-compose \u00b6 1 docker run --rm -it --network <NETWORK_NAME> busybox Containers \u00b6 docker container ls # docker ps docker container top { cont_name } # top command for cont docker container inspect { cont_name } # json with all cont parameters docker container inspect { cont_name } --format '{{ .NetworkSettings.IPAddress}}' docekr container stats # like watch command but for containers docker container start # start stopped cont docker container post { cont_name } # shows shared ports Network \u00b6 docker network ls docker network inspect { network_name } docker network create --driver docker network connect { network_name } docker network disconnect { network_name } Reset \u00b6 Delete all running and stopped containers \u00b6 docker container rm -f $( docker ps -aq ) Unused cont prune on current host \u00b6 docker system prune --all --force --volumes Full docker prune on current host \u00b6 docker stop $( docker container ls -a -q ) && docker system prune -a -f --volumes Print last container\u2019s logs \u00b6 docker container logs --tail 100 web","title":"Cheatsheet"},{"location":"docker/commans/#docker-commands-cheat-sheet","text":"Docker docs and manuals","title":"Docker commands cheat-sheet"},{"location":"docker/commans/#run-busybox-inside-docker-compose","text":"1 docker run --rm -it --network <NETWORK_NAME> busybox","title":"Run busybox inside docker-compose"},{"location":"docker/commans/#containers","text":"docker container ls # docker ps docker container top { cont_name } # top command for cont docker container inspect { cont_name } # json with all cont parameters docker container inspect { cont_name } --format '{{ .NetworkSettings.IPAddress}}' docekr container stats # like watch command but for containers docker container start # start stopped cont docker container post { cont_name } # shows shared ports","title":"Containers"},{"location":"docker/commans/#network","text":"docker network ls docker network inspect { network_name } docker network create --driver docker network connect { network_name } docker network disconnect { network_name }","title":"Network"},{"location":"docker/commans/#reset","text":"","title":"Reset"},{"location":"docker/commans/#delete-all-running-and-stopped-containers","text":"docker container rm -f $( docker ps -aq )","title":"Delete all running and stopped containers"},{"location":"docker/commans/#unused-cont-prune-on-current-host","text":"docker system prune --all --force --volumes","title":"Unused cont prune on current host"},{"location":"docker/commans/#full-docker-prune-on-current-host","text":"docker stop $( docker container ls -a -q ) && docker system prune -a -f --volumes","title":"Full docker prune on current host"},{"location":"docker/commans/#print-last-containers-logs","text":"docker container logs --tail 100 web","title":"Print last container\u2019s logs"},{"location":"kubernetes/advanced_tools/","text":"","title":"Advanced tools"},{"location":"kubernetes/configMaps/","text":"ConfigMaps and Secrets \u00b6","title":"ConfigMaps and Secrets"},{"location":"kubernetes/configMaps/#configmaps-and-secrets","text":"","title":"ConfigMaps and Secrets"},{"location":"kubernetes/helm/","text":"Helm \u00b6 Helm package manager","title":"Helm"},{"location":"kubernetes/helm/#helm","text":"Helm package manager","title":"Helm"},{"location":"kubernetes/ingress/","text":"Ingress \u00b6 An Ingress is a collection of rules that allow inbound connections to reach the cluster Services. To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following: 1 2 3 4 5 TLS (Transport Layer Security) Name-based virtual hosting Fanout routing Loadbalancing Custom rules. With Ingress, users do not connect directly to a Service. Users reach the Ingress endpoint, and, from there, the request is forwarded to the desired Service. You can see an example of a sample Ingress definition below: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : virtual-host-ingress namespace : default spec : rules : - host : blue.example.com http : paths : - backend : serviceName : webserver-blue-svc servicePort : 80 - host : green.example.com http : paths : - backend : serviceName : webserver-green-svc servicePort : 80 In the example above, user requests to both blue.example.com and green.example.com would go to the same Ingress endpoint, and, from there, they would be forwarded to webserver-blue-svc, and webserver-green-svc, respectively. This is an example of a Name-Based Virtual Hosting Ingress rule. We can also have Fanout Ingress rules, when requests to example.com/blue and example.com/green would be forwarded to webserver-blue-svc and webserver-green-svc, respectively: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : fan-out-ingress namespace : default spec : rules : - host : example.com http : paths : - path : /blue backend : serviceName : webserver-blue-svc servicePort : 80 - path : /green backend : serviceName : webserver-green-svc servicePort : 80 The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller. An Ingress Controller is an application watching the Master Node's API server for changes in the Ingress resources and updates the Layer 7 Load Balancer accordingly. Kubernetes supports different Ingress Controllers, and, if needed, we can also build our own. GCE L7 Load Balancer Controller and Nginx Ingress Controller are commonly used Ingress Controllers. Other controllers are Istio, Kong, Traefik, etc. # ingress.yaml apiVersion : v1 kind : Namespace metadata : name : nginx-ingress --- apiVersion : v1 kind : Secret metadata : name : default-server-secret namespace : nginx-ingress type : Opaque data : tls.crt : LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= tls.key : LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBdi91RWM4b1JkMHUvZXVJTHNFK1RYZUprckxMMnNJNGFWaEMvYjVyYy9XMlRiNHEvClJOcktGMEdYaVN1eE9ycXgrajlnamx4NXFjdnhkenRKbXNFUkJ1Z1B0ME9hVGtIekhvb3FVWmcwZGxmZ1dkT0EKUTZMNTdlT1l0Q29VOUZ4amRXdzZUVVRJVUQ4R0JsRlNjSVo0b1hFTkhzbysyR3VTTWk2Zk1wTVM3YUhudzFtMApxWkdvRWEzWFNyZEJ6eGc2clhkcUNlUDlCMXl3VmRyYURiUzc1aGQzdUdETDU4cGszOVFqVUFQaHpxdmRoK1JWClZGNGJCaW9CbTVpeTlZTW1hWVhsMm0wTGZzeTZuUTRRdFFzdEdNVWozcGJtdlFmazJBNnljeGRFeFpkZFZsdmwKMm82MjBsMllxcHFDZEtCRThCay90elFIVTlKcU56cHpoOUJUTXdJREFRQUJBb0lCQVFDZklHbXowOHhRVmorNwpLZnZJUXQwQ0YzR2MxNld6eDhVNml4MHg4Mm15d1kxUUNlL3BzWE9LZlRxT1h1SENyUlp5TnUvZ2IvUUQ4bUFOCmxOMjRZTWl0TWRJODg5TEZoTkp3QU5OODJDeTczckM5bzVvUDlkazAvYzRIbjAzSkVYNzZ5QjgzQm9rR1FvYksKMjhMNk0rdHUzUmFqNjd6Vmc2d2szaEhrU0pXSzBwV1YrSjdrUkRWYmhDYUZhNk5nMUZNRWxhTlozVDhhUUtyQgpDUDNDeEFTdjYxWTk5TEI4KzNXWVFIK3NYaTVGM01pYVNBZ1BkQUk3WEh1dXFET1lvMU5PL0JoSGt1aVg2QnRtCnorNTZud2pZMy8yUytSRmNBc3JMTnIwMDJZZi9oY0IraVlDNzVWYmcydVd6WTY3TWdOTGQ5VW9RU3BDRkYrVm4KM0cyUnhybnhBb0dCQU40U3M0ZVlPU2huMVpQQjdhTUZsY0k2RHR2S2ErTGZTTXFyY2pOZjJlSEpZNnhubmxKdgpGenpGL2RiVWVTbWxSekR0WkdlcXZXaHFISy9iTjIyeWJhOU1WMDlRQ0JFTk5jNmtWajJTVHpUWkJVbEx4QzYrCk93Z0wyZHhKendWelU0VC84ajdHalRUN05BZVpFS2FvRHFyRG5BYWkyaW5oZU1JVWZHRXFGKzJyQW9HQkFOMVAKK0tZL0lsS3RWRzRKSklQNzBjUis3RmpyeXJpY05iWCtQVzUvOXFHaWxnY2grZ3l4b25BWlBpd2NpeDN3QVpGdwpaZC96ZFB2aTBkWEppc1BSZjRMazg5b2pCUmpiRmRmc2l5UmJYbyt3TFU4NUhRU2NGMnN5aUFPaTVBRHdVU0FkCm45YWFweUNweEFkREtERHdObit3ZFhtaTZ0OHRpSFRkK3RoVDhkaVpBb0dCQUt6Wis1bG9OOTBtYlF4VVh5YUwKMjFSUm9tMGJjcndsTmVCaWNFSmlzaEhYa2xpSVVxZ3hSZklNM2hhUVRUcklKZENFaHFsV01aV0xPb2I2NTNyZgo3aFlMSXM1ZUtka3o0aFRVdnpldm9TMHVXcm9CV2xOVHlGanIrSWhKZnZUc0hpOGdsU3FkbXgySkJhZUFVWUNXCndNdlQ4NmNLclNyNkQrZG8wS05FZzFsL0FvR0FlMkFVdHVFbFNqLzBmRzgrV3hHc1RFV1JqclRNUzRSUjhRWXQKeXdjdFA4aDZxTGxKUTRCWGxQU05rMXZLTmtOUkxIb2pZT2pCQTViYjhibXNVU1BlV09NNENoaFJ4QnlHbmR2eAphYkJDRkFwY0IvbEg4d1R0alVZYlN5T294ZGt5OEp0ek90ajJhS0FiZHd6NlArWDZDODhjZmxYVFo5MWpYL3RMCjF3TmRKS2tDZ1lCbyt0UzB5TzJ2SWFmK2UwSkN5TGhzVDQ5cTN3Zis2QWVqWGx2WDJ1VnRYejN5QTZnbXo5aCsKcDNlK2JMRUxwb3B0WFhNdUFRR0xhUkcrYlNNcjR5dERYbE5ZSndUeThXczNKY3dlSTdqZVp2b0ZpbmNvVlVIMwphdmxoTUVCRGYxSjltSDB5cDBwWUNaS2ROdHNvZEZtQktzVEtQMjJhTmtsVVhCS3gyZzR6cFE9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo= --- apiVersion : v1 kind : ServiceAccount metadata : name : nginx-ingress namespace : nginx-ingress --- kind : ConfigMap apiVersion : v1 metadata : name : nginx-config namespace : nginx-ingress data : --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : nginx-ingress namespace : nginx-ingress spec : replicas : 1 selector : matchLabels : app : nginx-ingress template : metadata : labels : app : nginx-ingress spec : serviceAccountName : nginx-ingress containers : - image : nginx/nginx-ingress:edge imagePullPolicy : Always name : nginx-ingress ports : - name : http containerPort : 80 - name : https containerPort : 443 env : - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name args : - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret --- apiVersion : v1 kind : Service metadata : name : nginx-ingress namespace : nginx-ingress spec : type : NodePort ports : - port : 80 targetPort : 80 protocol : TCP name : http - port : 443 targetPort : 443 protocol : TCP name : https selector : app : nginx-ingress externalIPs : - 172.17.0.44 # ingress-rules.yaml apiVersion : extensions/v1beta1 kind : Ingress metadata : name : webapp-ingress spec : rules : - host : my.kubernetes.example http : paths : - path : /webapp1 backend : serviceName : webapp1-svc servicePort : 80 - path : /webapp2 backend : serviceName : webapp2-svc servicePort : 80 - backend : serviceName : webapp3-svc servicePort : 80 $ kubectl get inggress","title":"Ingress"},{"location":"kubernetes/ingress/#ingress","text":"An Ingress is a collection of rules that allow inbound connections to reach the cluster Services. To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following: 1 2 3 4 5 TLS (Transport Layer Security) Name-based virtual hosting Fanout routing Loadbalancing Custom rules. With Ingress, users do not connect directly to a Service. Users reach the Ingress endpoint, and, from there, the request is forwarded to the desired Service. You can see an example of a sample Ingress definition below: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : virtual-host-ingress namespace : default spec : rules : - host : blue.example.com http : paths : - backend : serviceName : webserver-blue-svc servicePort : 80 - host : green.example.com http : paths : - backend : serviceName : webserver-green-svc servicePort : 80 In the example above, user requests to both blue.example.com and green.example.com would go to the same Ingress endpoint, and, from there, they would be forwarded to webserver-blue-svc, and webserver-green-svc, respectively. This is an example of a Name-Based Virtual Hosting Ingress rule. We can also have Fanout Ingress rules, when requests to example.com/blue and example.com/green would be forwarded to webserver-blue-svc and webserver-green-svc, respectively: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : fan-out-ingress namespace : default spec : rules : - host : example.com http : paths : - path : /blue backend : serviceName : webserver-blue-svc servicePort : 80 - path : /green backend : serviceName : webserver-green-svc servicePort : 80 The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller. An Ingress Controller is an application watching the Master Node's API server for changes in the Ingress resources and updates the Layer 7 Load Balancer accordingly. Kubernetes supports different Ingress Controllers, and, if needed, we can also build our own. GCE L7 Load Balancer Controller and Nginx Ingress Controller are commonly used Ingress Controllers. Other controllers are Istio, Kong, Traefik, etc. # ingress.yaml apiVersion : v1 kind : Namespace metadata : name : nginx-ingress --- apiVersion : v1 kind : Secret metadata : name : default-server-secret namespace : nginx-ingress type : Opaque data : tls.crt : LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= tls.key : LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBdi91RWM4b1JkMHUvZXVJTHNFK1RYZUprckxMMnNJNGFWaEMvYjVyYy9XMlRiNHEvClJOcktGMEdYaVN1eE9ycXgrajlnamx4NXFjdnhkenRKbXNFUkJ1Z1B0ME9hVGtIekhvb3FVWmcwZGxmZ1dkT0EKUTZMNTdlT1l0Q29VOUZ4amRXdzZUVVRJVUQ4R0JsRlNjSVo0b1hFTkhzbysyR3VTTWk2Zk1wTVM3YUhudzFtMApxWkdvRWEzWFNyZEJ6eGc2clhkcUNlUDlCMXl3VmRyYURiUzc1aGQzdUdETDU4cGszOVFqVUFQaHpxdmRoK1JWClZGNGJCaW9CbTVpeTlZTW1hWVhsMm0wTGZzeTZuUTRRdFFzdEdNVWozcGJtdlFmazJBNnljeGRFeFpkZFZsdmwKMm82MjBsMllxcHFDZEtCRThCay90elFIVTlKcU56cHpoOUJUTXdJREFRQUJBb0lCQVFDZklHbXowOHhRVmorNwpLZnZJUXQwQ0YzR2MxNld6eDhVNml4MHg4Mm15d1kxUUNlL3BzWE9LZlRxT1h1SENyUlp5TnUvZ2IvUUQ4bUFOCmxOMjRZTWl0TWRJODg5TEZoTkp3QU5OODJDeTczckM5bzVvUDlkazAvYzRIbjAzSkVYNzZ5QjgzQm9rR1FvYksKMjhMNk0rdHUzUmFqNjd6Vmc2d2szaEhrU0pXSzBwV1YrSjdrUkRWYmhDYUZhNk5nMUZNRWxhTlozVDhhUUtyQgpDUDNDeEFTdjYxWTk5TEI4KzNXWVFIK3NYaTVGM01pYVNBZ1BkQUk3WEh1dXFET1lvMU5PL0JoSGt1aVg2QnRtCnorNTZud2pZMy8yUytSRmNBc3JMTnIwMDJZZi9oY0IraVlDNzVWYmcydVd6WTY3TWdOTGQ5VW9RU3BDRkYrVm4KM0cyUnhybnhBb0dCQU40U3M0ZVlPU2huMVpQQjdhTUZsY0k2RHR2S2ErTGZTTXFyY2pOZjJlSEpZNnhubmxKdgpGenpGL2RiVWVTbWxSekR0WkdlcXZXaHFISy9iTjIyeWJhOU1WMDlRQ0JFTk5jNmtWajJTVHpUWkJVbEx4QzYrCk93Z0wyZHhKendWelU0VC84ajdHalRUN05BZVpFS2FvRHFyRG5BYWkyaW5oZU1JVWZHRXFGKzJyQW9HQkFOMVAKK0tZL0lsS3RWRzRKSklQNzBjUis3RmpyeXJpY05iWCtQVzUvOXFHaWxnY2grZ3l4b25BWlBpd2NpeDN3QVpGdwpaZC96ZFB2aTBkWEppc1BSZjRMazg5b2pCUmpiRmRmc2l5UmJYbyt3TFU4NUhRU2NGMnN5aUFPaTVBRHdVU0FkCm45YWFweUNweEFkREtERHdObit3ZFhtaTZ0OHRpSFRkK3RoVDhkaVpBb0dCQUt6Wis1bG9OOTBtYlF4VVh5YUwKMjFSUm9tMGJjcndsTmVCaWNFSmlzaEhYa2xpSVVxZ3hSZklNM2hhUVRUcklKZENFaHFsV01aV0xPb2I2NTNyZgo3aFlMSXM1ZUtka3o0aFRVdnpldm9TMHVXcm9CV2xOVHlGanIrSWhKZnZUc0hpOGdsU3FkbXgySkJhZUFVWUNXCndNdlQ4NmNLclNyNkQrZG8wS05FZzFsL0FvR0FlMkFVdHVFbFNqLzBmRzgrV3hHc1RFV1JqclRNUzRSUjhRWXQKeXdjdFA4aDZxTGxKUTRCWGxQU05rMXZLTmtOUkxIb2pZT2pCQTViYjhibXNVU1BlV09NNENoaFJ4QnlHbmR2eAphYkJDRkFwY0IvbEg4d1R0alVZYlN5T294ZGt5OEp0ek90ajJhS0FiZHd6NlArWDZDODhjZmxYVFo5MWpYL3RMCjF3TmRKS2tDZ1lCbyt0UzB5TzJ2SWFmK2UwSkN5TGhzVDQ5cTN3Zis2QWVqWGx2WDJ1VnRYejN5QTZnbXo5aCsKcDNlK2JMRUxwb3B0WFhNdUFRR0xhUkcrYlNNcjR5dERYbE5ZSndUeThXczNKY3dlSTdqZVp2b0ZpbmNvVlVIMwphdmxoTUVCRGYxSjltSDB5cDBwWUNaS2ROdHNvZEZtQktzVEtQMjJhTmtsVVhCS3gyZzR6cFE9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo= --- apiVersion : v1 kind : ServiceAccount metadata : name : nginx-ingress namespace : nginx-ingress --- kind : ConfigMap apiVersion : v1 metadata : name : nginx-config namespace : nginx-ingress data : --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : nginx-ingress namespace : nginx-ingress spec : replicas : 1 selector : matchLabels : app : nginx-ingress template : metadata : labels : app : nginx-ingress spec : serviceAccountName : nginx-ingress containers : - image : nginx/nginx-ingress:edge imagePullPolicy : Always name : nginx-ingress ports : - name : http containerPort : 80 - name : https containerPort : 443 env : - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name args : - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret --- apiVersion : v1 kind : Service metadata : name : nginx-ingress namespace : nginx-ingress spec : type : NodePort ports : - port : 80 targetPort : 80 protocol : TCP name : http - port : 443 targetPort : 443 protocol : TCP name : https selector : app : nginx-ingress externalIPs : - 172.17.0.44 # ingress-rules.yaml apiVersion : extensions/v1beta1 kind : Ingress metadata : name : webapp-ingress spec : rules : - host : my.kubernetes.example http : paths : - path : /webapp1 backend : serviceName : webapp1-svc servicePort : 80 - path : /webapp2 backend : serviceName : webapp2-svc servicePort : 80 - backend : serviceName : webapp3-svc servicePort : 80 $ kubectl get inggress","title":"Ingress"},{"location":"kubernetes/kubectl/","text":"kubectl cheatsheet \u00b6 Connection details \u00b6 To look at the connection details, we can either see the content of the ~/.kube/config file (on Linux) or run the following command: $ kubectl config view Cluster info \u00b6 kubectl cluster-info When not using the kubectl proxy, we need to authenticate to the API server when sending API requests. We can authenticate by providing a Bearer Token when issuing a curl, or by providing a set of keys and certificates. A Bearer Token is an access token which is generated by the authentication server (the API server on the master node) and given back to the client. Using that token, the client can connect back to the Kubernetes API server without providing further authentication details, and then, access resources. Get the token: \u00b6 $ TOKEN = $( kubectl describe secret -n kube-system $( kubectl get secrets -n kube-system | grep default | cut -f1 -d ' ' ) | grep -E '^token' | cut -f2 -d ':' | tr -d '\\t' | tr -d \" \" ) Get the API server endpoint: \u00b6 $ APISERVER = $( kubectl config view | grep https | cut -f 2 - -d \":\" | tr -d \" \" ) Access the API server \u00b6 $ curl $APISERVER --header \"Authorization: Bearer $TOKEN \" --insecure Instead of the access token, we can extract the client certificate, client key, and certificate authority data from the .kube/config file. Once extracted, they are encoded and then passed with a curl command for authentication. The new curl command looks similar to: $ curl $APISERVER --cert encoded-cert --key encoded-key --cacert encoded-ca List the Pods \u00b6 Along with their attached Labels \u00b6 With the -L option to the kubectl get pods command, we add extra columns in the output to list Pods with their attached Label keys and their values. In the following example, we are listing Pods with the Label keys k8s-app and label2 : $ kubectl get pods -L k8s-app,label2 NAME READY STATUS RESTARTS AGE K8S-APP LABEL2 webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 16m webserver Pods with a given Label \u00b6 To use a selector with the kubectl get pods command, we can use the -l option. In the following example, we are selecting all the Pods that have the k8s-app Label key set to value webserver : $ kubectl get pods -l k8s-app = webserver NAME READY STATUS RESTARTS AGE webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 23m webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 24m webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 23m Look at a Pod's Details \u00b6 We can look at an object's details using kubectl describe command. In the following example, you can see a Pod's description: $ kubectl describe pod webserver-c8f4d5fbc-ggqkj Name: webserver-c8f4d5fbc-ggqkj Namespace: default Priority: 0 Node: minikube/10.0.2.15 Start Time: Sat, 05 Oct 2019 14 :37:34 +0300 Labels: k8s-app = webserver pod-template-hash = c8f4d5fbc Annotations: <none> Status: Running IP: 172 .17.0.7 IPs: IP: 172 .17.0.7 Controlled By: ReplicaSet/webserver-c8f4d5fbc Containers: webserver: Container ID: docker://b7d6bd7ce9eaefe48523d486f7174ac748571546bec4d8674b89d4438c8707da Image: nginx:alpine Image ID: docker-pullable://nginx@sha256:77f340700d08fd45026823f44fc0010a5bd2237c2d049178b473cd2ad977d071 Port: <none> Host Port: <none> State: Running Started: Sat, 05 Oct 2019 14 :37:36 +0300 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-r6llk ( ro ) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-r6llk: Type: Secret ( a volume populated by a Secret ) SecretName: default-token-r6llk Optional: false QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled <unknown> default-scheduler Successfully assigned default/webserver-c8f4d5fbc-ggqkj to minikube Normal Pulled 19m kubelet, minikube Container image \"nginx:alpine\" already present on machine Normal Created 19m kubelet, minikube Created container webserver Normal Started 19m kubelet, minikube Started container webserver Exposing an Application \u00b6 For a NodePort ServiceType, Kubernetes opens up a static port on all the worker nodes. If we connect to that port from any node, we are proxied to the ClusterIP of the Service. Next, let's use the NodePort ServiceType while creating a Service. Create a webserver-svc.yaml file with the following content: apiVersion : v1 kind : Service metadata : name : web-service labels : run : web-service spec : type : NodePort ports : - port : 80 protocol : TCP selector : app : nginx Using kubectl, create the Service: $ kubectl create -f webserver-svc.yaml service/web-service created A more direct method of creating a Service is by exposing the previously created Deployment (this method requires an existing Deployment). Expose a Deployment with the kubectl expose command: $ kubectl expose deployment webserver --name = web-service --type = NodePort service/web-service exposed Create an NGINX Pod kubectl run --generator=run-pod/v1 nginx --image=nginx Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml label Create a deployment kubectl run --generator=deployment/v1beta1 nginx --image=nginx Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run -o yaml Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml Save it to a file - (If you need to modify or add some other details) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml >\u00c2 nginx-deployment.yaml more kubectl commands and manuals is here","title":"kubectl cheatsheet"},{"location":"kubernetes/kubectl/#kubectl-cheatsheet","text":"","title":"kubectl cheatsheet"},{"location":"kubernetes/kubectl/#connection-details","text":"To look at the connection details, we can either see the content of the ~/.kube/config file (on Linux) or run the following command: $ kubectl config view","title":"Connection details"},{"location":"kubernetes/kubectl/#cluster-info","text":"kubectl cluster-info When not using the kubectl proxy, we need to authenticate to the API server when sending API requests. We can authenticate by providing a Bearer Token when issuing a curl, or by providing a set of keys and certificates. A Bearer Token is an access token which is generated by the authentication server (the API server on the master node) and given back to the client. Using that token, the client can connect back to the Kubernetes API server without providing further authentication details, and then, access resources.","title":"Cluster info"},{"location":"kubernetes/kubectl/#get-the-token","text":"$ TOKEN = $( kubectl describe secret -n kube-system $( kubectl get secrets -n kube-system | grep default | cut -f1 -d ' ' ) | grep -E '^token' | cut -f2 -d ':' | tr -d '\\t' | tr -d \" \" )","title":"Get the token:"},{"location":"kubernetes/kubectl/#get-the-api-server-endpoint","text":"$ APISERVER = $( kubectl config view | grep https | cut -f 2 - -d \":\" | tr -d \" \" )","title":"Get the API server endpoint:"},{"location":"kubernetes/kubectl/#access-the-api-server","text":"$ curl $APISERVER --header \"Authorization: Bearer $TOKEN \" --insecure Instead of the access token, we can extract the client certificate, client key, and certificate authority data from the .kube/config file. Once extracted, they are encoded and then passed with a curl command for authentication. The new curl command looks similar to: $ curl $APISERVER --cert encoded-cert --key encoded-key --cacert encoded-ca","title":"Access the API server"},{"location":"kubernetes/kubectl/#list-the-pods","text":"","title":"List the Pods"},{"location":"kubernetes/kubectl/#along-with-their-attached-labels","text":"With the -L option to the kubectl get pods command, we add extra columns in the output to list Pods with their attached Label keys and their values. In the following example, we are listing Pods with the Label keys k8s-app and label2 : $ kubectl get pods -L k8s-app,label2 NAME READY STATUS RESTARTS AGE K8S-APP LABEL2 webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 16m webserver webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 16m webserver","title":"Along with their attached Labels"},{"location":"kubernetes/kubectl/#pods-with-a-given-label","text":"To use a selector with the kubectl get pods command, we can use the -l option. In the following example, we are selecting all the Pods that have the k8s-app Label key set to value webserver : $ kubectl get pods -l k8s-app = webserver NAME READY STATUS RESTARTS AGE webserver-c8f4d5fbc-ggqkj 1 /1 Running 0 23m webserver-c8f4d5fbc-hmh2t 1 /1 Running 0 24m webserver-c8f4d5fbc-t2ntz 1 /1 Running 0 23m","title":"Pods with a given Label"},{"location":"kubernetes/kubectl/#look-at-a-pods-details","text":"We can look at an object's details using kubectl describe command. In the following example, you can see a Pod's description: $ kubectl describe pod webserver-c8f4d5fbc-ggqkj Name: webserver-c8f4d5fbc-ggqkj Namespace: default Priority: 0 Node: minikube/10.0.2.15 Start Time: Sat, 05 Oct 2019 14 :37:34 +0300 Labels: k8s-app = webserver pod-template-hash = c8f4d5fbc Annotations: <none> Status: Running IP: 172 .17.0.7 IPs: IP: 172 .17.0.7 Controlled By: ReplicaSet/webserver-c8f4d5fbc Containers: webserver: Container ID: docker://b7d6bd7ce9eaefe48523d486f7174ac748571546bec4d8674b89d4438c8707da Image: nginx:alpine Image ID: docker-pullable://nginx@sha256:77f340700d08fd45026823f44fc0010a5bd2237c2d049178b473cd2ad977d071 Port: <none> Host Port: <none> State: Running Started: Sat, 05 Oct 2019 14 :37:36 +0300 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-r6llk ( ro ) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-r6llk: Type: Secret ( a volume populated by a Secret ) SecretName: default-token-r6llk Optional: false QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled <unknown> default-scheduler Successfully assigned default/webserver-c8f4d5fbc-ggqkj to minikube Normal Pulled 19m kubelet, minikube Container image \"nginx:alpine\" already present on machine Normal Created 19m kubelet, minikube Created container webserver Normal Started 19m kubelet, minikube Started container webserver","title":"Look at a Pod's Details"},{"location":"kubernetes/kubectl/#exposing-an-application","text":"For a NodePort ServiceType, Kubernetes opens up a static port on all the worker nodes. If we connect to that port from any node, we are proxied to the ClusterIP of the Service. Next, let's use the NodePort ServiceType while creating a Service. Create a webserver-svc.yaml file with the following content: apiVersion : v1 kind : Service metadata : name : web-service labels : run : web-service spec : type : NodePort ports : - port : 80 protocol : TCP selector : app : nginx Using kubectl, create the Service: $ kubectl create -f webserver-svc.yaml service/web-service created A more direct method of creating a Service is by exposing the previously created Deployment (this method requires an existing Deployment). Expose a Deployment with the kubectl expose command: $ kubectl expose deployment webserver --name = web-service --type = NodePort service/web-service exposed Create an NGINX Pod kubectl run --generator=run-pod/v1 nginx --image=nginx Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml label Create a deployment kubectl run --generator=deployment/v1beta1 nginx --image=nginx Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run -o yaml Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml Save it to a file - (If you need to modify or add some other details) kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml >\u00c2 nginx-deployment.yaml more kubectl commands and manuals is here","title":"Exposing an Application"},{"location":"kubernetes/monitoring/","text":"Services Monitoring Workflow \u00b6 Services Monitoring Workflow consists of three main components: 1 Collection agent, Metrics Server, and Dashboards. Typical workflow, including most common components: Monitoring agent collects node metrics. cAdvisor collects containers and pods metrics. Monitoring Aggregation service collects data from its own agent and cAdvisor. Data is stored in the monitoring system\u2019s storage. Monitoring aggregation service exposes metrics through APIs and dashboards. A Few Notes: Prometheus is the official monitoring server sponsored and incubated by CNCF. It integrates directly with cAdvisor. You don\u2019t need to install a 3 rd party agent to retrieve additional metrics about your containers. However, if you need deeper insights about each node, you need to install an agent of your choice \u2014 see Prometheus integrations and third-party exporters page. Almost all monitoring systems piggyback on Kubernetes scheduling and orchestration. For example, their agents are installed as DeomonSets and depend on Kubernetes scheduler to have an instance scheduled on each node. Most monitoring agents depend on Kubelet to collect container relevant metrics, which in turn depends on cAdvisor. Very few agents collect container relevant details independently. Most monitoring aggregation services depend on agents pushing metrics to them. Prometheus is an exception. It pulls metrics out of the installed agents. What to monitor \u00b6 Ideal Services Workflow depends on this factors: - collection of relevant metrics - perception of continuous changes inside the k8s cluster. A good pipeline should focus on collecting relevant metrics. There are plenty of agents that can collect OS and process-level metrics. But you will find very few out there that can collect details about containers running at a given node, such as the number of running containers, container state, docker engine metrics, etc. cAdvisor is the best agent for this job. Perception of continuous changes means that the monitoring pipeline is aware of different pods, containers instances and can relate them to their parent entities, i.e. Deployment, Statefulsets, Namespace, etc. It also means that the metrics server is aware of system-wide metrics that should be visible to users, such as the number of pending pods, nodes status, etc. TL;DR \u00b6 You need to differentiate between core metrics pipeline and the services pipeline. You should pick the best pipeline that works for your needs. The community official metrics collector tool is Prometheus. Use Grafana Dashboards for visualization . But not for alerting.","title":"Monitoring"},{"location":"kubernetes/monitoring/#services-monitoring-workflow","text":"Services Monitoring Workflow consists of three main components: 1 Collection agent, Metrics Server, and Dashboards. Typical workflow, including most common components: Monitoring agent collects node metrics. cAdvisor collects containers and pods metrics. Monitoring Aggregation service collects data from its own agent and cAdvisor. Data is stored in the monitoring system\u2019s storage. Monitoring aggregation service exposes metrics through APIs and dashboards. A Few Notes: Prometheus is the official monitoring server sponsored and incubated by CNCF. It integrates directly with cAdvisor. You don\u2019t need to install a 3 rd party agent to retrieve additional metrics about your containers. However, if you need deeper insights about each node, you need to install an agent of your choice \u2014 see Prometheus integrations and third-party exporters page. Almost all monitoring systems piggyback on Kubernetes scheduling and orchestration. For example, their agents are installed as DeomonSets and depend on Kubernetes scheduler to have an instance scheduled on each node. Most monitoring agents depend on Kubelet to collect container relevant metrics, which in turn depends on cAdvisor. Very few agents collect container relevant details independently. Most monitoring aggregation services depend on agents pushing metrics to them. Prometheus is an exception. It pulls metrics out of the installed agents.","title":"Services Monitoring Workflow"},{"location":"kubernetes/monitoring/#what-to-monitor","text":"Ideal Services Workflow depends on this factors: - collection of relevant metrics - perception of continuous changes inside the k8s cluster. A good pipeline should focus on collecting relevant metrics. There are plenty of agents that can collect OS and process-level metrics. But you will find very few out there that can collect details about containers running at a given node, such as the number of running containers, container state, docker engine metrics, etc. cAdvisor is the best agent for this job. Perception of continuous changes means that the monitoring pipeline is aware of different pods, containers instances and can relate them to their parent entities, i.e. Deployment, Statefulsets, Namespace, etc. It also means that the metrics server is aware of system-wide metrics that should be visible to users, such as the number of pending pods, nodes status, etc.","title":"What to monitor"},{"location":"kubernetes/monitoring/#tldr","text":"You need to differentiate between core metrics pipeline and the services pipeline. You should pick the best pipeline that works for your needs. The community official metrics collector tool is Prometheus. Use Grafana Dashboards for visualization . But not for alerting.","title":"TL;DR"},{"location":"kubernetes/networking/","text":"About Flanel: https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c A ServiceAccount is required to login. A ClusterRoleBinding is used to assign the new ServiceAccount (admin-user) the role of cluster-admin on the cluster. cat <<EOF | kubectl create -f - apiVersion : v1 kind : ServiceAccount metadata : name : admin-user namespace : kube-system --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : admin-user roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : ServiceAccount name : admin-user namespace : kube-system EOF This means they can control all aspects of Kubernetes. With ClusterRoleBinding and RBAC, different level of permissions can be defined based on security requirements. More information on creating a user for the Dashboard can be found in the Dashboard documentation. Once the ServiceAccount has been created, the token to login can be found with: kubectl -n kube-system describe secret $( kubectl -n kube-system get secret | grep admin-user | awk '{print $1}' ) cluster-ip.yaml apiVersion : v1 kind : Service metadata : name : webapp1-clusterip-svc labels : app : webapp1-clusterip spec : ports : - port : 80 selector : app : webapp1-clusterip --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-clusterip-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-clusterip spec : containers : - name : webapp1-clusterip-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- export CLUSTER_IP = $( kubectl get services/webapp1-clusterip-svc -o go-template = '{{(index .spec.clusterIP)}}' ) clusterip-target.yaml apiVersion : v1 kind : Service metadata : name : webapp1-clusterip-targetport-svc labels : app : webapp1-clusterip-targetport spec : ports : - port : 8080 targetPort : 80 selector : app : webapp1-clusterip-targetport --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-clusterip-targetport-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-clusterip-targetport spec : containers : - name : webapp1-clusterip-targetport-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- export CLUSTER_IP = $( kubectl get services/webapp1-clusterip-targetport-svc -o go-template = '{{(index .spec.clusterIP)}}' ) nodeport.yaml apiVersion : v1 kind : Service metadata : name : webapp1-nodeport-svc labels : app : webapp1-nodeport spec : type : NodePort ports : - port : 80 nodePort : 30080 selector : app : webapp1-nodeport --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-nodeport-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-nodeport spec : containers : - name : webapp1-nodeport-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- externalip.yaml apiVersion : v1 kind : Service metadata : name : webapp1-externalip-svc labels : app : webapp1-externalip spec : ports : - port : 80 externalIPs : - 172.17.0.82 selector : app : webapp1-externalip --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-externalip-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-externalip spec : containers : - name : webapp1-externalip-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- cloudprovider.yaml \u00b6 apiVersion : extensions/v1beta1 kind : DaemonSet metadata : name : kube-keepalived-vip namespace : kube-system spec : template : metadata : labels : name : kube-keepalived-vip spec : hostNetwork : true containers : - image : gcr.io/google_containers/kube-keepalived-vip:0.9 name : kube-keepalived-vip imagePullPolicy : Always securityContext : privileged : true volumeMounts : - mountPath : /lib/modules name : modules readOnly : true - mountPath : /dev name : dev # use downward API env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace # to use unicast args : - --services-configmap=kube-system/vip-configmap # unicast uses the ip of the nodes instead of multicast # this is useful if running in cloud providers (like AWS) #- --use-unicast=true volumes : - name : modules hostPath : path : /lib/modules - name : dev hostPath : path : /dev nodeSelector : # type: worker # adjust this to match your worker nodes --- ## We also create an empty ConfigMap to hold our config apiVersion : v1 kind : ConfigMap metadata : name : vip-configmap namespace : kube-system data : --- apiVersion : apps/v1beta1 kind : Deployment metadata : labels : app : keepalived-cloud-provider name : keepalived-cloud-provider namespace : kube-system spec : replicas : 1 revisionHistoryLimit : 2 selector : matchLabels : app : keepalived-cloud-provider strategy : type : RollingUpdate template : metadata : annotations : scheduler.alpha.kubernetes.io/critical-pod : \"\" scheduler.alpha.kubernetes.io/tolerations : '[{\"key\":\"CriticalAddonsOnly\", \"operator\":\"Exists\"}]' labels : app : keepalived-cloud-provider spec : containers : - name : keepalived-cloud-provider image : quay.io/munnerz/keepalived-cloud-provider:0.0.1 imagePullPolicy : IfNotPresent env : - name : KEEPALIVED_NAMESPACE value : kube-system - name : KEEPALIVED_CONFIG_MAP value : vip-configmap - name : KEEPALIVED_SERVICE_CIDR value : 10.10.0.0/26 # pick a CIDR that is explicitly reserved for keepalived volumeMounts : - name : certs mountPath : /etc/ssl/certs resources : requests : cpu : 200m livenessProbe : httpGet : path : /healthz port : 10252 host : 127.0.0.1 initialDelaySeconds : 15 timeoutSeconds : 15 failureThreshold : 8 volumes : - name : certs hostPath : path : /etc/ssl/certs loadbalancer.yaml \u00b6 apiVersion : v1 kind : Service metadata : name : webapp1-loadbalancer-svc labels : app : webapp1-loadbalancer spec : type : LoadBalancer ports : - port : 80 selector : app : webapp1-loadbalancer --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-loadbalancer-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-loadbalancer spec : containers : - name : webapp1-loadbalancer-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- export LoadBalancerIP = $( kubectl get services/webapp1-loadbalancer-svc -o go-template = '{{(index .status.loadBalancer.ingress 0).ip}}' )","title":"Networking"},{"location":"kubernetes/networking/#cloudprovideryaml","text":"apiVersion : extensions/v1beta1 kind : DaemonSet metadata : name : kube-keepalived-vip namespace : kube-system spec : template : metadata : labels : name : kube-keepalived-vip spec : hostNetwork : true containers : - image : gcr.io/google_containers/kube-keepalived-vip:0.9 name : kube-keepalived-vip imagePullPolicy : Always securityContext : privileged : true volumeMounts : - mountPath : /lib/modules name : modules readOnly : true - mountPath : /dev name : dev # use downward API env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace # to use unicast args : - --services-configmap=kube-system/vip-configmap # unicast uses the ip of the nodes instead of multicast # this is useful if running in cloud providers (like AWS) #- --use-unicast=true volumes : - name : modules hostPath : path : /lib/modules - name : dev hostPath : path : /dev nodeSelector : # type: worker # adjust this to match your worker nodes --- ## We also create an empty ConfigMap to hold our config apiVersion : v1 kind : ConfigMap metadata : name : vip-configmap namespace : kube-system data : --- apiVersion : apps/v1beta1 kind : Deployment metadata : labels : app : keepalived-cloud-provider name : keepalived-cloud-provider namespace : kube-system spec : replicas : 1 revisionHistoryLimit : 2 selector : matchLabels : app : keepalived-cloud-provider strategy : type : RollingUpdate template : metadata : annotations : scheduler.alpha.kubernetes.io/critical-pod : \"\" scheduler.alpha.kubernetes.io/tolerations : '[{\"key\":\"CriticalAddonsOnly\", \"operator\":\"Exists\"}]' labels : app : keepalived-cloud-provider spec : containers : - name : keepalived-cloud-provider image : quay.io/munnerz/keepalived-cloud-provider:0.0.1 imagePullPolicy : IfNotPresent env : - name : KEEPALIVED_NAMESPACE value : kube-system - name : KEEPALIVED_CONFIG_MAP value : vip-configmap - name : KEEPALIVED_SERVICE_CIDR value : 10.10.0.0/26 # pick a CIDR that is explicitly reserved for keepalived volumeMounts : - name : certs mountPath : /etc/ssl/certs resources : requests : cpu : 200m livenessProbe : httpGet : path : /healthz port : 10252 host : 127.0.0.1 initialDelaySeconds : 15 timeoutSeconds : 15 failureThreshold : 8 volumes : - name : certs hostPath : path : /etc/ssl/certs","title":"cloudprovider.yaml"},{"location":"kubernetes/networking/#loadbalanceryaml","text":"apiVersion : v1 kind : Service metadata : name : webapp1-loadbalancer-svc labels : app : webapp1-loadbalancer spec : type : LoadBalancer ports : - port : 80 selector : app : webapp1-loadbalancer --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : webapp1-loadbalancer-deployment spec : replicas : 2 template : metadata : labels : app : webapp1-loadbalancer spec : containers : - name : webapp1-loadbalancer-pod image : katacoda/docker-http-server:latest ports : - containerPort : 80 --- export LoadBalancerIP = $( kubectl get services/webapp1-loadbalancer-svc -o go-template = '{{(index .status.loadBalancer.ingress 0).ip}}' )","title":"loadbalancer.yaml"},{"location":"kubernetes/volume_types/","text":"Volume Types \u00b6 A directory which is mounted inside a Pod is backed by the underlying Volume Type. A Volume Type decides the properties of the directory, like size, content, default access modes, etc. Some examples of Volume Types are: emptyDir An empty Volume is created for the Pod as soon as it is scheduled on the worker node. The Volume's life is tightly coupled with the Pod. If the Pod is terminated, the content of emptyDir is deleted forever. hostPath With the hostPath Volume Type, we can share a directory from the host to the Pod. If the Pod is terminated, the content of the Volume is still available on the host. gcePersistentDisk With the gcePersistentDisk Volume Type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStore With the awsElasticBlockStore Volume Type, we can mount an AWS EBS Volume into a Pod. azureDisk With azureDisk we can mount a Microsoft Azure Data Disk into a Pod. azureFile With azureFile we can mount a Microsoft Azure File Volume into a Pod. cephfs With cephfs, an existing CephFS volume can be mounted into a Pod. When a Pod terminates, the volume is unmounted and the contents of the volume are preserved. nfs With nfs, we can mount an NFS share into a Pod. iscsi With iscsi, we can mount an iSCSI share into a Pod. secret With the secret Volume Type, we can pass sensitive information, such as passwords, to Pods. configMap With configMap objects, we can provide configuration data, or shell commands and arguments into a Pod. persistentVolumeClaim We can attach a PersistentVolume to a Pod using a persistentVolumeClaim. PersistentVolumes \u00b6 In a typical IT environment, storage is managed by the storage/system administrators. The end user will just receive instructions to use the storage but is not involved with the underlying storage management. In the containerized world, we would like to follow similar rules, but it becomes challenging, given the many Volume Types we have seen earlier. Kubernetes resolves this problem with the PersistentVolume (PV) subsystem, which provides APIs for users and administrators to manage and consume persistent storage. To manage the Volume, it uses the PersistentVolume API resource type, and to consume it, it uses the PersistentVolumeClaim API resource type. A Persistent Volume is a network-attached storage in the cluster, which is provisioned by the administrator. PersistentVolumes can be dynamically provisioned based on the StorageClass resource. A StorageClass contains pre-defined provisioners and parameters to create a PersistentVolume. Using PersistentVolumeClaims, a user sends the request for dynamic PV creation, which gets wired to the StorageClass resource. Some of the Volume Types that support managing storage using PersistentVolumes are: 1 2 3 4 5 6 7 GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CephFS NFS iSCSI You can learn more details about Volume Types in the Kubernetes documentation .","title":"Volume Types"},{"location":"kubernetes/volume_types/#volume-types","text":"A directory which is mounted inside a Pod is backed by the underlying Volume Type. A Volume Type decides the properties of the directory, like size, content, default access modes, etc. Some examples of Volume Types are: emptyDir An empty Volume is created for the Pod as soon as it is scheduled on the worker node. The Volume's life is tightly coupled with the Pod. If the Pod is terminated, the content of emptyDir is deleted forever. hostPath With the hostPath Volume Type, we can share a directory from the host to the Pod. If the Pod is terminated, the content of the Volume is still available on the host. gcePersistentDisk With the gcePersistentDisk Volume Type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStore With the awsElasticBlockStore Volume Type, we can mount an AWS EBS Volume into a Pod. azureDisk With azureDisk we can mount a Microsoft Azure Data Disk into a Pod. azureFile With azureFile we can mount a Microsoft Azure File Volume into a Pod. cephfs With cephfs, an existing CephFS volume can be mounted into a Pod. When a Pod terminates, the volume is unmounted and the contents of the volume are preserved. nfs With nfs, we can mount an NFS share into a Pod. iscsi With iscsi, we can mount an iSCSI share into a Pod. secret With the secret Volume Type, we can pass sensitive information, such as passwords, to Pods. configMap With configMap objects, we can provide configuration data, or shell commands and arguments into a Pod. persistentVolumeClaim We can attach a PersistentVolume to a Pod using a persistentVolumeClaim.","title":"Volume Types"},{"location":"kubernetes/volume_types/#persistentvolumes","text":"In a typical IT environment, storage is managed by the storage/system administrators. The end user will just receive instructions to use the storage but is not involved with the underlying storage management. In the containerized world, we would like to follow similar rules, but it becomes challenging, given the many Volume Types we have seen earlier. Kubernetes resolves this problem with the PersistentVolume (PV) subsystem, which provides APIs for users and administrators to manage and consume persistent storage. To manage the Volume, it uses the PersistentVolume API resource type, and to consume it, it uses the PersistentVolumeClaim API resource type. A Persistent Volume is a network-attached storage in the cluster, which is provisioned by the administrator. PersistentVolumes can be dynamically provisioned based on the StorageClass resource. A StorageClass contains pre-defined provisioners and parameters to create a PersistentVolume. Using PersistentVolumeClaims, a user sends the request for dynamic PV creation, which gets wired to the StorageClass resource. Some of the Volume Types that support managing storage using PersistentVolumes are: 1 2 3 4 5 6 7 GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CephFS NFS iSCSI You can learn more details about Volume Types in the Kubernetes documentation .","title":"PersistentVolumes"},{"location":"linux/","text":"Linux \u00b6 Coming soon...","title":"Linux"},{"location":"linux/#linux","text":"Coming soon...","title":"Linux"},{"location":"miscellaneous/","text":"Notes \u00b6 This section is for my own notes, tests and tryouts. Maybe some ideas and drafts.","title":"Notes"},{"location":"miscellaneous/#notes","text":"This section is for my own notes, tests and tryouts. Maybe some ideas and drafts.","title":"Notes"},{"location":"miscellaneous/bookshelf/","text":"Useful Links \u00b6 r/devops Books \u00b6 My current have-to-be-read list \u00b6 The Kubernetes Scheduler Imperative vs. Declarative \u2014 a Kubernetes Tutorial Deploy a Kubernetes Cluster with Cluster Monitoring in Amazon EC2 using Rancher 10 Advanced Tricks with Docker 50 Useful Docker Tutorials, From Beginner to Advanced \u0417\u043d\u0430\u043a\u043e\u043c\u0441\u0442\u0432\u043e \u0441 Kubernetes Security as Standard in the Land of Kubernetes. 10 Data Structure, Algorithms, and Programming Courses to Crack Any Coding Interview CRUD App using Vue.js and Django Comparing Kubernetes CNI Providers: Flannel, Calico, Canal, and Weave","title":"Bookshelf"},{"location":"miscellaneous/bookshelf/#useful-links","text":"r/devops","title":"Useful Links"},{"location":"miscellaneous/bookshelf/#books","text":"","title":"Books"},{"location":"miscellaneous/bookshelf/#my-current-have-to-be-read-list","text":"The Kubernetes Scheduler Imperative vs. Declarative \u2014 a Kubernetes Tutorial Deploy a Kubernetes Cluster with Cluster Monitoring in Amazon EC2 using Rancher 10 Advanced Tricks with Docker 50 Useful Docker Tutorials, From Beginner to Advanced \u0417\u043d\u0430\u043a\u043e\u043c\u0441\u0442\u0432\u043e \u0441 Kubernetes Security as Standard in the Land of Kubernetes. 10 Data Structure, Algorithms, and Programming Courses to Crack Any Coding Interview CRUD App using Vue.js and Django Comparing Kubernetes CNI Providers: Flannel, Calico, Canal, and Weave","title":"My current have-to-be-read list"},{"location":"miscellaneous/makdocs/","text":"Some useful info about mkdocs \u00b6 Documentation links For full documentation visit -> mkdocs.org Docs about Material template -> github.com Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Examples \u00b6 Tables \u00b6 Listener Target Group Purpose Attachment TCP 80 environment -acme-http HTTP LE verification requests The autoscaling group for ECS instances populates this target group on port 80. TCP 443 environment -acme-https SSL offload for pm/owners requests ECS service environment -acme-proxy populates this target group on port 443. Code \u00b6 def fn (): pass Marked text \u00b6 This is marked line of text inside regular text Highlighting specific lines \u00b6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Keyboard shortcuts \u00b6 Ctrl + Alt + My Special Key Formula \u00b6 \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} Checklist \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Image as code \u00b6 Nice service to convert images to Base65: https://www.base64-image.de","title":"Makdocs helper"},{"location":"miscellaneous/makdocs/#some-useful-info-about-mkdocs","text":"Documentation links For full documentation visit -> mkdocs.org Docs about Material template -> github.com","title":"Some useful info about mkdocs"},{"location":"miscellaneous/makdocs/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"miscellaneous/makdocs/#project-layout","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"miscellaneous/makdocs/#examples","text":"","title":"Examples"},{"location":"miscellaneous/makdocs/#tables","text":"Listener Target Group Purpose Attachment TCP 80 environment -acme-http HTTP LE verification requests The autoscaling group for ECS instances populates this target group on port 80. TCP 443 environment -acme-https SSL offload for pm/owners requests ECS service environment -acme-proxy populates this target group on port 443.","title":"Tables"},{"location":"miscellaneous/makdocs/#code","text":"def fn (): pass","title":"Code"},{"location":"miscellaneous/makdocs/#marked-text","text":"This is marked line of text inside regular text","title":"Marked text"},{"location":"miscellaneous/makdocs/#highlighting-specific-lines","text":"\"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Highlighting specific lines"},{"location":"miscellaneous/makdocs/#keyboard-shortcuts","text":"Ctrl + Alt + My Special Key","title":"Keyboard shortcuts"},{"location":"miscellaneous/makdocs/#formula","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Formula"},{"location":"miscellaneous/makdocs/#checklist","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Checklist"},{"location":"miscellaneous/makdocs/#image-as-code","text":"Nice service to convert images to Base65: https://www.base64-image.de","title":"Image as code"}]}